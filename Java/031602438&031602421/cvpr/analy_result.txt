0
Title: Single-Shot Refinement Neural Network for Object Detection
Type: Poster
Abstracts: For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression accuracy and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet.
Authors: NLPR, CASIA;Longyin Wen, GE Global Research Center;Xiao Bian, ;Zhen Lei, Chinese Academy of Sciences ;Stan Li,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.pdf


1
Title: Video Captioning via Hierarchical Reinforcement Learning
Type: Poster
Abstracts: Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.
Authors: Xin Wang, UCSB;Wenhu Chen, ;Jiawei Wu, UCSB;Yuan-Fang Wang, UCSB;William Yang Wang, UCSB;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf


2
Title: DensePose: Multi-Person Dense Human Pose Estimation In The Wild
Type: Oral
Authors: Alp Guler, INRIA;Natalia Neverova, Facebook AI Research;Iasonas Kokkinos, FAIR/UCL;

3
Title: DensePose: Multi-Person Dense Human Pose Estimation In The Wild
Type: Poster
Authors: Alp Guler, INRIA;Natalia Neverova, Facebook AI Research;Iasonas Kokkinos, FAIR/UCL;

4
Title: Frustum PointNets for 3D Object Detection from RGB-D Data
Type: Poster
Authors: Charles R. Qi, Stanford University;Wei Liu, ;Chenxia Wu, ;hao Su, ;Leonidas J. Guibas,;

5
Title: Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge
Type: Poster
Authors: Damien Teney, Unversity of Adelaide;Peter Anderson, Australian National University;Xiaodong He, ;Anton Van den Hengel, University of Adelaide;

6
Title: Rethinking the Faster R-CNN Architecture for Temporal Action Localization
Type: Poster
Abstracts: We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.
Authors: Yu-Wei Chao, University of Michigan;Sudheendra Vijayanarasimhan, Google Research;Bryan Seybold, Google Research;David Ross, Google Research;Jia Deng, ;Rahul Sukthankar, Google Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Rethinking_the_Faster_CVPR_2018_paper.pdf


7
Title: Shape from Shading through Shape Evolution
Type: Spotlight
Authors: Dawei Yang, University of Michigan;Jia Deng,;

8
Title: Shape from Shading through Shape Evolution
Type: Poster
Authors: Dawei Yang, University of Michigan;Jia Deng,;

9
Title: A High-Quality Denoising Dataset for Smartphone Cameras
Type: Poster
Abstracts: The last decade has seen an astronomical shift from imaging with DSLR and point-and-shoot cameras to imaging with smartphone cameras. Due to the small aperture and sensor size, smartphone images have notably more noise than their DSLR counterparts. While denoising for smartphone images is an active research area, the research community currently lacks a denoising image dataset representative of real noisy images from smartphone cameras with high-quality ground truth. We address this issue in this paper with the following contributions. We propose a systematic procedure for estimating ground truth for noisy images that can be used to benchmark denoising performance for smartphone cameras. Using this procedure, we have captured a dataset, the Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images. We used this dataset to benchmark a number of denoising algorithms. We show that CNN-based methods perform better when trained on our high-quality dataset than when trained using alternative strategies, such as low-ISO images used as a proxy for ground truth data.
Authors: Abdelrahman Abdelhamed, York University;Microsoft Research Asia, China;Michael Brown, York University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.pdf


10
Title: Improving Color Reproduction Accuracy in the Camera Imaging Pipeline
Type: Poster
Authors: Hakki Karaimer, York University;Michael Brown, York University;

11
Title: End-to-End Dense Video Captioning with Masked Transformer
Type: Spotlight
Authors: Luowei Zhou, University of Michigan;Yingbo Zhou, Salesforce;Jason Corso, ;Richard Socher, Meta-Mind;Caiming Xiong, Salesforce;

12
Title: End-to-End Dense Video Captioning with Masked Transformer
Type: Poster
Authors: Luowei Zhou, University of Michigan;Yingbo Zhou, Salesforce;Jason Corso, ;Richard Socher, Meta-Mind;Caiming Xiong, Salesforce;

13
Title: pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment
Type: Poster
Abstracts: Bundle adjustment is a nonlinear refinement method for camera poses and 3D structure requiring sufficiently good initialization. In recent years, it was experimentally observed that useful minima can be reached even from arbitrary initialization for affine bundle adjustment problems (and fixed-rank matrix factorization instances in general). The key success factor lies in the use of the variable projection (VarPro) method, which is known to have a wide basin of convergence for such problems. In this paper, we propose the Pseudo Object Space Error (pOSE), which is an objective with cameras represented as a hybrid between the affine and projective models. This formulation allows us to obtain 3D reconstructions that are close to the true projective reconstructions while retaining a bilinear problem structure suitable for the VarPro method. Experimental results show that using pOSE has a high success rate to yield faithful 3D reconstructions from random initializations, taking one step towards initialization-free structure from motion.
Authors: Je Hyeong Hong, University of Cambridge;Christopher Zach, Toshiba Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_pOSE_Pseudo_Object_CVPR_2018_paper.pdf


14
Title: Learning to Segment Every Thing
Type: Poster
Abstracts: Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.
Authors: Ronghang Hu, UC Berkeley;Menlo Park, USA;Kaiming He, ;UC Berkeley, USA;Ross Girshick,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_to_Segment_CVPR_2018_paper.pdf


15
Title: Density-aware Single Image De-raining using a Multi-stream Dense Network
Type: Poster
Authors: He Zhang, Rutgers;Vishal Patel,;

16
Title: Densely Connected Pyramid Dehazing Network
Type: Poster
Abstracts: We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incor- We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Net- work (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end- to-end learning is achieved by directly embedding the atmo- spheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense net- work that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi- level pyramid pooling module for estimating the transmis- sion map. This network is optimized using a newly in- troduced edge-preserving loss function. To further incor- porate the mutual structural information between the esti- mated transmission map and the dehazed result, we pro- pose a joint-discriminator based on generative adversar- ial network framework to decide whether the correspond- ing dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demon- strate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Exten- sive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the- art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN
Authors: He Zhang, Rutgers;Vishal Patel,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Densely_Connected_Pyramid_CVPR_2018_paper.pdf


17
Title: Embodied Question Answering
Type: Poster
Abstracts: We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question ("What color is the car?"). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question ("orange"). EmbodiedQA requires a range of AI skills -- language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments, evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning.
Authors: Abhishek Das, Georgia Tech;Samyak Datta, Georgia Tech;Georgia Gkioxari, Facebook;Devi Parikh, Georgia Tech;Dhruv Batra, Georgia Tech;Stefan Lee, Georgia Tech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf


18
Title: TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays
Type: Spotlight
Authors: Xiaosong Wang, NIH;Yifan Peng, NIH NLM;Le Lu, Nvidia Corp;Zhiyong Lu, ;Ronald Summers,;

19
Title: TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays
Type: Poster
Authors: Xiaosong Wang, NIH;Yifan Peng, NIH NLM;Le Lu, Nvidia Corp;Zhiyong Lu, ;Ronald Summers,;

20
Title: Towards Open-Set Identity Preserving Face Synthesis
Type: Poster
Abstracts: We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.
Authors: Jianmin Bao, USTC;Dong Chen, Microsoft Research Asia;Fang Wen, ;Houqiang Li, ;Gang Hua, Microsoft Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bao_Towards_Open-Set_Identity_CVPR_2018_paper.pdf


21
Title: Baseline Desensitizing In Translation Averaging
Type: Poster
Authors: Bingbing Zhuang, National University of Singapore;Loong Fah Cheong, National University of Singapore;Gim Hee Lee, National University of SIngapore;

22
Title: Learning from the Deep: A Revised Underwater Image Formation Model
Type: Poster
Authors: Derya Akkaynak, University of Haifa;Tali Treibitz, University of Haifa;

23
Title: Context Encoding for Semantic Segmentation
Type: Oral
Abstracts: Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.
Authors: Hang Zhang, Rutgers University;Kristin Dana, ;Jianping Shi, SenseTime;Zhongyue Zhang, Amazon;Xiaogang Wang, Chinese University of Hong Kong;Ambrish Tyagi, Amazon;Amit Agrawal, Amazon;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf


24
Title: Context Encoding for Semantic Segmentation
Type: Poster
Abstracts: Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.
Authors: Hang Zhang, Rutgers University;Kristin Dana, ;Jianping Shi, SenseTime;Zhongyue Zhang, Amazon;Xiaogang Wang, Chinese University of Hong Kong;Ambrish Tyagi, Amazon;Amit Agrawal, Amazon;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf


25
Title: Deep Texture Manifold for Ground Terrain Recognition
Type: Poster
Abstracts: We present a texture network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition. Recognition of ground terrain is an important task in establishing robot or vehicular control parameters, as well as for localization within an outdoor environment. The architecture of DEP integrates orderless texture details and local spatial information and the performance of DEP surpasses state-of-the-art methods for this task. The GTOS database (comprised of over 30,000 images of 40 classes of ground terrain in outdoor scenes) enables supervised recognition. For evaluation under realistic conditions, we use test images that are not from the existing GTOS dataset, but are instead from hand-held mobile phone videos of similar terrain. This new evaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground terrain such as grass, gravel, asphalt and sand. The resultant network shows excellent performance not only for GTOS-mobile, but also for more general databases (MINC and DTD). Leveraging the discriminant features learned from this network, we build a new texture manifold called DEP-manifold. We learn a parametric distribution in feature space in a fully supervised manner, which gives the distance relationship among classes and provides a means to implicitly represent ambiguous class boundaries. The source code and database are publicly available.
Authors: Jia Xue, Rutgers;Hang Zhang, Rutgers University;Kristin Dana,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xue_Deep_Texture_Manifold_CVPR_2018_paper.pdf


26
Title: DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems
Type: Poster
Abstracts: In this work we study convex relaxations of quadratic optimisation problems over permutation matrices. While existing semidefinite programming approaches can achieve remarkably tight relaxations, they have the strong disadvantage that they lift the original n^2-dimensional variable to an n^4-dimensional variable, which limits their practical applicability. In contrast, here we present a lifting-free convex relaxation that is provably at least as tight as existing (lifting-free) convex relaxations. We demonstrate experimentally that our approach is superior to existing convex and non-convex methods for various problems, including image arrangement and multi-graph matching.
Authors: Florian Bernard, ;Christian Theobalt, MPI Informatics;Michael Moeller, University of Siegen;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bernard_DS_Tighter_Lifting-Free_CVPR_2018_paper.pdf


27
Title: Sparse
Type: Poster
Authors: Tali Dekel, Google;Dilip Krishnan, Google;Chuang Gan, Tsinghua University;Cambridge, USA;William Freeman, Google;

28
Title: Every Smile is Unique: Landmark-guided Diverse Smile Generation
Type: Poster
Authors: Wei Wang, University of Trento;Xavier Alameda-Pineda, University of Trento;Dan Xu, ;Elisa Ricci, U. Perugia;Nicu Sebe, University of Trento;

29
Title: Generative Non-Rigid Shape Completion with Graph Convolutional Autoencoders
Type: Poster
Authors: Or Litany, Tel Aviv University;Alex Bronstein, ;Michael Bronstein, ;Ameesh Makadia, Google Research;

30
Title: Learning a Discriminative Prior for Blind Image Deblurring
Type: Poster
Abstracts: We present an effective blind image deblurring method based on a data-driven discriminative prior. Our work is motivated by the fact that a good image prior should favor clear images over blurred images. To obtain such an image prior for deblurring, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN). The learned image prior has a significant discriminative property and is able to distinguish whether the image is clear or not. Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring on various scenarios, including natural, face, text, and low-illumination images. However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN. Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model. Furthermore, the proposed model can be easily extended to non-uniform deblurring. Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches.
Authors: Lerenhan Li, HUST;Jinshan Pan, UC Merced;University of California, Merced;Changxin Gao, HUST;Nong Sang, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_a_Discriminative_CVPR_2018_paper.pdf


31
Title: Attentional ShapeContextNet for Point Cloud Recognition
Type: Poster
Abstracts: We tackle the problem of point cloud recognition. Unlike previous approaches where a point cloud is either converted into a volume/image or represented independently in a permutation-invariant set, we develop a new representation by adopting the concept of shape context as the building block in our network design. The resulting model, called ShapeContextNet, consists of a hierarchy with modules not relying on a fixed grid while still enjoying properties similar to those in convolutional neural networks --- being able to capture and propagate the object part information. In addition, we find inspiration from self-attention based models to include a simple yet effective contextual modeling mechanism --- making the contextual region selection, the feature aggregation, and the feature transformation process fully automatic. ShapeContextNet is an end-to-end model that can be applied to the general point cloud classification and segmentation problems. We observe competitive results on a number of benchmark datasets.
Authors: Saining Xie, UCSD;Sainan Liu, UCSD;Zeyu Chen, UCSD;UCSD, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.pdf


32
Title: Learning Superpixels with Segmentation-Aware Affinity Loss
Type: Poster
Authors: Wei-Chih Tu, National Taiwan University;Ming-Yu Liu, NVIDIA;Varun Jampani, NVIDIA Research;Deqing Sun, NVIDIA;Shao-Yi Chien, National Taiwan University;Ming-Hsuan Yang, UC Merced;Jan Kautz, NVIDIA;

33
Title: Real-World Repetition Estimation by Div
Type: Spotlight
Authors: Tom Runia, University of Amsterdam;Cees Snoek, University of Amsterdam;University of Amsterdam, Netherlands;

34
Title: Real-World Repetition Estimation by Div
Type: Poster
Authors: Tom Runia, University of Amsterdam;Cees Snoek, University of Amsterdam;University of Amsterdam, Netherlands;

35
Title: Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation
Type: Poster
Abstracts: We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage. This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.
Authors: Qihang Yu, Peking University;Lingxi Xie, UCLA;Yan Wang, JHU;Yuyin Zhou, JHU;Elliot Fishman, ;Alan Yuille, JHU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.pdf


36
Title: MegaDepth: Learning Single-View Depth Prediction from Internet Photos
Type: Poster
Authors: Zhengqi Li, Cornell University;Noah Snavely, Cornell University / Google;

37
Title: Learning Intrinsic Image Decomposition from Watching the World
Type: Spotlight
Authors: Zhengqi Li, Cornell University;Noah Snavely, Cornell University / Google;

38
Title: Learning Intrinsic Image Decomposition from Watching the World
Type: Poster
Authors: Zhengqi Li, Cornell University;Noah Snavely, Cornell University / Google;

39
Title: Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering
Type: Poster
Abstracts: A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.
Authors: Aishwarya Agrawal, Georgia Institute of Technology;Dhruv Batra, Georgia Tech;Devi Parikh, Georgia Tech;Aniruddha Kembhavi, Allen Institute for Artificial Intelligence;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf


40
Title: Human-centric Indoor Scene Synthesis Using Stochastic Grammar
Type: Poster
Authors: Siyuan Qi, UCLA;Yixin Zhu, UCLA;Siyuan Huang, UCLA;Chenfanfu Jiang, ;Song-Chun Zhu,;

41
Title: Learning by Asking Questions
Type: Poster
Abstracts: We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.
Authors: Ishan Misra, CMU;Ross Girshick, ;Rob Fergus, New York University;Martial Hebert, Carnegie Mellon University;Abhinav Gupta, ;Laurens van der Maaten, Facebook;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Misra_Learning_by_Asking_CVPR_2018_paper.pdf


42
Title: Instance Embedding Transfer to Unsupervised Video Object Segmentation
Type: Poster
Abstracts: We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.
Authors: Siyang Li, USC;Bryan Seybold, Google Research;Alexey Vorobyov, Google Inc.;Alireza Fathi, Stanford University;Qin Huang, University of Southern California;C.-C. Jay Kuo, University of Southern California;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Instance_Embedding_Transfer_CVPR_2018_paper.pdf


43
Title: Detect-and-Track: Efficient Pose Estimation in Videos
Type: Poster
Abstracts: This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.
Authors: Rohit Girdhar, CMU;Georgia Gkioxari, Facebook;Darthmout College, USA;Manohar Paluri, ;Du Tran, Dartmouth College;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.pdf


44
Title: Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval
Type: Poster
Abstracts: Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (SSAH) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.
Authors: Chao Li, Xidian University;Cheng Deng, Xidian University;Ning Li, Xidian University;Wei Liu, ;Dacheng Tao, University of Sydney;Xinbo Gao,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Self-Supervised_Adversarial_Hashing_CVPR_2018_paper.pdf


45
Title: Guided Proofreading of Automatic Segmentations for Connectomics
Type: Poster
Abstracts: Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.
Authors: Daniel Haehn, Harvard University;Verena Kaynig, ;James Tompkin, Brown University;Jeff Lichtman, Harvard University;Hanspeter Pfister, Harvard University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Haehn_Guided_Proofreading_of_CVPR_2018_paper.pdf


46
Title: Augmented Skeleton Space Transfer for Depth-based Hand Pose Estimation
Type: Oral
Authors: Seungryul Baek, Imperial College London;Kwang In Kim, University of Bath;Tae-Kyun Kim, Imperial College London;

47
Title: Augmented Skeleton Space Transfer for Depth-based Hand Pose Estimation
Type: Poster
Authors: Seungryul Baek, Imperial College London;Kwang In Kim, University of Bath;Tae-Kyun Kim, Imperial College London;

48
Title: Context-aware Synthesis for Video Frame Interpolation
Type: Poster
Authors: Simon Niklaus, Portland State University;Feng Liu, Portland State University;

49
Title: 2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning
Type: Poster
Authors: Diogo Luvizon, ETIS Lab;David Picard, ETIS /LIP6;Hedi Tabia, ETIS / ENSEA;

50
Title: NAG: Network for Adversary Generation
Type: Poster
Abstracts: Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.
Authors: Konda Reddy Mopuri, Indian Institute of Science;Utkarsh Ojha, MNNIT Allahabad;Utsav Garg, Nanyang Technological University;Venkatesh Babu Radhakrishnan, Indian Institute of Science;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mopuri_NAG_Network_for_CVPR_2018_paper.pdf


51
Title: LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation
Type: Spotlight
Abstracts: FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.
Authors: Tak-Wai Hui, The Chinese University of Hong Kong;Chen-Change Loy, the Chinese University of Hong Kong;Xiaoou Tang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.pdf


52
Title: LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation
Type: Poster
Abstracts: FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.
Authors: Tak-Wai Hui, The Chinese University of Hong Kong;Chen-Change Loy, the Chinese University of Hong Kong;Xiaoou Tang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.pdf


53
Title: Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration
Type: Poster
Authors: Lu Sheng, The Chinese University of HK;Jing Shao, The Sensetime Group Limited;Ziyi Lin, SenseTime Co. Ltd.;Xiaogang Wang, Chinese University of Hong Kong;

54
Title: Multi-view Harmonized Bilinear Network for 3D Object Recognition
Type: Spotlight
Authors: Tan Yu, Nanyang Technological Univ;Jingjing Meng, ;Junsong Yuan, Nanyang Technological University;

55
Title: Multi-view Harmonized Bilinear Network for 3D Object Recognition
Type: Poster
Authors: Tan Yu, Nanyang Technological Univ;Jingjing Meng, ;Junsong Yuan, Nanyang Technological University;

56
Title: Tangent Convolutions for Dense Prediction in 3D
Type: Spotlight
Abstracts: We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes.
Authors: Maxim Tatarchenko, Freiburg;Jaesik Park, Intel Labs;Qian-Yi Zhou, ABQ Technologies;Vladlen Koltun, Intel Labs;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf


57
Title: Tangent Convolutions for Dense Prediction in 3D
Type: Poster
Abstracts: We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes.
Authors: Maxim Tatarchenko, Freiburg;Jaesik Park, Intel Labs;Qian-Yi Zhou, ABQ Technologies;Vladlen Koltun, Intel Labs;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf


58
Title: Semi-parametric Image Synthesis
Type: Oral
Authors: Xiaojuan Qi, CUHK;Qifeng Chen, Intel Labs;Jiaya Jia, Chinese University of Hong Kong;Vladlen Koltun, Intel Labs;

59
Title: Semi-parametric Image Synthesis
Type: Poster
Authors: Xiaojuan Qi, CUHK;Qifeng Chen, Intel Labs;Jiaya Jia, Chinese University of Hong Kong;Vladlen Koltun, Intel Labs;

60
Title: Interactive Image Segmentation with Latent Diversity
Type: Poster
Authors: Zhuwen Li, Intel Labs;Qifeng Chen, Intel Labs;Vladlen Koltun, Intel Labs;

61
Title: 3D Hand Pose Estimation: From Current Achievements to Future Goals
Type: Spotlight
Authors: Shanxin Yuan, Imperial College London;Guillermo Garcia-Hernando, Imperial College London;Bjorn Stenger, ;Tae-Kyun Kim, Imperial College London;Gyeongsik Moon, Seoul National University;Ju Yong Chang, Kwangwoon University;Kyoung Mu Lee, ;Pavlo Molchanov, NVIDIA Research;Liuhao Ge, NTU;Junsong Yuan, Nanyang Technological University;Xinghao Chen, Tsinghua University;Guijin Wang, Tsinghua University;Fan Yang, Nara institute of science and technology;Kai Akiyama, Nara Institute of Science and Technology;Yang Wu, Nara Institute of Science and Technology;Qingfu Wan, Fudan University;Barcelona, Spain;Sergio Escalera, University of Barcelona;Shile Li, Technical University of Munich;Dongheui Lee, Technical University of Munich;Iason Oikonomidis, FORTH;Antonis Argyros, FORTH;

62
Title: 3D Hand Pose Estimation: From Current Achievements to Future Goals
Type: Poster
Authors: Shanxin Yuan, Imperial College London;Guillermo Garcia-Hernando, Imperial College London;Bjorn Stenger, ;Tae-Kyun Kim, Imperial College London;Gyeongsik Moon, Seoul National University;Ju Yong Chang, Kwangwoon University;Kyoung Mu Lee, ;Pavlo Molchanov, NVIDIA Research;Liuhao Ge, NTU;Junsong Yuan, Nanyang Technological University;Xinghao Chen, Tsinghua University;Guijin Wang, Tsinghua University;Fan Yang, Nara institute of science and technology;Kai Akiyama, Nara Institute of Science and Technology;Yang Wu, Nara Institute of Science and Technology;Qingfu Wan, Fudan University;Barcelona, Spain;Sergio Escalera, University of Barcelona;Shile Li, Technical University of Munich;Dongheui Lee, Technical University of Munich;Iason Oikonomidis, FORTH;Antonis Argyros, FORTH;

63
Title: W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection
Type: Poster
Abstracts: Weakly-supervised object detection has attracted much attention lately, since it does not require bounding box annotations for training. Although significant progress has also been made, there is still a large gap in performance between weakly-supervised and fully-supervised object detection. Recently, some works use pseudo ground-truths which are generated by a weakly-supervised detector to train a supervised detector. Such approaches incline to find the most representative parts of objects, and only seek one ground-truth box per class even though many same-class instances exist. To overcome these issues, we propose a weakly-supervised to fully-supervised framework, where a weakly-supervised detector is implemented using multiple instance learning. Then, we propose a pseudo ground-truth excavation (PGE) algorithm to find the pseudo ground-truth of each instance in the image. Moreover, the pseudo ground-truth adaptation (PGA) algorithm is designed to further refine the pseudo ground-truths from PGE. Finally, we use these pseudo ground-truths to train a fully-supervised detector. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 benchmarks strongly demonstrate the effectiveness of our framework. We obtain 52.4% and 47.8% mAP on VOC2007 and VOC2012 respectively, a significant improvement over previous state-of-the-art methods.
Authors: Yongqiang Zhang, Harbin institute of Technology/KAUST;Yancheng Bai, Kaust/Iscas;Mingli Ding, ;Yongqiang Li, ;Bernard Ghanem,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.pdf


64
Title: BlockDrop: Dynamic Inference Paths in Residual Networks
Type: Spotlight
Abstracts: Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20% on average, going as high as 36% for some images, while maintaining the same 76.4% top-1 accuracy on ImageNet.
Authors: Zuxuan Wu, University of Maryland;Tushar Nagarajan, University of Texas at Austin;Abhishek Kumar, ;Steven Rennie, ;University of Maryland, USA;Kristen Grauman, ;Rogerio Feris, IBM;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.pdf


65
Title: BlockDrop: Dynamic Inference Paths in Residual Networks
Type: Poster
Abstracts: Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20% on average, going as high as 36% for some images, while maintaining the same 76.4% top-1 accuracy on ImageNet.
Authors: Zuxuan Wu, University of Maryland;Tushar Nagarajan, University of Texas at Austin;Abhishek Kumar, ;Steven Rennie, ;University of Maryland, USA;Kristen Grauman, ;Rogerio Feris, IBM;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.pdf


66
Title: MapNet: Geometry-Aware Learning of Maps for Camera Localization
Type: Spotlight
Authors: Samarth Brahmbhatt, Georgia Tech;Jinwei Gu, NVIDIA;Kihwan Kim, NVIDIA Research;James Hays, Georgia Tech;Jan Kautz, NVIDIA;

67
Title: MapNet: Geometry-Aware Learning of Maps for Camera Localization
Type: Poster
Authors: Samarth Brahmbhatt, Georgia Tech;Jinwei Gu, NVIDIA;Kihwan Kim, NVIDIA Research;James Hays, Georgia Tech;Jan Kautz, NVIDIA;

68
Title: BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning
Type: Poster
Abstracts: Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, {em BPGrad}, towards optimizing deep models globally via branch and pruning. Our BPGrad is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation.
Authors: Ziming Zhang, MERL;Yuanwei Wu, University of Kansas;Guanghui Wang, University of Kansas;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_BPGrad_Towards_Global_CVPR_2018_paper.pdf


69
Title: Salient Object Detection Driven by Fixation Prediction
Type: Poster
Abstracts: Research in visual saliency has been focused on two major types of models namely fixation prediction and salient object detection. The relationship between the two, however, has been less explored. In this paper, we propose to employ the former model type to identify and segment salient objects in scenes. We build a novel neural network called Attentive Saliency Network (ASNet) that learns to detect salient objects from fixation maps. The fixation map, derived at the upper network layers, captures a high-level understanding of the scene. Salient object detection is then viewed as fine-grained object-level saliency segmentation and is progressively optimized with the guidance of the fixation map in a top-down manner. ASNet is based on a hierarchy of convolutional LSTMs (convLSTMs) that offers an efficient recurrent mechanism for sequential refinement of the segmentation map. Several loss functions are introduced for boosting the performance of the ASNet. Extensive experimental evaluation shows that our proposed ASNet is capable of generating accurate segmentation maps with the help of the computed fixation map. Our work offers a deeper insight into the mechanisms of attention and narrows the gap between salient object detection and fixation prediction.
Authors: Wenguan Wang, Beijing Institute of Technology;Jianbing Shen, Beijing Institute of Technolog;Xingping Dong, Beijing Institute of Technology;Ali Borji, UCF;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Salient_Object_Detection_CVPR_2018_paper.pdf


70
Title: 3D Object Detection with Latent Support Surfaces
Type: Poster
Authors: Zhile Ren, Brown University;Erik Sudderth, UC Irvine;

71
Title: Practical Block-wise Neural Network Architecture Generation
Type: Oral
Authors: Institute of Automation,CAS;Junjie Yan, ;Wei Wu, ;Jing Shao, The Sensetime Group Limited;cheng-lin Liu,;

72
Title: Practical Block-wise Neural Network Architecture Generation
Type: Poster
Authors: Institute of Automation,CAS;Junjie Yan, ;Wei Wu, ;Jing Shao, The Sensetime Group Limited;cheng-lin Liu,;

73
Title: Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points
Type: Poster
Authors: LIRIS, INSA-Lyon;CITI, LIRIS;Julien Mille, INSA Val de Loire;Graham Taylor, University of Guelph;

74
Title: Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning
Type: Oral
Authors: Qi Wu, University of Adelaide;Peng Wang, ;Chunhua Shen, University of Adelaide;Ian Reid, ;Anton Van den Hengel, University of Adelaide;

75
Title: Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning
Type: Poster
Authors: Qi Wu, University of Adelaide;Peng Wang, ;Chunhua Shen, University of Adelaide;Ian Reid, ;Anton Van den Hengel, University of Adelaide;

76
Title: Visual Grounding via Accumulated Attention
Type: Poster
Abstracts: Visual Grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. The query can be a phrase, a sentence or even a multi-round dialogue. There are three main challenges in VG: 1) what is the main focus in a query; 2) how to understand an image; 3) how to locate an object. Most existing methods combine all the information curtly, which may suffer from the problem of information redundancy (i.e. ambiguous query, complicated image and a large number of objects). In this paper, we formulate these challenges as three attention problems and propose an accumulated attention (A-ATT) mechanism to reason among them jointly. Our A-ATT mechanism can circularly accumulate the attention for useful information in image, query, and objects, while the noises are ignored gradually. We evaluate the performance of A-ATT on four popular datasets (namely ReferCOCO, ReferCOCO+, ReferCOCOg, and Guesswhat?!), and the experimental results show the superiority of the proposed method in term of accuracy.
Authors: chaorui Deng, ;Qi Wu, University of Adelaide;Fuyuan Hu, ;Fan Lyu, Suzhou University of Science and Technology;Mingkui Tan, South China University of Technology;School of Software Engineering, South China University of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf


77
Title: Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors
Type: Poster
Abstracts: In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame t-1 followed by optical flow tracking from frame t-1 to frame t should coincide with the location of the detection at frame t. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.
Authors: Xuanyi Dong, UTS;Shoou-I Yu, Oculus;Xinshuo Weng, Carnegie Mellon University;Shih-En Wei, Oculus Research;Yi Yang, ;Yaser Sheikh,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper.pdf


78
Title: ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing
Type: Poster
Abstracts: With the aim of developing a fast yet accurate algorithm for compressive sensing (CS) reconstruction of natural images, we combine in this paper the merits of two existing categories of CS methods: the structure insights of traditional optimization-based methods and the performance/speed of recent network-based ones. Specifically, we propose a novel structured deep network, dubbed ISTA-Net, which is inspired by the Iterative Shrinkage-Thresholding Algorithm (ISTA) for optimizing a general L1 norm CS reconstruction model. To cast ISTA into deep network form, we develop an effective strategy to solve the proximal mapping associated with the sparsity-inducing regularizer using nonlinear transforms. All the parameters in ISTA-Net (e.g. nonlinear transforms, shrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than being hand-crafted. Moreover, considering that the residuals of natural images are more compressible, an enhanced version of ISTA-Net in the residual domain, dubbed ISTA-Net+, is derived to further improve CS reconstruction. Extensive CS experiments demonstrate that the proposed ISTA-Nets outperform existing state-of-the-art optimization-based and network-based CS methods by large margins, while maintaining fast computational speed.
Authors: Jian Zhang, KAUST;Bernard Ghanem,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ISTA-Net_Interpretable_Optimization-Inspired_CVPR_2018_paper.pdf


79
Title: Perturbative Neural Networks: Rethinking Convolution in CNNs
Type: Poster
Authors: Felix Juefei-Xu, Carnegie Mellon University;Vishnu Naresh Boddeti, Michigan State University;Marios Savvides, Carnegie Mellon University;

80
Title: Nonlinear 3D Face Morphable Model
Type: Spotlight
Abstracts: As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.
Authors: LUAN TRAN, Michigan State University;Xiaoming Liu, Michigan State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_Nonlinear_3D_Face_CVPR_2018_paper.pdf


81
Title: Nonlinear 3D Face Morphable Model
Type: Poster
Abstracts: As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.
Authors: LUAN TRAN, Michigan State University;Xiaoming Liu, Michigan State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_Nonlinear_3D_Face_CVPR_2018_paper.pdf


82
Title: Neural Baby Talk
Type: Spotlight
Abstracts: We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk
Authors: Jiasen Lu, Georgia Institute of Technology;Jianwei Yang, Georgia Tech;Dhruv Batra, Georgia Tech;Devi Parikh, Georgia Tech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lu_Neural_Baby_Talk_CVPR_2018_paper.pdf


83
Title: Neural Baby Talk
Type: Poster
Abstracts: We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence `template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk
Authors: Jiasen Lu, Georgia Institute of Technology;Jianwei Yang, Georgia Tech;Dhruv Batra, Georgia Tech;Devi Parikh, Georgia Tech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lu_Neural_Baby_Talk_CVPR_2018_paper.pdf


84
Title: Towards Pose Invariant Face Recognition in the Wild
Type: Poster
Abstracts: Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a "learning to learn" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.
Authors: Jian Zhao, NUS;Yu Cheng, Nanyang Technological University;Learning & Vision, Panasonic R&D Center Singapore;Learning & Vision, Panasonic R&D Center Singapore;Jianshu Li, National University of Singapo;Fang Zhao, National University of Singapore;Learning & Vision, Panasonic R&D Center Singapore;Learning & Vision, Panasonic R&D Center Singapore;Learning & Vision, Panasonic R&D Center Singapore;Institute of Automation, Chinese Academy of Sciences;Shuicheng Yan, National University of Singapore;Jiashi Feng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf


85
Title: MoNet: Deep Motion Exploitation for Video Object Segmentation
Type: Poster
Abstracts: In this paper, we propose a novel MoNet model to deeply exploit motion cues for boosting video object segmentation performance from two aspects, i.e., frame representation learning and segmentation refinement. Concretely, MoNet exploits computed motion cue (i.e., optical flow) to reinforce the representation of the target frame by aligning and integrating representations from its neighbors. The new representation provides valuable temporal contexts for segmentation and improves robustness to various common contaminating factors, e.g., motion blur, appearance variation and deformation of video objects. Moreover, MoNet exploits motion inconsistency and transforms such motion cue into foreground/background prior to eliminate distraction from confusing instances and noisy regions. By introducing a distance transform layer, MoNet can effectively separate motion-inconstant instances/regions and thoroughly refine segmentation results. Integrating the proposed two motion exploitation components with a standard segmentation network, MoNet provides new state-of-the-art performance on three competitive benchmark datasets.
Authors: Huaxin Xiao, Nudt;Jiashi Feng, ;Guosheng Lin, Nanyang Technological Universi;Yu Liu, NUDT;Maojun Zhang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xiao_MoNet_Deep_Motion_CVPR_2018_paper.pdf


86
Title: Exploring Disentangled Feature Representation Beyond Face Identification
Type: Poster
Abstracts: This paper proposes learning disentangled but complementary face features with a minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Auto-Encoder (D^2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity verification performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.
Authors: Yu Liu, CUHK;Fangyin Wei, Peking University;Jing Shao, The Sensetime Group Limited;Lu Sheng, The Chinese University of HK;Junjie Yan, ;Xiaogang Wang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf


87
Title: Towards Effective Low-bitwidth Convolutional Neural Networks
Type: Poster
Authors: Bohan Zhuang, The University of Adelaide;Chunhua Shen, University of Adelaide;Mingkui Tan, South China University of Technology;Lingqiao Liu, University of Adelaide;Ian Reid,;

88
Title: Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries
Type: Poster
Authors: Bohan Zhuang, The University of Adelaide;Qi Wu, University of Adelaide;Chunhua Shen, University of Adelaide;Ian Reid, ;Anton Van den Hengel, University of Adelaide;

89
Title: Learning Facial Action Units from Web Images with Scalable Weakly Supervised Clustering
Type: Poster
Authors: Kaili Zhao, Beijing University of Post & T;Wen-Sheng Chu, Carnegie Mellon University;Aleix Martinez, The ohio state university;

90
Title: Few-Shot Image Recognition by Predicting Parameters from Activations
Type: Spotlight
Authors: Siyuan Qiao, Johns Hopkins University;Chenxi Liu, JHU;Wei Shen, Shanghai University;Alan Yuille,;

91
Title: Few-Shot Image Recognition by Predicting Parameters from Activations
Type: Poster
Authors: Siyuan Qiao, Johns Hopkins University;Chenxi Liu, JHU;Wei Shen, Shanghai University;Alan Yuille,;

92
Title: Single-Shot Object Detection with Enriched Semantics
Type: Poster
Authors: Zhishuai Zhang, Johns Hopkins University;Siyuan Qiao, Johns Hopkins University;Cihang Xie, JHU;Wei Shen, Shanghai University;Bo Wang, HikVision USA Inc.;Alan Yuille, JHU;

93
Title: Unifying Identification and Context Learning for Person Recognition
Type: Poster
Abstracts: Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies.
Authors: Qingqiu Huang, CUHK;Yu Xiong, CUHK;Dahua Lin, CUHK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Unifying_Identification_and_CVPR_2018_paper.pdf


94
Title: Separating Self-Expression and Visual Content in Hashtag Supervision
Type: Poster
Abstracts: The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be polysemous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs with a joint model of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging.
Authors: Andreas Veit, Cornel Tech ;Maximillian Nickel, ;Serge Belongie, ;Laurens van der Maaten, Facebook;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Veit_Separating_Self-Expression_and_CVPR_2018_paper.pdf


95
Title: Multi-Cue Correlation Filters for Robust Visual Tracking
Type: Poster
Abstracts: In recent years, many tracking algorithms achieve impressive performance via fusing multiple types of features, however, most of them fail to fully explore the context among the adopted multiple features and the strength of them. In this paper, we propose an efficient multi-cue analysis framework for robust visual tracking. By combining different types of features, our approach constructs multiple experts through Discriminative Correlation Filter (DCF) and each of them tracks the target independently. With the proposed robustness evaluation strategy, the suitable expert is selected for tracking in each frame. Furthermore, the divergence of multiple experts reveals the reliability of the current tracking, which is quantified to update the experts adaptively to keep them from corruption. Through the proposed multi-cue analysis, our tracker with standard DCF and deep features achieves outstanding results on several challenging benchmarks: OTB-2013, OTB-2015, Temple-Color and VOT 2016. On the other hand, when evaluated with only simple hand-crafted features, our method demonstrates comparable performance amongst complex non-realtime trackers, but exhibits much better efficiency, with a speed of 45 FPS on a CPU.
Authors: Ning Wang, USTC;Wengang Zhou, USTC;Qi Tian, ;Richang Hong, ;Meng Wang, HeFei University of Technology;Houqiang Li,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Cue_Correlation_Filters_CVPR_2018_paper.pdf


96
Title: Beyond Trade-off: Accelerate FCN-based Face Detection with Higher Accuracy
Type: Poster
Authors: Guanglu Song, Beihang University;Yu Liu, CUHK;Ming Jiang, BUAA;Yujie Wang, Beihang university;

97
Title: On the Robustness of Semantic Segmentation Models to Adversarial Attacks
Type: Poster
Abstracts: Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.
Authors: Anurag Arnab, University of Oxford;Ondrej Miksik, University of Oxford;Phil Torr, Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Arnab_On_the_Robustness_CVPR_2018_paper.pdf


98
Title: PWC-Net: CNNs for Optical Flow Using Pyramid
Type: Oral
Authors: Deqing Sun, NVIDIA;Xiaodong Yang, NVIDIA;Ming-Yu Liu, NVIDIA;Jan Kautz, NVIDIA;

99
Title: PWC-Net: CNNs for Optical Flow Using Pyramid
Type: Poster
Authors: Deqing Sun, NVIDIA;Xiaodong Yang, NVIDIA;Ming-Yu Liu, NVIDIA;Jan Kautz, NVIDIA;

100
Title: Illuminant Spectra-based Source Separation Using Flash Photography
Type: Oral
Authors: Zhuo Hui, Carnegie Mellon University;Kalyan Sunkavalli, Adobe Systems Inc.;Sunil Hadap, ;Aswin Sankaranarayanan, Carnegie Mellon University;

101
Title: Illuminant Spectra-based Source Separation Using Flash Photography
Type: Poster
Authors: Zhuo Hui, Carnegie Mellon University;Kalyan Sunkavalli, Adobe Systems Inc.;Sunil Hadap, ;Aswin Sankaranarayanan, Carnegie Mellon University;

102
Title: Tracking Multiple Objects Outside the Line of Sight using Speckle Imaging
Type: Spotlight
Authors: Brandon Smith, University of Wisconsin-Madiso;Matthew O'Toole, Stanford University;Mohit Gupta, Wisconsin;

103
Title: Tracking Multiple Objects Outside the Line of Sight using Speckle Imaging
Type: Poster
Authors: Brandon Smith, University of Wisconsin-Madiso;Matthew O'Toole, Stanford University;Mohit Gupta, Wisconsin;

104
Title: Improved Human Pose Estimation through Adversarial Data Augmentation
Type: Poster
Authors: Zhiqiang Tang, Rutgers;Xi Peng, ;Fei Yang, facebook;Rogerio Feris, IBM;Dimitris Metaxas, Rutgers;

105
Title: Generative Adversarial Learning Towards Fast Weakly Supervised Detection
Type: Poster
Abstracts: Weakly supervised object detection has attracted extensive research efforts in recent years. Without the need of annotating bounding boxes, the existing methods usually follow a two/multi-stage pipeline with an online compulsive stage to extract object proposals, which is an order of magnitude slower than fast fully supervised object detectors such as SSD [31] and YOLO [34]. In this paper, we speedup online weakly supervised object detectors by orders of magnitude by proposing a novel generative adversarial learning paradigm. In the proposed paradigm, the generator is a one-stage object detector to generate bounding boxes from images. To guide the learning of object-level generator, a surrogator is introduced to mine high-quality bounding boxes for training. We further adapt a structural similarity loss in combination with an adversarial loss into the training objective, which solves the challenge that the bounding boxes produced by the surrogator may not well capture their ground truth. Our one-stage detector outperforms all existing schemes in terms of detection accuracy, running at 118 frames per second, which is up to 438x faster than the state-of-the-art weakly supervised detectors [8, 30, 15, 27, 45]. The code will be available publicly soon.
Authors: Yunhang Shen, Xiamen University;Rongrong Ji, ;Shengchuan Zhang, ;Wangmeng Zuo, Harbin Institute of Technology;Yan Wang, Microsoft;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Generative_Adversarial_Learning_CVPR_2018_paper.pdf


106
Title: Audio to Body Dynamics
Type: Spotlight
Abstracts: We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Notably, it's not clear if body movement can be predicted from music at all and our aim in this work is to explore this possibility. In this paper, we present the first result that shows that natural body dynamics can be predicted. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation.
Authors: Eli Shlizerman, Facebook;Lucio Dery, Stanford;Hayden Schoen, Facebook;Ira Kemelmacher,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shlizerman_Audio_to_Body_CVPR_2018_paper.pdf


107
Title: Audio to Body Dynamics
Type: Poster
Abstracts: We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Notably, it's not clear if body movement can be predicted from music at all and our aim in this work is to explore this possibility. In this paper, we present the first result that shows that natural body dynamics can be predicted. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation.
Authors: Eli Shlizerman, Facebook;Lucio Dery, Stanford;Hayden Schoen, Facebook;Ira Kemelmacher,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shlizerman_Audio_to_Body_CVPR_2018_paper.pdf


108
Title: The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
Type: Poster
Abstracts: While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ``perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.
Authors: Richard Zhang, UC Berkeley;Phillip Isola, UC Berkeley;Alexei Efros, UC Berkeley;Eli Shechtman, Adobe Research;Oliver Wang, Adobe;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf


109
Title: Frame-Recurrent Video Super-Resolution
Type: Poster
Abstracts: Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results. In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.
Authors: Mehdi S. M. Sajjadi, Max Planck Institute for Intel;Raviteja Vemulapalli, Google;Matthew Brown,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.pdf


110
Title: Deep Mutual Learning
Type: Poster
Abstracts: Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.
Authors: Ying Zhang, QMUL;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;Huchuan Lu, Dalian University of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf


111
Title: Real-world Anomaly Detection in Surveillance Videos
Type: Poster
Authors: Waqas Sultani, ;Chen Chen, University of Central Florida;Mubarak Shah, UCF;

112
Title: Soccer on Your Tabletop
Type: Poster
Abstracts: We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device. At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games. We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage.
Authors: Konstantinos Rematas, University of Washington;Ira Kemelmacher, ;Brian Curless, Washington;Steve Seitz, Washington/Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Rematas_Soccer_on_Your_CVPR_2018_paper.pdf


113
Title: Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification
Type: Poster
Authors: Shuang Li, The Chinese University of HK;Slawomir Bak, Disney Research;Peter Carr, Disney Research;

114
Title: HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN
Type: Poster
Authors: Yue Cao, Tsinghua University;Mingsheng Long, Tsinghua University;Bin Liu, Tsinghua University;Jianmin Wang,;

115
Title: Excitation Backprop for RNNs
Type: Poster
Abstracts: Deep models are state-of-the-art or many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.
Authors: Sarah Bargal, Boston University;Andrea Zunino, Istituto Italiano di Tecnologia;Donghyun Kim, Boston University;Jianming Zhang, Adobe Research;Vittorio Murino, Istituto Italiano di Tecnologia;Stan Sclaroff, Boston University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bargal_Excitation_Backprop_for_CVPR_2018_paper.pdf


116
Title: Dynamic-Structured Semantic Propagation Network
Type: Poster
Abstracts: Semantic concept hierarchy is yet under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into the dense prediction. This lack of modeling dependencies among concepts severely limits the generalization capability of segmentation models for open set large-scale vocabularies. Prior works thus must tune highly-specified models for each task due to the label discrepancy across datasets. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph to explicitly incorporate the concept hierarchy into dynamic network construction, leading to an interpretable reasoning process. Each neuron for one super-class (eg food) represents the instantiated module for recognizing among fine-grained child concepts (eg editable fruit or pizza), and then its learned features flow into the child neurons (eg distinguishing between orange or apple) for hierarchical categorization in finer levels. A dense semantic-enhanced neural block propagates the learned knowledge of all ancestral neurons into each fine-grained child neuron for progressive feature evolving. During training, DSSPN performs the dynamic-structured neuron computational graph by only activating a sub-graph of neurons for each image. Another merit of such semantic explainable structure is the ability to learn a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of DSSPN, and a universal segmentation model that is jointly trained on diverse datasets can surpass the common fine-tuning scheme for exploiting multi-domain knowledge.
Authors: Xiaodan Liang, Carnegie Mellon University;Hongfei Zhou, ;Eric Xing, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Dynamic-Structured_Semantic_Propagation_CVPR_2018_paper.pdf


117
Title: Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation
Type: Spotlight
Abstracts: Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.
Authors: Huaizu Jiang, UMass Amherst;Deqing Sun, NVIDIA;Varun Jampani, NVIDIA Research;Ming-Hsuan Yang, UC Merced;Erik Miller, ;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jiang_Super_SloMo_High_CVPR_2018_paper.pdf


118
Title: Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation
Type: Poster
Abstracts: Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.
Authors: Huaizu Jiang, UMass Amherst;Deqing Sun, NVIDIA;Varun Jampani, NVIDIA Research;Ming-Hsuan Yang, UC Merced;Erik Miller, ;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jiang_Super_SloMo_High_CVPR_2018_paper.pdf


119
Title: SPLATNet: Sparse Lattice Networks for Point Cloud Processing
Type: Oral
Abstracts: We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.
Authors: University of Massachusetts, Amherst;Varun Jampani, NVIDIA Research;Deqing Sun, NVIDIA;Evangelos Kalogerakis, UMass;Subhransu Maji, ;Ming-Hsuan Yang, UC Merced;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf


120
Title: SPLATNet: Sparse Lattice Networks for Point Cloud Processing
Type: Poster
Abstracts: We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.
Authors: University of Massachusetts, Amherst;Varun Jampani, NVIDIA Research;Deqing Sun, NVIDIA;Evangelos Kalogerakis, UMass;Subhransu Maji, ;Ming-Hsuan Yang, UC Merced;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf


121
Title: Video Representation Learning Using Discriminative Pooling
Type: Poster
Abstracts: Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.
Authors: Jue Wang, ANU;Anoop Cherian, ;NICTA, Australia;Stephen Gould, Australian National University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Representation_Learning_CVPR_2018_paper.pdf


122
Title: Attend and Interact: Higher-Order Object Interactions for Video Understanding
Type: Poster
Abstracts: Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs.
Authors: CHIH-YAO MA, GEORGIA TECH;Asim Kadav, NEC Labs;Iain Melvin, ;Zsolt Kira, ;Ghassan AlRegib, ;Hans Peter Graf,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Attend_and_Interact_CVPR_2018_paper.pdf


123
Title: Human Pose Estimation with Parsing Induced Learner
Type: Poster
Authors: Xuecheng Nie, National University of Singapo;Jiashi Feng, ;Yiming Zuo, Tsinghua University;Shuicheng Yan,;

124
Title: 4D Human Body Correspondences from Panoramic Depth Maps
Type: Poster
Authors: Zhong Li, University of Delaware;Minye Wu, ShanghaiTech;Wangyiteng Zhou, ShanghaiTech University;University of Delaware, USA;

125
Title: Recognizing Human Actions as Evolution of Pose Estimation Maps
Type: Poster
Authors: Mengyuan Liu, Nanyang Technological University;Junsong Yuan,;

126
Title: GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning
Type: Poster
Abstracts: In this paper, we propose a GraphBit method to learn deep binary descriptors in a directed acyclic graph unsupervisedly, representing bitwise interactions as edges between the nodes of bits. Conventional binary representation learning methods enforce each element to be binarized into zero or one. However, there are elements lying in the boundary which suffer from doubtful binarization as ``ambiguous bits''. Ambiguous bits fail to collect effective information for confident binarization, which are unreliable and sensitive to noise. We argue that there are implicit inner relationships between bits in binary descriptors, where the related bits can provide extra instruction as prior knowledge for ambiguity elimination. Specifically, we design a deep reinforcement learning model to learn the structure of the graph for bitwise interaction mining, reducing the uncertainty of binary codes by maximizing the mutual information with inputs and related bits, so that the ambiguous bits receive additional instruction from the graph for confident binarization. Due to the reliability of the proposed binary codes with bitwise interaction, we obtain an average improvement of 9.64%, 8.84% and 3.22% on the CIFAR-10, Brown and HPatches datasets respectively compared with the state-of-the-art unsupervised binary descriptors.
Authors: Yueqi Duan, Tsinghua University;Ziwei Wang, Tsinghua University;Jiwen Lu, Tsinghua University;Xudong Lin, Tsinghua University;Jie Zhou,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.pdf


127
Title: Deep Adversarial Metric Learning
Type: Spotlight
Abstracts: Learning an effective distance metric between image pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negatives in the training set usually account for the tiny minority, which may fail to fully describe the distribution of negative samples close to the margin. In this paper, we propose a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the observed negative samples, which is widely applicable to supervised deep metric learning methods. Different from existing metric learning approaches which simply ignore numerous easy negatives, the proposed DAML exploits them to generate potential hard negatives adversary to the learned metric as complements. We simultaneously train the hard negative generator and feature embedding in an adversarial manner, so that more precise distance metrics can be learned with adequate and targeted synthetic hard negatives. Extensive experimental results on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that DAML effectively boosts the performance of existing deep metric learning approaches through adversarial learning.
Authors: Yueqi Duan, Tsinghua University;Wenzhao Zheng, Tsinghua University;Xudong Lin, Tsinghua University;Jiwen Lu, Tsinghua University;Jie Zhou,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf


128
Title: Deep Adversarial Metric Learning
Type: Poster
Abstracts: Learning an effective distance metric between image pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negatives in the training set usually account for the tiny minority, which may fail to fully describe the distribution of negative samples close to the margin. In this paper, we propose a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the observed negative samples, which is widely applicable to supervised deep metric learning methods. Different from existing metric learning approaches which simply ignore numerous easy negatives, the proposed DAML exploits them to generate potential hard negatives adversary to the learned metric as complements. We simultaneously train the hard negative generator and feature embedding in an adversarial manner, so that more precise distance metrics can be learned with adequate and targeted synthetic hard negatives. Extensive experimental results on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that DAML effectively boosts the performance of existing deep metric learning approaches through adversarial learning.
Authors: Yueqi Duan, Tsinghua University;Wenzhao Zheng, Tsinghua University;Xudong Lin, Tsinghua University;Jiwen Lu, Tsinghua University;Jie Zhou,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf


129
Title: Revisiting Video Saliency: A Large-scale Benchmark and a New Model
Type: Poster
Authors: Wenguan Wang, Beijing Institute of Technology;Jianbing Shen, Beijing Institute of Technolog;Fang Guo, Beijing Institute of Technology;Ming-Ming Cheng, Nankai University;Ali Borji, UCF;

130
Title: Graph-Cut RANSAC
Type: Poster
Abstracts: A novel method for robust estimation, called Graph-Cut RANSAC, GC-RANSAC in short, is introduced. To separate inliers and outliers, it runs the graph-cut algorithm in the local optimization (LO) step which is applied when a so-far-the-best model is found. The proposed LO step is conceptually simple, easy to implement, globally optimal and efficient. GC-RANSAC is shown experimentally, both on synthesized tests and real image pairs, to be more geometrically accurate than state-of-the-art methods on a range of problems, e.g. line fitting, homography, affine transformation, fundamental and essential matrix estimation. It runs in real-time for many problems at a speed approximately equal to that of the less accurate alternatives (in milliseconds on standard CPU).
Authors: Daniel Barath, MTA SZTAKI;Jiri Matas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Barath_Graph-Cut_RANSAC_CVPR_2018_paper.pdf


131
Title: Five-point Fundamental Matrix Estimation for Uncalibrated Cameras
Type: Poster
Authors: Daniel Barath, MTA SZTAKI;

132
Title: Hashing as Tie-Aware Learning to Rank
Type: Poster
Abstracts: Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.
Authors: Kun He, Boston University;Fatih Cakir, Boston University;Sarah Bargal, Boston University;Stan Sclaroff, Boston University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/He_Hashing_as_Tie-Aware_CVPR_2018_paper.pdf


133
Title: Optimizing Local Feature Descriptors for Nearest Neighbor Matching
Type: Poster
Authors: Kun He, Boston University;Yan Lu, ;Stan Sclaroff, Boston University;

134
Title: Total Capture: A 3D Deformation Model for Tracking Faces
Type: Oral
Authors: Hanbyul Joo, CMU;Tomas Simon, Oculus Research;Yaser Sheikh,;

135
Title: Total Capture: A 3D Deformation Model for Tracking Faces
Type: Poster
Authors: Hanbyul Joo, CMU;Tomas Simon, Oculus Research;Yaser Sheikh,;

136
Title: Consensus Maximization for Semantic Region Correspondences
Type: Spotlight
Abstracts: We propose a novel method for the geometric registration of semantically labeled regions. We approximate semantic regions by ellipsoids, and leverage their convexity to formulate the correspondence search effectively as a constrained optimization problem that maximizes the number of matched regions, and which we solve globally optimal in a branch-and-bound fashion. To this end, we derive suitable linear matrix inequality constraints which describe ellipsoid-to-ellipsoid assignment conditions. Our approach is robust to large percentages of outliers and thus applicable to difficult correspondence search problems. In multiple experiments we demonstrate the flexibility and robustness of our approach on a number of challenging vision problems.
Authors: Pablo Speciale, ETH;Danda Paudel, ;Martin Oswald, ETH Zurich;Computer Vision Lab, ETH Zurich;Luc Van Gool, KTH;Marc Pollefeys, ETH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Speciale_Consensus_Maximization_for_CVPR_2018_paper.pdf


137
Title: Consensus Maximization for Semantic Region Correspondences
Type: Poster
Abstracts: We propose a novel method for the geometric registration of semantically labeled regions. We approximate semantic regions by ellipsoids, and leverage their convexity to formulate the correspondence search effectively as a constrained optimization problem that maximizes the number of matched regions, and which we solve globally optimal in a branch-and-bound fashion. To this end, we derive suitable linear matrix inequality constraints which describe ellipsoid-to-ellipsoid assignment conditions. Our approach is robust to large percentages of outliers and thus applicable to difficult correspondence search problems. In multiple experiments we demonstrate the flexibility and robustness of our approach on a number of challenging vision problems.
Authors: Pablo Speciale, ETH;Danda Paudel, ;Martin Oswald, ETH Zurich;Computer Vision Lab, ETH Zurich;Luc Van Gool, KTH;Marc Pollefeys, ETH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Speciale_Consensus_Maximization_for_CVPR_2018_paper.pdf


138
Title: ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing
Type: Poster
Abstracts: We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.
Authors: Chen-Hsuan Lin, CMU;Ersin Yumer, Argo AI;Oliver Wang, Adobe;Eli Shechtman, Adobe Research;Simon Lucey,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.pdf


139
Title: Motion-Guided Cascaded Refinement Network for Video Object Segmentation
Type: Poster
Abstracts: Deep CNNs have achieved superior performance in many tasks of computer vision and image understanding. However, it is still difficult to effectively apply deep CNNs to video object segmentation(VOS) since treating video frames as separate and static will lose the information hidden in motion. To tackle this problem, we propose a Motion-guided Cascaded Refinement Network for VOS. By assuming the object motion is normally different from the background motion, for a video frame we first apply an active contour model on optical flow to coarsely segment objects of interest. Then, the proposed Cascaded Refinement Network(CRN) takes the coarse segmentation as guidance to generate an accurate segmentation of full resolution. In this way, the motion information and the deep CNNs can well complement each other to accurately segment objects from video frames. Furthermore, in CRN we introduce a Single-channel Residual Attention Module to incorporate the coarse segmentation map as attention, making our network effective and efficient in both training and testing. We perform experiments on the popular benchmarks and the results show that our method achieves state-of-the-art performance at a much faster speed.
Authors: Ping Hu, ;Gang Wang, ;Xiangfei Kong, Nanyang Technological University;NTU, Singapore;Yap-Peng Tan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Motion-Guided_Cascaded_Refinement_CVPR_2018_paper.pdf


140
Title: Zigzag Learning for Weakly Supervised Object Detection
Type: Poster
Abstracts: This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.
Authors: Xiaopeng Zhang, National University of Singapore;Jiashi Feng, ;Hongkai Xiong, Shanghai Jiao Tong University;Qi Tian,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Zigzag_Learning_for_CVPR_2018_paper.pdf


141
Title: Look
Type: Spotlight
Authors: Jiuxiang Gu, Nanyang Technological Universi;Jianfei Cai, ;Joty Shafiq Rayhan, ;Li Niu, Rice University;Gang Wang,;

142
Title: Look
Type: Poster
Authors: Jiuxiang Gu, Nanyang Technological Universi;Jianfei Cai, ;Joty Shafiq Rayhan, ;Li Niu, Rice University;Gang Wang,;

143
Title: VITON: An Image-based Virtual Try-on Network
Type: Spotlight
Authors: Xintong Han, University of Maryland;Zuxuan Wu, University of Maryland;Zhe Wu, University of Maryland;Ruichi Yu, ;University of Maryland, USA;

144
Title: VITON: An Image-based Virtual Try-on Network
Type: Poster
Authors: Xintong Han, University of Maryland;Zuxuan Wu, University of Maryland;Zhe Wu, University of Maryland;Ruichi Yu, ;University of Maryland, USA;

145
Title: Cross-Domain Self-supervised Multi-task Feature Learning Using Synthetic Game Imagery
Type: Poster
Authors: Zhongzheng Ren, UC Davis;Yong Jae Lee, UC Davis;

146
Title: LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image
Type: Poster
Authors: Chuhang Zou, UIUC;Alex Colburn, Zillow Group Inc.;Qi Shan, Zillow Group;Derek Hoiem,;

147
Title: Thoracic Disease Identification and Localization with Limited Supervision
Type: Poster
Authors: Zhe Li, Syracuse University;Chong Wang, Google Inc;Mei Han, Google Inc;Yuan Xue, Google;Wei Wei, Google Inc.;Li-jia Li, Google Inc;Fei-Fei Li, Google Inc.;

148
Title: Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks
Type: Poster
Abstracts: It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification.
Authors: NTU, Singapore;Xiangfei Kong, Nanyang Technological University;Adobe Systems, Inc.;Gang Wang, ;Jianxiong Yin, NVIDIA;Simon See, NVIDIA;Yap-Peng Tan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kuen_Stochastic_Downsampling_for_CVPR_2018_paper.pdf


149
Title: Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation
Type: Poster
Authors: Jiwoon Ahn, DGIST;Suha Kwak, POSTECH;

150
Title: Deep End-to-End Time-of-Flight Imaging
Type: Poster
Abstracts: We present an end-to-end image processing framework for time-of-flight (ToF) cameras. Existing ToF image processing pipelines consist of a sequence of operations including modulated exposures, denoising, phase unwrapping and multipath interference correction. While this cascaded modular design offers several benefits, such as closed-form solutions and power-efficient processing, it also suffers from error accumulation and information loss as each module can only observe the output from its direct predecessor, resulting in erroneous depth estimates. We depart from a conventional pipeline model and propose a deep convolutional neural network architecture that recovers scene depth directly from dual-frequency, raw ToF correlation measurements. To train this network, we simulate ToF images for a variety of scenes using a time-resolved renderer, devise depth-specific losses, and apply normalization and augmentation strategies to generalize this model to real captures. We demonstrate that the proposed network can efficiently exploit the spatio-temporal structures of ToF frequency measurements, and validate the performance of the joint multipath removal, denoising and phase unwrapping method on a wide range of challenging scenes.
Authors: Shuochen Su, University of British Columbia;Felix Heide, Stanford University;Gordon Wetzstein, ;Wolfgang Heidrich,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Deep_End-to-End_Time-of-Flight_CVPR_2018_paper.pdf


151
Title: Fast and Accurate Online Video Object Segmentation via Tracking Parts
Type: Spotlight
Abstracts: Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.
Authors: Jingchun Cheng, Tsinghua University;Yi-Hsuan Tsai, NEC Labs America;University of California, Merced;Shengjin Wang, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Fast_and_Accurate_CVPR_2018_paper.pdf


152
Title: Fast and Accurate Online Video Object Segmentation via Tracking Parts
Type: Poster
Abstracts: Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.
Authors: Jingchun Cheng, Tsinghua University;Yi-Hsuan Tsai, NEC Labs America;University of California, Merced;Shengjin Wang, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Fast_and_Accurate_CVPR_2018_paper.pdf


153
Title: Min-Entropy Latent Model for Weakly Supervised Object Detection
Type: Poster
Abstracts: Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy is used as a metric to measure the randomness of object localization during learning, as well as serving as a model to learn object locations. It aims to principally reduce the variance of positive instances and alleviate the ambiguity of detectors. MELM is deployed as two sub-models, which respectively discovers and localizes objects by minimizing the global and local entropy. MELM is unified with feature learning and optimized with a recurrent learning algorithm, which progressively transfers the weak supervision to object locations. Experiments demonstrate that MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, against the state-of-the-art approaches.
Authors: Fang Wan, UCAS;Pengxu Wei, ;Jianbin Jiao, ;Zhenjun Han, ;Qixiang Ye,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Min-Entropy_Latent_Model_CVPR_2018_paper.pdf


154
Title: Future Frame Prediction for Anomaly Detection  A New Baseline
Type: Poster
Authors: Wen Liu, ShanghaiTech University;Weixin Luo, Shanghaitech University;Dongze Lian, ShanghaiTech University;Shenghua Gao, ShanghaiTech University;

155
Title: Face Aging with Identity-Preserved Conditional Generative Adversarial Networks
Type: Poster
Authors: Zongwei WANG, ;Xu Tang, ;Weixin Luo, Shanghaitech University;Shenghua Gao, ShanghaiTech University;

156
Title: Learning to Compare: Relation Network for Few-Shot Learning
Type: Poster
Abstracts: We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.
Authors: Flood Sung, Independent Researcher;Yongxin Yang, Queen Mary University of London;Li Zhang, Queen Mary University of London;Tao Xiang, Queen Mary University of London;Phil Torr, Oxford;Timothy Hospedales, University of Edinburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf


157
Title: Deep Layer Aggregation
Type: Oral
Abstracts: Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.
Authors: Fisher Yu, UC Berkeley;Dequan Wang, UC Berkeley;Evan Shelhamer, UC Berkeley;UC Berkeley, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf


158
Title: Deep Layer Aggregation
Type: Poster
Abstracts: Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.
Authors: Fisher Yu, UC Berkeley;Dequan Wang, UC Berkeley;Evan Shelhamer, UC Berkeley;UC Berkeley, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf


159
Title: Style Aggregated Network for Facial Landmark Detection
Type: Poster
Abstracts: Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN
Authors: Xuanyi Dong, UTS;Yan Yan, UTS;Wanli Ouyang, The University of Sydney;Yi Yang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf


160
Title: M3: Multimodal Memory Modelling for Video Captioning
Type: Spotlight
Abstracts: Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress. However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to long-term sequential problems [35] and working memory is the key factor of visual attention [33], we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to [10], the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the state-of-the-art methods in terms of BLEU and METEOR.
Authors: Institute of Automation, Chine;Wei Wang, ;Yan Huang, ;Liang Wang, unknown;Tieniu Tan, NLPR China;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_M3_Multimodal_Memory_CVPR_2018_paper.pdf


161
Title: M3: Multimodal Memory Modelling for Video Captioning
Type: Poster
Abstracts: Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress. However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to long-term sequential problems [35] and working memory is the key factor of visual attention [33], we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to [10], the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the state-of-the-art methods in terms of BLEU and METEOR.
Authors: Institute of Automation, Chine;Wei Wang, ;Yan Huang, ;Liang Wang, unknown;Tieniu Tan, NLPR China;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_M3_Multimodal_Memory_CVPR_2018_paper.pdf


162
Title: Classification Driven Dynamic Image Enhancement
Type: Poster
Authors: Vivek Sharma, Karlsruhe Institute of Technology;Ali Diba, ;Davy Neven, KU Leuven;Michael Brown, York University;Luc Van Gool, KTH;Rainer Stiefelhagen, Karlsruhe Institute of Technology;

163
Title: Generative Image Inpainting with Contextual Attention
Type: Poster
Authors: Jiahui Yu, UIUC;Adobe Systems, Inc.;Jimei Yang, ;Xiaohui Shen, Adobe Research;Xin Lu, ;Thomas Huang,;

164
Title: Iterative Visual Reasoning Beyond Convolutions
Type: Spotlight
Abstracts: We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs in parallel; and a global graph-reasoning module. Our graph has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to class nodes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, eg achieving an $8.4%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.
Authors: Xinlei Chen, Facebook;Li-jia Li, Google Inc;Fei-Fei Li, Google Inc.;Abhinav Gupta,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.pdf


165
Title: Iterative Visual Reasoning Beyond Convolutions
Type: Poster
Abstracts: We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs in parallel; and a global graph-reasoning module. Our graph has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to class nodes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, eg achieving an $8.4%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.
Authors: Xinlei Chen, Facebook;Li-jia Li, Google Inc;Fei-Fei Li, Google Inc.;Abhinav Gupta,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.pdf


166
Title: Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification
Type: Poster
Authors: Jianlou Si, BUPT;Honggang Zhang, ;Chun-Guang Li, Beijing Univ. of Posts&Telecom;NTU, Singapore;Xiangfei Kong, Nanyang Technological University;Alex Kot, ;Gang Wang,;

167
Title: Textbook Question Answering under Teacher Guidance with Memory Networks
Type: Spotlight
Authors: Juzheng Li, Tsinghua University;Hang Su, Tsinghua University;Jun Zhu, Tsinghua University;Siyu Wang, ;Bo Zhang,;

168
Title: Textbook Question Answering under Teacher Guidance with Memory Networks
Type: Poster
Authors: Juzheng Li, Tsinghua University;Hang Su, Tsinghua University;Jun Zhu, Tsinghua University;Siyu Wang, ;Bo Zhang,;

169
Title: Multi-Level Factorisation Net for Person Re-Identification
Type: Poster
Abstracts: Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.
Authors: Xiaobin Chang, Queen Mary Univ. of London;Timothy Hospedales, University of Edinburgh;Tao Xiang, Queen Mary University of London;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.pdf


170
Title: Functional Map of the World
Type: Spotlight
Abstracts: We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a "false detection" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available.
Authors: Gordon Christie, JHU/APL;Neil Fendley, JHU/APL;James Wilson, DigitalGlobe;Ryan Mukherjee, JHU/APL;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Christie_Functional_Map_of_CVPR_2018_paper.pdf


171
Title: Functional Map of the World
Type: Poster
Abstracts: We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a "false detection" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available.
Authors: Gordon Christie, JHU/APL;Neil Fendley, JHU/APL;James Wilson, DigitalGlobe;Ryan Mukherjee, JHU/APL;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Christie_Functional_Map_of_CVPR_2018_paper.pdf


172
Title: A Two-Step Disentanglement Method
Type: Poster
Abstracts: We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/A-Two-Step-Disentanglement-Method.
Authors: Naama Hadad, Tel Aviv University;Tel Aviv University, Israel;Moni Shahar, Tel Aviv University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.pdf


173
Title: Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization
Type: Poster
Abstracts: Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU. Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV.
Authors: Peihua Li, ;Jiangtao Xie, ;Qilong Wang, ;Zilin Gao, Dalian University of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Towards_Faster_Training_CVPR_2018_paper.pdf


174
Title: Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?
Type: Poster
Abstracts: The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch
Authors: Kensho Hara, AIST;Hirokatsu Kataoka, AIST;Yutaka Satoh, AIST;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.pdf


175
Title: Left-Right Comparative Recurrent Model for Stereo Matching
Type: Oral
Abstracts: Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.
Authors: Zequn Jie, ;Pengfei Wang, NUS;Yonggen Ling, Tencent;Bo Zhao, ;Jiashi Feng, ;Wei Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.pdf


176
Title: Left-Right Comparative Recurrent Model for Stereo Matching
Type: Poster
Abstracts: Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.
Authors: Zequn Jie, ;Pengfei Wang, NUS;Yonggen Ling, Tencent;Bo Zhao, ;Jiashi Feng, ;Wei Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.pdf


177
Title: Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input
Type: Oral
Authors: Adel Bibi, KAUST;Modar Alfadly, King Abdullah University of Science and Technology;Bernard Ghanem,;

178
Title: Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input
Type: Poster
Authors: Adel Bibi, KAUST;Modar Alfadly, King Abdullah University of Science and Technology;Bernard Ghanem,;

179
Title: Zero-Shot Sketch-Image Hashing
Type: Spotlight
Abstracts: Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.
Authors: Yuming Shen, University of East Anglia;Li Liu, University of East Anglia;Fumin Shen, ;Ling Shao, University of East Anglia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Zero-Shot_Sketch-Image_Hashing_CVPR_2018_paper.pdf


180
Title: Zero-Shot Sketch-Image Hashing
Type: Poster
Abstracts: Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.
Authors: Yuming Shen, University of East Anglia;Li Liu, University of East Anglia;Fumin Shen, ;Ling Shao, University of East Anglia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Zero-Shot_Sketch-Image_Hashing_CVPR_2018_paper.pdf


181
Title: Interpretable Convolutional Neural Networks
Type: Spotlight
Abstracts: This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e., what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.
Authors: Quanshi Zhang, UCLA;Yingnian Wu, ;Song-Chun Zhu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.pdf


182
Title: Interpretable Convolutional Neural Networks
Type: Poster
Abstracts: This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e., what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.
Authors: Quanshi Zhang, UCLA;Yingnian Wu, ;Song-Chun Zhu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.pdf


183
Title: Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves
Type: Poster
Abstracts: The manifold surface reconstruction in multi-view stereo often fails in retaining thin structures due to incomplete and noisy reconstructed point clouds. In this paper, we address this problem by leveraging spatial curves. The curve representation in nature is advantageous in modeling thin and elongated structures, implying topology and connectivity information of the underlying geometry, which exactly compensates the weakness of scattered point clouds. We present a novel surface reconstruction method using both curves and point clouds. First, we propose a 3D curve reconstruction algorithm based on the initialize-optimize-expand strategy. Then, tetrahedra are constructed from points and curves, where the volumes of thin structures are robustly preserved by the Curve-conformed Delaunay Refinement. Finally, the mesh surface is extracted from tetrahedra by a graph optimization. The method has been intensively evaluated on both synthetic and real-world datasets, showing significant improvements over state-of-the-art methods.
Authors: Shiwei Li, HKUST;Yao Yao, HKUST;Tian Fang, HKUST;The Hong Kong University of Science and Technology, Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Reconstructing_Thin_Structures_CVPR_2018_paper.pdf


184
Title: Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior
Type: Poster
Authors: Daniel S. Jeon, KAIST;Seung-Hwan Baek, KAIST;Inchang Choi, ;Min H. Kim, KAIST;

185
Title: Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB
Type: Poster
Authors: Tomoyuki Suzuki, Keio University;Hirokatsu Kataoka, AIST;Yoshimitsu Aoki, Keio University;Yutaka Satoh, AIST;

186
Title: Generating Synthetic X-ray Images of a Person from the Surface Geometry
Type: Spotlight
Authors: Brian Teixeira, Siemens Healthineers;Vivek Singh, Siemens Healthineers;Kai Ma, Siemens Healthineers;Birgi Tamersoy, Siemens Healthineers;Terrence Chen, Siemens Healthineers;Yifan Wu, Temple University ;Elena Balashova, Princeton University;Dorin Comaniciu, Siemens Healthineers;

187
Title: Generating Synthetic X-ray Images of a Person from the Surface Geometry
Type: Poster
Authors: Brian Teixeira, Siemens Healthineers;Vivek Singh, Siemens Healthineers;Kai Ma, Siemens Healthineers;Birgi Tamersoy, Siemens Healthineers;Terrence Chen, Siemens Healthineers;Yifan Wu, Temple University ;Elena Balashova, Princeton University;Dorin Comaniciu, Siemens Healthineers;

188
Title: Attentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category Classification
Type: Poster
Abstracts: This paper proposes a knowledge-guided fashion network to solve the problem of visual fashion analysis, e.g., fashion landmark localization and clothing category classification. The suggested fashion model is leveraged with high-level human knowledge in this domain. We propose two important fashion grammars: (i) dependency grammar capturing kinematics-like relation, and (ii) symmetry grammar accounting for the bilateral symmetry of clothes. We introduce Bidirectional Convolutional Recurrent Neural Networks (BCRNNs) for efficiently approaching message passing over grammar topologies, and producing regularized landmark layouts. For enhancing clothing category classification, our fashion network is encoded with two novel attention mechanisms, i.e., landmark-aware attention and category-driven attention. The former enforces our network to focus on the functional parts of clothes, and learns domain-knowledge centered representations, leading to a supervised attention mechanism. The latter is goal-driven, which directly enhances task-related features and can be learned in an implicit, top-down manner. Experimental results on large-scale fashion datasets demonstrate the superior performance of our fashion grammar network.
Authors: Wenguan Wang, Beijing Institute of Technology;University of California, Los Angeles;Jianbing Shen, Beijing Institute of Technolog;Song-Chun Zhu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Attentive_Fashion_Grammar_CVPR_2018_paper.pdf


189
Title: Unsupervised CCA
Type: Poster
Authors: Yedid Hoshen, Facebook AI Research (FAIR);Tel Aviv University, Israel;

190
Title: Discovering Point Lights with Intensity Distance Fields
Type: Poster
Authors: Edward Zhang, University of Washington;MIchael Cohen, ;Brian Curless, Washington;

191
Title: Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising
Type: Poster
Authors: Stamatios Lefkimmiatis, Skolkovo Institute of Science;

192
Title: Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints
Type: Poster
Authors: Jiahuan Zhou, Northwestern University;Bing Su, Chinese Academy of Sciences;Northwestern University, USA;

193
Title: Recurrent Pixel Embedding for Instance Grouping
Type: Spotlight
Abstracts: We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.
Authors: University of California, Irvine;Irvine, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kong_Recurrent_Pixel_Embedding_CVPR_2018_paper.pdf


194
Title: Recurrent Pixel Embedding for Instance Grouping
Type: Poster
Abstracts: We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth. This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.
Authors: University of California, Irvine;Irvine, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kong_Recurrent_Pixel_Embedding_CVPR_2018_paper.pdf


195
Title: Recurrent Scene Parsing with Perspective Understanding in the Loop
Type: Poster
Authors: University of California, Irvine;Irvine, USA;

196
Title: Learning to Hash by Discrepancy Minimization
Type: Poster
Authors: Zhixiang Chen, Tsinghua University;Xin Yuan, Tsinghua University;Jiwen Lu, Tsinghua University;Jie Zhou,;

197
Title: Fast End-to-End Trainable Guided Filter
Type: Poster
Abstracts: Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100 times faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks.
Authors: Huikai Wu, CASIA;Shuai Zheng, EBay;Junge Zhang, ;Kaiqi Huang, National Laboratory of Pattern Recognition;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Fast_End-to-End_Trainable_CVPR_2018_paper.pdf


198
Title: Disentangling Structure and Aesthetics for Content-aware Image Completion
Type: Poster
Authors: Andrew Gilbert, University of Surrey;University of Surrey, UK.;Hailin Jin, ;Brian Price,;

199
Title: An Analysis of Scale Invariance in Object Detection - SNIP
Type: Oral
Authors: Bharat Singh, ;University of Maryland, USA;

200
Title: An Analysis of Scale Invariance in Object Detection - SNIP
Type: Poster
Authors: Bharat Singh, ;University of Maryland, USA;

201
Title: CSGNet: Neural Shape Parser for Constructive Solid Geometry
Type: Poster
Abstracts: We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.
Authors: Gopal Sharma, University of Massachusetts;Subhransu Maji, ;Indian Institute of Technology, Kanpu;Difan Liu, UMass Amherst;Evangelos Kalogerakis, UMass;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sharma_CSGNet_Neural_Shape_CVPR_2018_paper.pdf


202
Title: Finding Tiny Faces in the Wild with Generative Adversarial Network
Type: Oral
Authors: Yancheng Bai, Kaust/Iscas;Yongqiang Zhang, Harbin institute of Technology/KAUST;Mingli Ding, ;Bernard Ghanem,;

203
Title: Finding Tiny Faces in the Wild with Generative Adversarial Network
Type: Poster
Authors: Yancheng Bai, Kaust/Iscas;Yongqiang Zhang, Harbin institute of Technology/KAUST;Mingli Ding, ;Bernard Ghanem,;

204
Title: SSNet: Scale Selection Network for Online 3D Action Prediction
Type: Spotlight
Abstracts: In action prediction (early action recognition), the goal is to predict the class label of an ongoing action using its observed part so far. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the time axis. As there are significant temporal scale variations of the observed part of the ongoing action at different progress levels, we propose a novel window scale selection scheme to make our network focus on the performed part of the ongoing action and try to suppress the noise from the previous actions at each time step. Furthermore, an activation sharing scheme is proposed to deal with the overlapping computations among the adjacent steps, which allows our model to run more efficiently. The extensive experiments on two challenging datasets show the effectiveness of the proposed action prediction framework.
Authors: Jun Liu, Nanyang Technological University;Amir Shahroudy, NTU Singapore;Gang Wang, ;Ling-Yu Duan, ;Alex Kot,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_SSNet_Scale_Selection_CVPR_2018_paper.pdf


205
Title: SSNet: Scale Selection Network for Online 3D Action Prediction
Type: Poster
Abstracts: In action prediction (early action recognition), the goal is to predict the class label of an ongoing action using its observed part so far. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the time axis. As there are significant temporal scale variations of the observed part of the ongoing action at different progress levels, we propose a novel window scale selection scheme to make our network focus on the performed part of the ongoing action and try to suppress the noise from the previous actions at each time step. Furthermore, an activation sharing scheme is proposed to deal with the overlapping computations among the adjacent steps, which allows our model to run more efficiently. The extensive experiments on two challenging datasets show the effectiveness of the proposed action prediction framework.
Authors: Jun Liu, Nanyang Technological University;Amir Shahroudy, NTU Singapore;Gang Wang, ;Ling-Yu Duan, ;Alex Kot,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_SSNet_Scale_Selection_CVPR_2018_paper.pdf


206
Title: Integrated facial landmark localization and super-resolution of real-world very low resolution faces in arbitrary poses with GANs
Type: Spotlight
Authors: Adrian Bulat, ;Georgios Tzimiropoulos,;

207
Title: Integrated facial landmark localization and super-resolution of real-world very low resolution faces in arbitrary poses with GANs
Type: Poster
Authors: Adrian Bulat, ;Georgios Tzimiropoulos,;

208
Title: The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation
Type: Poster
Abstracts: Traditional methods of motion segmentation use powerful geometric constraints to understand motion, but fail to leverage the semantics of high-level image understanding. Modern CNN methods of motion analysis, on the other hand, excel at identifying well-known structures, but may not precisely characterize well-known geometric constraints. In this work, we build a new statistical model of rigid motion flow based on classical perspective projection constraints. We then combine piecewise rigid motions into complex deformable and articulated objects, guided by semantic segmentation from CNNs and a second ``object-level" statistical model. This combination of classical geometric knowledge combined with the pattern recognition abilities of CNNs yields excellent performance on a wide range of motion segmentation benchmarks, from complex geometric scenes to camouflaged animals.
Authors: Pia Bideau, University of Massachusets;Aruni RoyChowdhury, University of Massachusetts;Rakesh Radhakrishnan Menon, University of Massachusetts;Erik Miller,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bideau_The_Best_of_CVPR_2018_paper.pdf


209
Title: In-Place Activated BatchNorm for Memory-Optimized Training of DNNs
Type: Poster
Abstracts: In this work we present In-Place Activated Batch Normalization (InPlace-ABN) -- a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report competitive results for COCO-Stuff and set new state-of-the-art results for Cityscapes and Mapillary Vistas. Code can be found at https://github.com/mapillary/inplace_abn.
Authors: Samuel Rota Bulo', Mapillary Research;Lorenzo Porzi, Mapillary Research;Peter Kontschieder,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper.pdf


210
Title: Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks
Type: Poster
Authors: Zhenhua Feng, University of Surrey;Muhammad Awais, university of surrey;Josef Kittler, ;Patrik Huber, University of Surrey;Xiaojun Wu, Jiangnan University;

211
Title: Deep Cross-media Knowledge Transfer
Type: Spotlight
Authors: Xin Huang, Peking University;Yuxin Peng, Peking University;

212
Title: Deep Cross-media Knowledge Transfer
Type: Poster
Authors: Xin Huang, Peking University;Yuxin Peng, Peking University;

213
Title: Coupled End-to-end Transfer Learning with Generalized Fisher Information
Type: Poster
Authors: Shixing Chen, Wayne State University;Caojin Zhang, Wayne State University;Ming Dong,;

214
Title: Knowledge Aided Consistency for Weakly Supervised Phrase Grounding
Type: Poster
Abstracts: Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.
Authors: Kan Chen, Univ. of Southern California;Jiyang Gao, ;Ram Nevatia,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Knowledge_Aided_Consistency_CVPR_2018_paper.pdf


215
Title: Viewpoint-aware Attentive Multi-view Inference for Vehicle Re-identification
Type: Poster
Authors: Yi Zhou, University of East Anglia;Ling Shao, University of East Anglia;

216
Title: MatNet: Modular Attention Network for Referring Expression Comprehension
Type: Poster
Authors: Licheng Yu, UNC Chapel Hill;Adobe Systems, Inc.;Xiaohui Shen, Adobe Research;Jimei Yang, ;Xin Lu, ;Mohit Bansal, UNC Chapel Hill;Tamara Berg, University on North carolina;

217
Title: CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation
Type: Poster
Abstracts: Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it generalizes to unseen data much better. In fact, the results we submitted to the KITTI benchmarks were generated using a classifier trained on the Middlebury dataset.
Authors: Konstantinos Batsos, Stevens Institute of Technolog;Changjiang Cai, ;Philippos Mordohai, Stevens Institute of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.pdf


218
Title: NISP: Pruning Networks using Neuron Importance Score Propagation
Type: Spotlight
Authors: Ruichi Yu, ;Ang Li, Google DeepMind;Chun-Fu (Richard) Chen, IBM T.J. Watson Research Cente;Jui-Hsin Lai, ;Vlad Morariu, University of Maryland;Xintong Han, University of Maryland;Mingfei Gao, University of Maryland;Ching-Yung Lin, ;University of Maryland, USA;

219
Title: NISP: Pruning Networks using Neuron Importance Score Propagation
Type: Poster
Authors: Ruichi Yu, ;Ang Li, Google DeepMind;Chun-Fu (Richard) Chen, IBM T.J. Watson Research Cente;Jui-Hsin Lai, ;Vlad Morariu, University of Maryland;Xintong Han, University of Maryland;Mingfei Gao, University of Maryland;Ching-Yung Lin, ;University of Maryland, USA;

220
Title: Who Let The Dogs Out? Modeling Dog Behavior From Visual Data
Type: Poster
Authors: KIANA EHSANI, 1993;Hessam Bagherinezhad, University of Washington;Joe Redmon, University of Washington;Roozbeh Mottaghi, Allen Institute for Artificial Intelligence;Ali Farhadi,;

221
Title: Efficient Video Object Segmentation via Network Modulation
Type: Poster
Abstracts: Video object segmentation targets segmenting a specific object throughout a video sequence when given only an annotated first frame. Recent deep learning based approaches find it effective to fine-tune a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy that these methods achieve, the fine-tuning process is inefficient and fails to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is trained to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70 times faster than fine-tuning approaches and achieves similar accuracy.
Authors: Linjie Yang, Snap Research;YANRAN WANG, NORTHWESTERN;Xuehan Xiong, Snapchat;Jianchao Yang, Snap;Aggelos Katsaggelos, Northwestern University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Efficient_Video_Object_CVPR_2018_paper.pdf


222
Title: Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision
Type: Poster
Abstracts: Face anti-spoofing is crucial to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. The estimated depth and rPPG are fused to distinguish live vs. spoof faces. Further, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experiments show that our model achieves the state-of-the-art results on both intra- and cross-database testing.
Authors: Yaojie Liu, Michigan State University;Amin Jourabloo, ;Xiaoming Liu, Michigan State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Deep_Models_CVPR_2018_paper.pdf


223
Title: Feedback-prop: Convolutional Neural Network Inference under Partial Evidence
Type: Poster
Authors: Tianlu Wang, 1994;CyberAgent, Inc.;Vicente Ordonez, University of Virginia;

224
Title: A Memory Network Approach for Story-based Temporal Summarization of 360? Videos
Type: Poster
Authors: Sangho Lee, Seoul National University;Jinyoung Sung, Seoul National University;Youngjae Yu, ;Gunhee Kim, Carnegie Mellon University;

225
Title: Improving Occlusion and Hard Negative Handling for Single-Stage Object Detectors
Type: Poster
Authors: Junhyug Noh, Seoul National University;Soochan Lee, ;Beomsu Kim, ;Gunhee Kim, Carnegie Mellon University;

226
Title: UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition
Type: Poster
Authors: Jiankang Deng, Imperial College London;Shiyang Cheng, Imperial College London;Niannan Xue, Imperial College London;Yuxiang Zhou, Imperial College;Stefanos Zafeiriou, Imperial College London;

227
Title: Learning a Toolchain for Image Restoration
Type: Spotlight
Authors: Ke Yu, CUHK;Chao Dong, Sensetime Co. Ltd ;Chen-Change Loy, the Chinese University of Hong Kong;

228
Title: Learning a Toolchain for Image Restoration
Type: Poster
Authors: Ke Yu, CUHK;Chao Dong, Sensetime Co. Ltd ;Chen-Change Loy, the Chinese University of Hong Kong;

229
Title: Learning to Act Properly: Predicting and Explaining Affordances from Images
Type: Poster
Authors: Ching-Yao Chuang, University of Toronto;Jiaman Li, University of Toronto;Antonio Torralba, MIT;Sanja Fidler,;

230
Title: Learning a Discriminative Feature Network for Semantic Segmentation
Type: Poster
Abstracts: Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.
Authors: Changqian Yu, HUST;Jingbo Wang, Peking University;Chao Peng, Megvii;Changxin Gao, HUST;Gang Yu, Face++;Nong Sang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_a_Discriminative_CVPR_2018_paper.pdf


231
Title: Optimizing Video Object Detection via a Scale-Time Lattice
Type: Poster
Abstracts: High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that re- quire detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed tradeoff.
Authors: Kai Chen, CUHK;Jiaqi Wang, CUHK;Shuo Yang, ;Xingcheng Zhang, CUHK;Yuanjun Xiong, Amazon ;Chen-Change Loy, the Chinese University of Hong Kong;Dahua Lin, CUHK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Optimizing_Video_Object_CVPR_2018_paper.pdf


232
Title: ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
Type: Poster
Abstracts: We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet~cite{howard2017mobilenets} on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $sim$13$ imes$ actual speedup over AlexNet while maintaining comparable accuracy.
Authors: Xiangyu Zhang, Megvii Inc;Xinyu Zhou, Megvii Technology Inc.;Mengxiao Lin, Megvii Technology Ltd.(Face++);Jian Sun,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf


233
Title: Cascaded Pyramid Network for Multi-Person Pose Estimation
Type: Poster
Abstracts: The topic of multi-person pose estimation has beenlargely improved recently, especially with the developmentof convolutional neural network. However, there still exista lot of challenging cases, such as occluded keypoints, in-visible keypoints and complex background, which cannot bewell addressed. In this paper, we present a novel networkstructure called Cascaded Pyramid Network (CPN) whichtargets to relieve the problem from these “hard” keypoints.More specifically, our algorithm includes two stages: Glob-alNet and RefineNet. GlobalNet is a feature pyramid net-work which can successfully localize the “simple” key-points like eyes and hands but may fail to precisely rec-ognize the occluded or invisible keypoints. Our RefineNettries explicitly handling the “hard” keypoints by integrat-ing all levels of feature representations from the Global-Net together with an online hard keypoint mining loss. Ingeneral, to address the multi-person pose estimation prob-lem, a top-down pipeline is adopted to first generate a setof human bounding boxes based on a detector, followed byour CPN for keypoint localization in each human boundingbox. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with averageprecision at 73.0 on the COCO test-dev dataset and 72.1 onthe COCO test-challenge dataset, which is a 19% relativeimprovement compared with 60.5 from the COCO 2016 key-point challenge. Code and the detection results for personused will be publicly available for further research.
Authors: Yilun Chen, Beihang University;Zhicheng Wang, Megvii(Face++);Yuxiang Peng, Tsinghua University;Zhiqiang Zhang, HUST;Gang Yu, Face++;Jian Sun,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.pdf


234
Title: Seeing Temporal Modulation of Lights from Standard Cameras
Type: Poster
Authors: Naoki Sakakibara, Nagoya Institute of Technology;Fumihiko Sakaue, Nagoya Institute of Technology;JUN SATO, Nagoya Institute of Technology;

235
Title: Point-wise Convolutional Neural Networks
Type: Poster
Authors: Binh-Son Hua, SUTD;Khoi Tran, SUTD;Sai-Kit Yeung,;

236
Title: Fine-grained Video Captioning for Sports Narrative
Type: Spotlight
Authors: Huanyu Yu, Shanghai Jiao Tong University;Shuo Cheng, SJTU;Bingbing Ni, ;Minsi Wang, Shanghai Jiao Tong University;Zhang Jian, Shanghai Jiao Tong University;Xiaokang Yang,;

237
Title: Fine-grained Video Captioning for Sports Narrative
Type: Poster
Authors: Huanyu Yu, Shanghai Jiao Tong University;Shuo Cheng, SJTU;Bingbing Ni, ;Minsi Wang, Shanghai Jiao Tong University;Zhang Jian, Shanghai Jiao Tong University;Xiaokang Yang,;

238
Title: Dense 3D Regression for Hand Pose Estimation
Type: Poster
Abstracts: We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-arts based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D direction vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes and explicitly handles the global 3D estimation in consensus with pixel-wise 2D and 3D estimations. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-arts by a large margin. On ICVL hand dataset, our method achieves similar accuracy compared to the state-of-art which is nearly saturated and outperforms other state-of-arts. Code will be made available.
Authors: Chengde Wan, ;Thomas Probst, ;Luc Van Gool, KTH;Angela Yao, University of Bonn;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Dense_3D_Regression_CVPR_2018_paper.pdf


239
Title: Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space
Type: Poster
Authors: Tatsuya Yokota, Nagoya Institute of Technology;Burak Erem, ;Seyhmus Guler, ;Simon Warfield, Harvard Medical School;Hidekata Hontani,;

240
Title: Learning Convolutional Networks for Content-weighted Image Compression
Type: Poster
Authors: Mu LI, PolyU;Wangmeng Zuo, Harbin Institute of Technology;Shuhang Gu, ;debin Zhao, ;David Zhang, Hong Kong Polytechnic University;

241
Title: Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking
Type: Poster
Abstracts: Offline training for object tracking has recently shown great potentials in balancing tracking accuracy and speed. However, it is still difficult to adapt an offline trained model to a target tracked online. This work presents a Residual Attentional Siamese Network (RASNet) for high performance object tracking. The RASNet model reformulates the correlation filter within a Siamese tracking framework, and introduces different kinds of the attention mechanisms to adapt the model without updating the model online. In particular, by exploiting the offline trained general attention, the target adapted residual attention, and the channel favored feature attention, the RASNet not only mitigates the over-fitting problem in deep network training, but also enhances its discriminative capacity and adaptability due to the separation of representation learning and discriminator learning. The proposed deep architecture is trained from end to end and takes full advantage of the rich spatial temporal information to achieve robust visual tracking. Experimental results on two latest benchmarks, OTB-2015 and VOT2017, show that the RASNet tracker has the state-of-the-art tracking accuracy while runs at more than 80 frames per second.
Authors: Qiang Wang, CASIA;Zhu Teng, Beijing Jiaotong University;Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;Weiming Hu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf


242
Title: Deep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age Estimation
Type: Poster
Abstracts: Facial age estimation from a face image is an important yet very challenging task in computer vision, since humans with different races and/or genders, exhibit quite different patterns in their facial aging processes. To deal with the influence of race and gender, previous methods perform age estimation within each population separately. In practice, however, it is often very difficult to collect and label sufficient data for each population. Therefore, it would be helpful to exploit an existing large labeled dataset of one (source) population to improve the age estimation performance on another (target) population with only a small labeled dataset available. In this work, we propose a Deep Cross-Population (DCP) age estimation model to achieve this goal. In particular, our DCP model develops a two-stage training strategy. First, a novel cost-sensitive multi-task loss function is designed to learn transferable aging features by training on the source population. Second, a novel order-preserving pair-wise loss function is designed to align the aging features of the two populations. By doing so, our DCP model can transfer the knowledge encoded in the source population to the target population. Extensive experiments on the two of the largest benchmark datasets show that our DCP model outperforms several strong baseline methods and many state-of-the-art methods.
Authors: Kai Li, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;Chi Su, KingSoft;Weiming Hu, ;Yundong Zhang, Vimicro Corporation;Stephen Maybank, Birkbeck University of London;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Deep_Cost-Sensitive_and_CVPR_2018_paper.pdf


243
Title: First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations
Type: Poster
Authors: Guillermo Garcia-Hernando, Imperial College London;Shanxin Yuan, Imperial College London;Seungryul Baek, Imperial College London;Tae-Kyun Kim, Imperial College London;

244
Title: Hand PointNet: 3D Hand Pose Estimation using Point Sets
Type: Spotlight
Authors: Liuhao Ge, NTU;Junwu Weng, Nanyang Technological Univ.;Yujun Cai, NTU;Junsong Yuan, Nanyang Technological University;

245
Title: Hand PointNet: 3D Hand Pose Estimation using Point Sets
Type: Poster
Authors: Liuhao Ge, NTU;Junwu Weng, Nanyang Technological Univ.;Yujun Cai, NTU;Junsong Yuan, Nanyang Technological University;

246
Title: Recovering Realistic Texture in Image Super-resolution by Spatial Feature Modulation
Type: Poster
Authors: Xintao Wang, CUHK University;Ke Yu, CUHK;Chao Dong, Sensetime Co. Ltd ;Chen-Change Loy, the Chinese University of Hong Kong;

247
Title: Cube Padding for Weakly-Supervised Saliency Prediction in 360$^{\circ}$ Videos
Type: Poster
Authors: Hsien-Tzu Cheng, National Tsing Hua University;Chun-Hung Chao, ;Jin-Dong Dong, ;Hao-Kai Wen, ;Tyng-Luh Liu, IIS/Academia Sinica;Min Sun, University of Washington;

248
Title: A Face to Face Neural Conversation Model
Type: Poster
Authors: Hang Chu, University of Toronto;Sanja Fidler,;

249
Title: SurfConv: Bridging 3D and 2D Convolution for RGBD Images
Type: Poster
Abstracts: The last few years have seen approaches trying to combine the increasing popularity of depth sensors and the success of the convolutional neural networks. Using depth as additional channel alongside the RGB input has the scale variance problem present in image convolution based approaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor. Instead, we propose SurfConv, which “slides” compact 2D filters along the visible 3D surface. SurfConv is formulated as a simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D4) scheme. We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets. Our method achieves state-of-the-art performance while using less than 30% parameters used by the 3D convolution based approaches.
Authors: Hang Chu, University of Toronto;Wei-Chiu Ma, MIT;Kaustav Kundu, University of Toronto;Raquel Urtasun, University of Toronto;Sanja Fidler,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.pdf


250
Title: Dynamic Video Segmentation Network
Type: Poster
Abstracts: In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation. DVSNet consists of two convolutional neural networks: a segmentation network and a flow network. The former generates highly accurate semantic segmentations, but is deeper and slower. The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations. We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score. Frame regions with a higher expected confidence score traverse the flow network. Frame regions with a lower expected confidence score have to pass through the segmentation network. We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network. The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset. DVSNet is also able to reduce up to 95% of the computational workloads.
Authors: Yu-Shuan Xu, National Tsing Hua University;Chun-Yi Lee, National Tsing Hua University;TSUJUI FU, NTHUCS;HsuanKung Yang, National Tsing Hua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Dynamic_Video_Segmentation_CVPR_2018_paper.pdf


251
Title: Multiple Granularity Group Interaction Prediction
Type: Poster
Abstracts: Most human activity analysis works (i.e., recognition or　prediction) only focus on a single granularity, i.e., either　modelling global motion based on the coarse level movement such as human trajectories or　forecasting future detailed action based on body parts’ movement such as skeleton motion. In contrast, in this work, we propose a multi-granularity interaction prediction network which integrates　both global motion and detailed local action. Built on a bi- directional LSTM network, the　proposed method possesses　between granularities links which encourage feature sharing as well as cross-feature consistency between both global　and local granularity (e.g., trajectory or local action), and in turn predict long-term global location and local dynamics of each individual. We validate our method on several　public datasets with promising performance.
Authors: Taiping Yao, Shanghai Jiaotong University;Minsi Wang, Shanghai Jiao Tong University;Huawei Wei, Shanghai Jiao Tong University;Bingbing Ni, ;Xiaokang Yang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yao_Multiple_Granularity_Group_CVPR_2018_paper.pdf


252
Title: Visual Question Reasoning on General Dependency Tree
Type: Spotlight
Abstracts: The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.
Authors: Qingxing Cao, Sun Yat-Sen University;Xiaodan Liang, Carnegie Mellon University;Bailin Li, SUN-YAT SEN UNIVERSITY;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Visual_Question_Reasoning_CVPR_2018_paper.pdf


253
Title: Visual Question Reasoning on General Dependency Tree
Type: Poster
Abstracts: The collaborative reasoning for understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.
Authors: Qingxing Cao, Sun Yat-Sen University;Xiaodan Liang, Carnegie Mellon University;Bailin Li, SUN-YAT SEN UNIVERSITY;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Visual_Question_Reasoning_CVPR_2018_paper.pdf


254
Title: From Lifestyle VLOGs to Everyday Interactions
Type: Poster
Authors: David Fouhey, UC Berkeley;WEICHENG KUO, Berkeley;Alexei Efros, UC Berkeley;Jitendra Malik,;

255
Title: COCO-Stuff: Thing and Stuff Classes in Context
Type: Poster
Abstracts: Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.
Authors: Holger Caesar, University of Edinburgh;Jasper Uijlings, Google;Vitto Ferrari,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf


256
Title: GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB
Type: Spotlight
Authors: Franziska Mueller, MPI Informatics;Florian Bernard, MPI Informatics;Oleksandr Sotnychenko, MPI Informatics;Dushyant Mehta, MPI For Informatics;Srinath Sridhar, ;Dan Casas, MPI;Christian Theobalt, MPI Informatics;

257
Title: GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB
Type: Poster
Authors: Franziska Mueller, MPI Informatics;Florian Bernard, MPI Informatics;Oleksandr Sotnychenko, MPI Informatics;Dushyant Mehta, MPI For Informatics;Srinath Sridhar, ;Dan Casas, MPI;Christian Theobalt, MPI Informatics;

258
Title: Non-local Neural Networks
Type: Poster
Authors: Xiaolong Wang, Carnegie Mellon University;Ross Girshick, ;Abhinav Gupta, ;Kaiming He,;

259
Title: Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs
Type: Poster
Authors: Xiaolong Wang, Carnegie Mellon University;Yufei Ye, Carnegie Mellon University;Abhinav Gupta,;

260
Title: Taskonomy: Disentangling Task Transfer Learning
Type: Oral
Abstracts: Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity. This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.
Authors: Alexander Sax, Stanford University;William Shen, ;Stanford, UC Berkeley;Jitendra Malik, ;Silvio Savarese, ;Leonidas J. Guibas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.pdf


261
Title: Taskonomy: Disentangling Task Transfer Learning
Type: Poster
Abstracts: Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity. This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.
Authors: Alexander Sax, Stanford University;William Shen, ;Stanford, UC Berkeley;Jitendra Malik, ;Silvio Savarese, ;Leonidas J. Guibas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.pdf


262
Title: Embodied Real-World Active Perception
Type: Spotlight
Authors: Fei Xia, Stanford University;Stanford, UC Berkeley;Zhi-Yang He, Stanford University;Alexander Sax, Stanford University;Jitendra Malik, ;Silvio Savarese,;

263
Title: Embodied Real-World Active Perception
Type: Poster
Authors: Fei Xia, Stanford University;Stanford, UC Berkeley;Zhi-Yang He, Stanford University;Alexander Sax, Stanford University;Jitendra Malik, ;Silvio Savarese,;

264
Title: SfSNet : Learning Shape
Type: Spotlight
Authors: Soumyadip Sengupta, University of Maryland;Angjoo Kanazawa, University of Maryland;Carlos Castillo, ;David Jacobs, University of Maryland;

265
Title: SfSNet : Learning Shape
Type: Poster
Authors: Soumyadip Sengupta, University of Maryland;Angjoo Kanazawa, University of Maryland;Carlos Castillo, ;David Jacobs, University of Maryland;

266
Title: End-to-end Recovery of Human Shape and Pose
Type: Poster
Authors: Angjoo Kanazawa, University of Maryland;Michael Black, Max Planck Institute for Intelligent Systems;David Jacobs, University of Maryland;Jitendra Malik,;

267
Title: Factoring Shape
Type: Poster
Authors: Shubham Tulsiani, UC Berkeley;David Fouhey, UC Berkeley;Saurabh Gupta, ;Alexei Efros, UC Berkeley;Jitendra Malik,;

268
Title: Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction
Type: Poster
Authors: Shubham Tulsiani, UC Berkeley;Alexei Efros, UC Berkeley;Jitendra Malik,;

269
Title: A Fast Resection-Intersection Method for the Known Rotation Problem
Type: Poster
Abstracts: The known rotation problem refers to a special case of structure-from-motion where the absolute orientations of the cameras are known. When formulated as a minimax (l_infty) problem on reprojection errors, the problem is an instance of pseudo-convex programming. Though theoretically tractable, solving the known rotation problem on large-scale data (1,000’s of views, 10,000’s scene points) using existing methods can be very time-consuming. In this paper, we devise a fast algorithm for the known rotation problem. Our approach alternates between pose estimation and triangulation (i.e., resection-intersection) to break the problem into multiple simpler instances of pseudo-convex programming. The key to the vastly superior performance of our method lies in using a novel minimum enclosing ball (MEB) technique for the calculation of updating steps, which obviates the need for convex optimisation routines and greatly reduces memory footprint. We demonstrate the practicality of our method on large-scale problem instances which easily overwhelm current state-of-the-art algorithms (demo program available in supplementary).
Authors: Qianggong Zhang, The University of Adelaide;Tat-Jun Chin, ;Huu Le, The University of Adelaide;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_A_Fast_Resection-Intersection_CVPR_2018_paper.pdf


270
Title: Image Generation from Scene Graphs
Type: Poster
Authors: Justin Johnson, Stanford University;Agrim Gupta, Stanford University;Fei-Fei Li, Stanford University;

271
Title: What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets
Type: Spotlight
Abstracts: The ability to capture temporal information has been critical to the development of video understanding models. While there have been numerous attempts at modeling motion in videos, an explicit analysis of the effect of temporal information for video understanding is still missing. In this work, we aim to bridge this gap and ask the following question: How important is the motion in the video for recognizing the action? To this end, we propose two novel frameworks: (i) class-agnostic temporal generator and (ii) motion-invariant frame selector to reduce/remove motion for an ablation analysis without introducing other artifacts. This isolates the analysis of motion from other aspects of the video. The proposed frameworks provide a much tighter estimate of the effect of motion (from 25% to 6% on UCF101 and 15% to 5% on Kinetics) compared to baselines in our analysis. Our analysis provides critical insights about existing models like C3D, and how it could be made to achieve comparable results with a sparser set of frames.
Authors: De-An Huang, Stanford University;Vignesh Ramanathan, Facebook;Dhruv Mahajan, ;Juan Carlos Niebles, Stanford University;Fei-Fei Li, Stanford University;Darthmout College, USA;Manohar Paluri,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_What_Makes_a_CVPR_2018_paper.pdf


272
Title: What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets
Type: Poster
Abstracts: The ability to capture temporal information has been critical to the development of video understanding models. While there have been numerous attempts at modeling motion in videos, an explicit analysis of the effect of temporal information for video understanding is still missing. In this work, we aim to bridge this gap and ask the following question: How important is the motion in the video for recognizing the action? To this end, we propose two novel frameworks: (i) class-agnostic temporal generator and (ii) motion-invariant frame selector to reduce/remove motion for an ablation analysis without introducing other artifacts. This isolates the analysis of motion from other aspects of the video. The proposed frameworks provide a much tighter estimate of the effect of motion (from 25% to 6% on UCF101 and 15% to 5% on Kinetics) compared to baselines in our analysis. Our analysis provides critical insights about existing models like C3D, and how it could be made to achieve comparable results with a sparser set of frames.
Authors: De-An Huang, Stanford University;Vignesh Ramanathan, Facebook;Dhruv Mahajan, ;Juan Carlos Niebles, Stanford University;Fei-Fei Li, Stanford University;Darthmout College, USA;Manohar Paluri,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_What_Makes_a_CVPR_2018_paper.pdf


273
Title: PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation
Type: Poster
Abstracts: We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform on par or better than the state-of-the-art on these diverse datasets without any dataset-specific model tuning.
Authors: Danfei Xu, Stanford Univesity;dragomir Anguelov, Zoox Inc.;Ashesh Jain, Zoox Inc.;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf


274
Title: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
Type: Oral
Authors: Ting-Chun Wang, NVIDIA;Ming-Yu Liu, NVIDIA;Jun-Yan Zhu, UC Berkeley;Andrew Tao, NVIDIA;Bryan Catanzaro, NVIDIA;Jan Kautz, NVIDIA;

275
Title: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
Type: Poster
Authors: Ting-Chun Wang, NVIDIA;Ming-Yu Liu, NVIDIA;Jun-Yan Zhu, UC Berkeley;Andrew Tao, NVIDIA;Bryan Catanzaro, NVIDIA;Jan Kautz, NVIDIA;

276
Title: Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
Type: Poster
Authors: Agrim Gupta, Stanford University;Justin Johnson, Stanford University;Fei-Fei Li, Stanford University;Silvio Savarese, ;Alexandre Alahi, EPFL;

277
Title: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
Type: Spotlight
Abstracts: The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based visual recognition models call for efficient on-device inference schemes. We propose a quantization scheme along with a co-designed training procedure allowing inference to be carried out using integer-only arithmetic while preserving an end-to-end model accuracy that is close to floating-point inference. Inference using integer-only arithmetic performs better than floating-point arithmetic on typical ARM CPUs and can be implemented on integer-arithmetic-only hardware such as mobile accelerators (e.g. Qualcomm Hexagon). By quantizing both activations and weights as 8-bit integers, we obtain a close to 4x memory footprint reduction compared to 32-bit floating-point representations. Even on MobileNets, a model family known for runtime efficiency, our quantization approach results in an improved tradeoff between latency and accuracy on popular ARM CPUs for ImageNet classification and COCO detection.
Authors: Benoit Jacob, Google;Skirmantas Kligys, Google;Bo Chen, Google;Matthew Tang, Google;Menglong Zhu, ;Andrew Howard, Google;Dmitry Kalenichenko, Google;Hartwig Adam, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf


278
Title: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
Type: Poster
Abstracts: The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based visual recognition models call for efficient on-device inference schemes. We propose a quantization scheme along with a co-designed training procedure allowing inference to be carried out using integer-only arithmetic while preserving an end-to-end model accuracy that is close to floating-point inference. Inference using integer-only arithmetic performs better than floating-point arithmetic on typical ARM CPUs and can be implemented on integer-arithmetic-only hardware such as mobile accelerators (e.g. Qualcomm Hexagon). By quantizing both activations and weights as 8-bit integers, we obtain a close to 4x memory footprint reduction compared to 32-bit floating-point representations. Even on MobileNets, a model family known for runtime efficiency, our quantization approach results in an improved tradeoff between latency and accuracy on popular ARM CPUs for ImageNet classification and COCO detection.
Authors: Benoit Jacob, Google;Skirmantas Kligys, Google;Bo Chen, Google;Matthew Tang, Google;Menglong Zhu, ;Andrew Howard, Google;Dmitry Kalenichenko, Google;Hartwig Adam, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf


279
Title: Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video
Type: Oral
Authors: De-An Huang, Stanford University;Shyamal Buch, Stanford University;Lucio Dery, Stanford University;Animesh Garg, Stanford University;Fei-Fei Li, Stanford University;Juan Carlos Niebles, Stanford University;

280
Title: Finding It: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video
Type: Poster
Authors: De-An Huang, Stanford University;Shyamal Buch, Stanford University;Lucio Dery, Stanford University;Animesh Garg, Stanford University;Fei-Fei Li, Stanford University;Juan Carlos Niebles, Stanford University;

281
Title: Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns
Type: Poster
Authors: Jianming Lv, South China University of Technology;Weihang Chen, South China University of Technology;Qing Li, City University of Hong Kong;Can Yang, South China University of Technology;

282
Title: Kernelized Subspace Pooling for Deep Local Descriptors
Type: Poster
Abstracts: Representing local image patches in an invariant and discriminative manner is an active research topic in computer vision. It has recently been demonstrated that local feature learning based on deep Convolutional Neural Network (CNN) can significantly improve the matching performance. Previous works on learning such descriptors have focused on developing various loss functions, regularizations and data mining strategies to learn discriminative CNN representations. Such methods, however, have little analysis on how to increase geometric invariance of their generated descriptors. In this paper, we propose a descriptor that has both highly invariant and discriminative power. The abilities come from a novel pooling method, dubbed Subspace Pooling (SP) which is invariant to a range of geometric deformations. To further increase the discriminative power of our descriptor, we propose a simple distance kernel integrated to the marginal triplet loss that helps to focus on hard examples in CNN training. Finally, we show that by combining SP with the projection distance metric, the generated feature descriptor is equivalent to that of the Bilinear CNN model, but outperforms the latter with much lower memory and computation consumptions. The proposed method is simple, easy to understand and achieves good performance. Experimental results on several patch matching benchmarks show that our method outperforms the state-of-the-arts significantly.
Authors: Xing Wei, Xi'an Jiaotong University;Yihong Gong, Xi'an Jiaotong University;IAIR,Xi'an Jiaotong University;Nanning Zheng, Xi'an Jiaotong University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Kernelized_Subspace_Pooling_CVPR_2018_paper.pdf


283
Title: Video Rain Removal By Multiscale Convolutional Sparse Coding
Type: Poster
Authors: Li Minghan, Xi'an Jiaotong University;Qi Xie, ;Qian Zhao, ;Wei Wei, Xi'an Jiaotong University;Shuhang Gu, ;Jing Tao, ;Deyu Meng, Xi'an Jiaotong University;

284
Title: Learning from Millions of 3D Scans for Large-scale 3D Face Recognition
Type: Poster
Authors: Syed Zulqarnain Gilani, The University of Western Aust;Ajmal Mian, UWA;

285
Title: Referring Relationships
Type: Poster
Abstracts: Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these "referring relationships" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories.
Authors: Ranjay Krishna, Stanford University;Ines Chami, Stanford University;Michael Bernstein, Stanford University;Fei-Fei Li, Stanford University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf


286
Title: Improving Object Localization with Fitness NMS and Bounded IoU Loss
Type: Poster
Authors: Lachlan Tychsen-Smith, CSIRO (Data61);Lars Petersson,;

287
Title: Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination
Type: Spotlight
Authors: Zhirong Wu, UC Berkeley;Yuanjun Xiong, Amazon ;Stella Yu, UC Berkeley / ICSI;Dahua Lin, CUHK;

288
Title: Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination
Type: Poster
Authors: Zhirong Wu, UC Berkeley;Yuanjun Xiong, Amazon ;Stella Yu, UC Berkeley / ICSI;Dahua Lin, CUHK;

289
Title: CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization
Type: Spotlight
Abstracts: The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets.
Authors: Sixing Hu, NUS;Mengdan Feng, NUS;Rang Nguyen, National Uni. of Singapore;Gim Hee Lee, National University of SIngapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.pdf


290
Title: CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization
Type: Poster
Abstracts: The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets.
Authors: Sixing Hu, NUS;Mengdan Feng, NUS;Rang Nguyen, National Uni. of Singapore;Gim Hee Lee, National University of SIngapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.pdf


291
Title: Visual Question Generation as Dual Task of Visual Question Answering
Type: Spotlight
Abstracts: Visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, but they are usually explored separately despite their intrinsic complementary relationship. In this paper, we propose an end-to-end unified model, the Invertible Question Answering Network (iQAN), to introduce question generation as a dual task of question answering to improve the VQA performance. With our proposed invertible bilinear fusion module and parameter sharing scheme, our iQAN can accomplish VQA and its dual task VQG simultaneously. By jointly trained on two tasks with our proposed dual regularizers~(termed as Dual Training), our model has a better understanding of the interactions among images, questions and answers. After training, iQAN can take either question or answer as input, and output the counterpart. Evaluated on the CLEVR and VQA2 datasets, our iQAN improves the top-1 accuracy of the prior art MUTAN VQA method by 1.33% and 0.88% (absolute increase). We also show that our proposed dual training framework can consistently improve model performances of many popular VQA architectures.
Authors: Yikang Li, ;Nan Duan, Microsoft;Bolei Zhou, Massachuate Institute of Technology;Xiao Chu, Baidu;Wanli Ouyang, The University of Sydney;Xiaogang Wang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Visual_Question_Generation_CVPR_2018_paper.pdf


292
Title: Visual Question Generation as Dual Task of Visual Question Answering
Type: Poster
Abstracts: Visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, but they are usually explored separately despite their intrinsic complementary relationship. In this paper, we propose an end-to-end unified model, the Invertible Question Answering Network (iQAN), to introduce question generation as a dual task of question answering to improve the VQA performance. With our proposed invertible bilinear fusion module and parameter sharing scheme, our iQAN can accomplish VQA and its dual task VQG simultaneously. By jointly trained on two tasks with our proposed dual regularizers~(termed as Dual Training), our model has a better understanding of the interactions among images, questions and answers. After training, iQAN can take either question or answer as input, and output the counterpart. Evaluated on the CLEVR and VQA2 datasets, our iQAN improves the top-1 accuracy of the prior art MUTAN VQA method by 1.33% and 0.88% (absolute increase). We also show that our proposed dual training framework can consistently improve model performances of many popular VQA architectures.
Authors: Yikang Li, ;Nan Duan, Microsoft;Bolei Zhou, Massachuate Institute of Technology;Xiao Chu, Baidu;Wanli Ouyang, The University of Sydney;Xiaogang Wang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Visual_Question_Generation_CVPR_2018_paper.pdf


293
Title: Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation
Type: Spotlight
Authors: Yunchao Wei, ;Huaxin Xiao, ;Honghui Shi, UIUC;Zequn Jie, ;Jiashi Feng, ;Thomas Huang,;

294
Title: Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation
Type: Poster
Authors: Yunchao Wei, ;Huaxin Xiao, ;Honghui Shi, UIUC;Zequn Jie, ;Jiashi Feng, ;Thomas Huang,;

295
Title: Learning Dual Convolutional Neural Networks for Low-Level Vision
Type: Poster
Abstracts: In this paper, we propose a general dual convolutional neural network (DualCNN) for low-level vision problems, e.g., super-resolution, edge-preserving filtering, deraining and dehazing. These problems usually involve the estimation of two components of the target signals: structures and details. Motivated by this, our proposed DualCNN consists of two parallel branches, which respectively recovers the structures and details in an end-to-end manner. The recovered structures and details can generate the target signals according to the formation model for each particular application. The DualCNN is a flexible framework for low-level vision tasks and can be easily incorporated with existing CNNs. Experimental results show that the DualCNN can be effectively applied to numerous low-level vision tasks with favorable performance against the state-of-the-art methods.
Authors: Jinshan Pan, UC Merced;Sifei Liu, ;Deqing Sun, NVIDIA;Jiawei Zhang, City University of Hong Kong;Yang Liu, DUT;Jimmy Ren, SenseTime Group Limited;Zechao Li, Nanjing University of Science and Technology ;Jinhui Tang, ;Huchuan Lu, Dalian University of Technology;Yu-Wing Tai, Tencent YouTu;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Learning_Dual_Convolutional_CVPR_2018_paper.pdf


296
Title: Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation
Type: Poster
Abstracts: Video super-resolution (VSR) has become even more important recently to provide high resolution (HR) contents for ultra high definition displays. While many deep learning based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and compensation. We introduce a fundamentally different framework for VSR in this paper. We propose a novel end-to-end deep neural network that generates dynamic upsampling filters and a residual image, which are computed depending on the local spatio-temporal neighborhood of each pixel to avoid explicit motion compensation. With our approach, an HR image is reconstructed directly from the input image using the dynamic upsampling filters, and the fine details are added through the computed residual. Our network with the help of a new data augmentation technique can generate much sharper HR videos with temporal consistency, compared with the previous methods. We also provide analysis of our network through extensive experiments to show how the network deals with motions implicitly.
Authors: Younghyun Jo, Yonsei University;Seoung Wug Oh, Yonsei Univeristy;JaeYeon Kang, Yonsei Univ.;Seon Joo Kim, Yonsei University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf


297
Title: MegDet: A Large Mini-Batch Object Detector
Type: Spotlight
Abstracts: The development of object detection in the era of deep learning, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from novel network, new framework, or loss design. How- ever, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detec- tion. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with a large mini- batch size up to 256, so that we can effectively utilize at most 128 GPUs to significantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our sub- mission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.
Authors: Chao Peng, Megvii;Tete Xiao, Peking University;Tsinghua University, Megvii;Yuning Jiang, Megvii inc.;Xiangyu Zhang, Megvii Inc;Kai Jia, Mevii;Gang Yu, Face++;Jian Sun,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_MegDet_A_Large_CVPR_2018_paper.pdf


298
Title: MegDet: A Large Mini-Batch Object Detector
Type: Poster
Abstracts: The development of object detection in the era of deep learning, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from novel network, new framework, or loss design. How- ever, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detec- tion. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with a large mini- batch size up to 256, so that we can effectively utilize at most 128 GPUs to significantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our sub- mission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.
Authors: Chao Peng, Megvii;Tete Xiao, Peking University;Tsinghua University, Megvii;Yuning Jiang, Megvii inc.;Xiangyu Zhang, Megvii Inc;Kai Jia, Mevii;Gang Yu, Face++;Jian Sun,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_MegDet_A_Large_CVPR_2018_paper.pdf


299
Title: AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks
Type: Poster
Authors: Tao Xu, Lehigh University;Pengchuan Zhang, ;Qiuyuan Huang, ;Han Zhang, Rutgers;Zhe Gan, ;Xiaolei Huang, Lehigh ;Xiaodong He,;

300
Title: TOM-Net: Learning Transparent Object Matting from a Single Image
Type: Spotlight
Authors: Guanying Chen, The University of Hong Kong;Kai Han, ;Kwan-Yee Kenneth Wong, The University of Hong Kong;

301
Title: TOM-Net: Learning Transparent Object Matting from a Single Image
Type: Poster
Authors: Guanying Chen, The University of Hong Kong;Kai Han, ;Kwan-Yee Kenneth Wong, The University of Hong Kong;

302
Title: End-to-End Deep Kronecker-Product Matching for Person Re-identification
Type: Poster
Authors: Yantao Shen, CUHK;Tong Xiao, The Chinese University of HK;Hongsheng Li, ;Shuai Yi, The Chinese University of Hong Kong;Xiaogang Wang, Chinese University of Hong Kong;

303
Title: Semantic Visual Localization
Type: Poster
Abstracts: Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, e.g., in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes.
Authors: Johannes Sch?nberger, ETH Zurich;Marc Pollefeys, ETH;Andreas Geiger, MPI Tuebingen / ETH Zuerich;Torsten Sattler, ETH Zurich;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.pdf


304
Title: Joint Cuts and Matching of Partitions in One Graph
Type: Poster
Abstracts: As two fundamental problems, graph cuts and graph matching have been intensively investigated over the decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures.
Authors: Tianshu Yu, Arizona State University;Junchi Yan, Shanghai Jiao Tong University;Jieyi Zhao, University of Texas Health Science Center at Houston;Baoxin Li, Arizona State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Joint_Cuts_and_CVPR_2018_paper.pdf


305
Title: Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions
Type: Spotlight
Abstracts: Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.
Authors: Torsten Sattler, ETH Zurich;Will Maddern, University of Oxford;Carl Toft, Chalmers University ;Akihiko Torii, Tokyo Institute of Technology;Lars Hammarstrand, Chalmers university of technol;Erik Stenborg, Chalmers University of Tech.;Daniel Safari, DTU;Marc Pollefeys, ETH;Josef Sivic, ;Fredrik Kahl, Chalmers;Tomas Pajdla,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.pdf


306
Title: Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions
Type: Poster
Abstracts: Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.
Authors: Torsten Sattler, ETH Zurich;Will Maddern, University of Oxford;Carl Toft, Chalmers University ;Akihiko Torii, Tokyo Institute of Technology;Lars Hammarstrand, Chalmers university of technol;Erik Stenborg, Chalmers University of Tech.;Daniel Safari, DTU;Marc Pollefeys, ETH;Josef Sivic, ;Fredrik Kahl, Chalmers;Tomas Pajdla,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.pdf


307
Title: Crowd Counting via Adversarial Cross-Scale Consistency Pursuit
Type: Poster
Abstracts: Crowd counting or density estimation is a challenging task in computer vision due to large scale variations, perspective distortions and serious occlusions, etc. Existing methods generally suffers from two issues: 1) the model averaging effects in multi-scale CNNs induced by the widely adopted L2 regression loss; and 2) inconsistent estimation across different scaled inputs. To explicitly address these issues, we propose a novel crowd counting (density estimation) framework called Adversarial Cross-Scale Consistency Pursuit (ACSCP). On one hand, a U-net structural network is designed to generate density map from input patch, and an adversarial loss is employed to shrink the solution onto a realistic subspace, thus attenuating the blurry effects of density map estimation. On the other hand, we design a novel scale-consistency regularizer which enforces that the sum up of the crowd counts from local patches (i.e., small scale) is coherent with the overall count of their region union (i.e., large scale). The above losses are integrated via a joint training scheme, so as to help boost density estimation performance by further exploring the collaboration between both objectives. Extensive experiments on four benchmarks have well demonstrated the effectiveness of the proposed innovations as well as the superior performance over prior art.
Authors: Institute of Image Communication and Network Engineering, Shanghai Jiao Tong U;Bingbing Ni, ;Yi Xu, Shanghai Jiao Tong University;Minsi Wang, Shanghai Jiao Tong University;jianguo Hu, Minivision;Xiaokang Yang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Crowd_Counting_via_CVPR_2018_paper.pdf


308
Title: Deep Group-shuffling Random Walk for Person Re-identification
Type: Poster
Authors: Yantao Shen, CUHK;Hongsheng Li, ;Tong Xiao, The Chinese University of HK;Shuai Yi, The Chinese University of Hong Kong;Dapeng Chen, CUHK;Xiaogang Wang, Chinese University of Hong Kong;

309
Title: Learning to Detect Features in Texture Images
Type: Spotlight
Abstracts: Local feature detection is a fundamental task in computer vision, and hand-crafted feature detectors such as SIFT have shown success in applications including image-based localization and registration. Recent work has used features detected in texture images for precise global localization, but is limited by the performance of existing feature detectors on textures, as opposed to natural images. We propose an effective and scalable method for learning feature detectors for textures, which combines an existing "ranking" loss with an efficient fully-convolutional architecture as well as a new training-loss term that maximizes the "peakedness" of the response map. We demonstrate that our detector is more repeatable than existing methods, leading to improvements in a real-world texture-based localization application.
Authors: Linguang Zhang, Princeton University;Szymon Rusinkiewicz, Princeton University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_to_Detect_CVPR_2018_paper.pdf


310
Title: Learning to Detect Features in Texture Images
Type: Poster
Abstracts: Local feature detection is a fundamental task in computer vision, and hand-crafted feature detectors such as SIFT have shown success in applications including image-based localization and registration. Recent work has used features detected in texture images for precise global localization, but is limited by the performance of existing feature detectors on textures, as opposed to natural images. We propose an effective and scalable method for learning feature detectors for textures, which combines an existing "ranking" loss with an efficient fully-convolutional architecture as well as a new training-loss term that maximizes the "peakedness" of the response map. We demonstrate that our detector is more repeatable than existing methods, leading to improvements in a real-world texture-based localization application.
Authors: Linguang Zhang, Princeton University;Szymon Rusinkiewicz, Princeton University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_to_Detect_CVPR_2018_paper.pdf


311
Title: Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification
Type: Poster
Abstracts: Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of- the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.
Authors: Jingya Wang, QMUL;Xiatian Zhu, Vision Semantics Ltd.;Shaogang Gong, Queen Mary University;Wei Li, Queen Mary University of Lond;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.pdf


312
Title: CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles
Type: Poster
Abstracts: Despite significant research in the area, reconstruction of multiple dynamic rigid objects (eg. vehicles) observed from wide-baseline, uncalibrated and unsynchronized cameras, remains hard. On one hand, feature tracking works well within each view but is hard to correspond across multiple cameras with limited overlap in fields of view or due to occlusions. On the other hand, advances in deep learning have resulted in strong detectors that work across different viewpoints but are still not precise enough for triangulation-based reconstruction. In this work, we develop a framework to fuse both the single-view feature tracks and multi-view detected part locations to significantly improve the detection, localization and reconstruction of moving vehicles, even in the presence of strong occlusions. We demonstrate our framework at a busy traffic intersection by reconstructing over 62 vehicles passing within a 3-minute window. We evaluate the different components within our framework and compare to alternate approaches such as reconstruction using tracking-by-detection.
Authors: Dinesh reddy Narapureddy, Carnegie mellon university;Minh Vo, CMU;Srinivasa Narasimhan, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf


313
Title: Context-aware Deep Feature Compression for High-speed Visual Tracking
Type: Poster
Authors: Jongwon Choi, ;Hyung Jin Chang, Imperial College London;Tobias Fischer, Imperial College London;Sangdoo Yun, Seoul National University;Jiyeoup Jeong, Seoul National University;kyuewang Lee, Seoul National University;Yiannis Demiris, ;Jin Choi,;

314
Title: Deep Material-aware Cross-spectral Stereo Matching
Type: Poster
Authors: Tiancheng Zhi, Carnegie Mellon University;Bernardo Pires, CMU;Martial Hebert, Carnegie Mellon University;Srinivasa Narasimhan, Carnegie Mellon University;

315
Title: Deep Extreme Cut: From Extreme Points to Object Segmentation
Type: Poster
Abstracts: This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr.
Authors: Kevis-Kokitsi Maninis, ETH Zurich;Sergi Caelles, ETH Zurich;Jordi Pont-Tuset, ETHZ;Luc Van Gool, KTH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Maninis_Deep_Extreme_Cut_CVPR_2018_paper.pdf


316
Title: Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images
Type: Spotlight
Authors: Hao Zhou, UMD;Jin Sun, University of Maryland;Yaser Yacoob, Univ of Maryland;David Jacobs, University of Maryland;

317
Title: Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images
Type: Poster
Authors: Hao Zhou, UMD;Jin Sun, University of Maryland;Yaser Yacoob, Univ of Maryland;David Jacobs, University of Maryland;

318
Title: Harmonious Attention Network for Person Re-Identication
Type: Poster
Authors: Wei Li, Queen Mary University of Lond;Xiatian Zhu, Vision Semantics Ltd.;Shaogang Gong, Queen Mary University;

319
Title: Unsupervised Deep Generative Adversarial Hashing Network
Type: Spotlight
Abstracts: Unsupervised deep hash functions have not shown satisfactory improvements against the shallow alternatives, and usually, require supervised pretraining to avoid getting stuck in bad local minima. In this paper, we propose a deep unsupervised hashing function, called HashGAN, which outperforms unsupervised hashing models with significant margins without any supervised pretraining. HashGAN consists of three networks, a generator, a discriminator and an encoder. By sharing the parameters of the encoder and discriminator, we benefit from the adversarial loss as a data dependent regularization in training our deep hash function. Moreover, a novel loss function is introduced for hashing real images, resulting in minimum entropy, uniform frequency, consistent and independent hash bits. Furthermore, we train the generator conditioning on random binary inputs and also use these binary variables in a triplet ranking loss for improving hash codes. In our experiments, HashGAN outperforms the previous unsupervised hash functions in image retrieval and achieves the state-of-the-art performance in image clustering. We also provide an ablation study, showing the contribution of each component in our loss function.
Authors: Kamran Ghasedi Dizaji, University of Pittsburgh;Feng Zheng, University of Pittsburgh;Najmeh Sadoughi, University of Texas at Dallas;Heng Huang, University of Pittsburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.pdf


320
Title: Unsupervised Deep Generative Adversarial Hashing Network
Type: Poster
Abstracts: Unsupervised deep hash functions have not shown satisfactory improvements against the shallow alternatives, and usually, require supervised pretraining to avoid getting stuck in bad local minima. In this paper, we propose a deep unsupervised hashing function, called HashGAN, which outperforms unsupervised hashing models with significant margins without any supervised pretraining. HashGAN consists of three networks, a generator, a discriminator and an encoder. By sharing the parameters of the encoder and discriminator, we benefit from the adversarial loss as a data dependent regularization in training our deep hash function. Moreover, a novel loss function is introduced for hashing real images, resulting in minimum entropy, uniform frequency, consistent and independent hash bits. Furthermore, we train the generator conditioning on random binary inputs and also use these binary variables in a triplet ranking loss for improving hash codes. In our experiments, HashGAN outperforms the previous unsupervised hash functions in image retrieval and achieves the state-of-the-art performance in image clustering. We also provide an ablation study, showing the contribution of each component in our loss function.
Authors: Kamran Ghasedi Dizaji, University of Pittsburgh;Feng Zheng, University of Pittsburgh;Najmeh Sadoughi, University of Texas at Dallas;Heng Huang, University of Pittsburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.pdf


321
Title: Pseudo-Mask Augmented Object Detection
Type: Poster
Authors: Xiangyun Zhao, Northwestern University;Shuang Liang, Tongji University;Yichen Wei, Microsoft Research Asia;

322
Title: LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)
Type: Spotlight
Authors: Pelin Dogan, ETH Zurich;Albert Li, Disney Research;Leonid Sigal, University of British Columbia;Markus Gross,;

323
Title: LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)
Type: Poster
Authors: Pelin Dogan, ETH Zurich;Albert Li, Disney Research;Leonid Sigal, University of British Columbia;Markus Gross,;

324
Title: Adversarial Complementary Learning for Weakly Supervised Object Localization
Type: Poster
Abstracts: In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14%, which is the new state-of-the-art.
Authors: Xiaolin Zhang, University of Technology Sydey;Yunchao Wei, ;Jiashi Feng, ;Yi Yang, ;Thomas Huang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.pdf


325
Title: Unsupervised Discovery of Object Landmarks as Structural Representations
Type: Oral
Abstracts: Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neuralnetwork representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures.
Authors: Yuting Zhang, University of Michigan;Yijie Guo, University of Michigan;Yixin Jin, ;Yijun Luo, University of Michigan;Zhiyuan He, University of Michigan;University of Michigan, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf


326
Title: Unsupervised Discovery of Object Landmarks as Structural Representations
Type: Poster
Abstracts: Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neuralnetwork representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures.
Authors: Yuting Zhang, University of Michigan;Yijie Guo, University of Michigan;Yixin Jin, ;Yijun Luo, University of Michigan;Zhiyuan He, University of Michigan;University of Michigan, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf


327
Title: DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map
Type: Poster
Authors: Peng Wang, Baidu;Ruigang Yang, University of Kentucky;Binbin Cao, Baidu;Wei Xu, ;Yuanqing Lin,;

328
Title: Monocular Relative Depth Perception with Web Stereo Data Supervision
Type: Poster
Authors: Ke Xian, Huazhong University of Science and Technology;Chunhua Shen, University of Adelaide;Zhiguo Cao, Huazhong University of Science and Technology;Hao Lu, Huazhong University of Science and Technology;yang xiao, Huazhong University of Science and Technology;Ruibo Li, Huazhong University of Science and Technology;Zhenbo Luo, Samsung Research Beijing;

329
Title: Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification
Type: Poster
Authors: Weijian Deng, University of Chinese Academy;Liang Zheng, University of Texas at San Ant;GUOLIANG KANG, UTS;Yi Yang, ;Qixiang Ye, ;Jianbin Jiao,;

330
Title: Objects as context for detecting their semantic parts
Type: Poster
Authors: Abel Gonzalez-Garcia, University of Edinburgh;Davide Modolo, Amazon;Vitto Ferrari,;

331
Title: Camera Style Adaptation for Person Re-identification
Type: Poster
Authors: Zhun Zhong, Xiamen University;Liang Zheng, University of Texas at San Ant;Zhedong Zheng, UTS;Shaozi Li, ;University of Technology, Sydney;

332
Title: Conditional Generative Adversarial Network for Structured Domain Adaptation
Type: Poster
Abstracts: In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Specifically, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method significantly outperforms the baselines as well as state-of-the-art methods by 12% ∼ 20% mean IoU on the Cityscapes dataset.
Authors: Weixiang Hong, Nanyang Technological Universi;Zhenzhen Wang, Nanyang Technological University;Ming Yang, Horizon Robotics Inc.;Junsong Yuan, Nanyang Technological University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf


333
Title: Rotation-sensitive Regression for Oriented Scene Text Detection
Type: Poster
Authors: Minghui Liao, Huazhong University of Science and Technology;Zhen Zhu, Huazhong University of Science and Technology;Baoguang Shi, Huazhong University of Science and Technology;Gui-Song Xia, Wuhan University;Xiang Bai, Huazhong University of Science and Technology;

334
Title: Residual Parameter Transfer for Deep Domain Adaptation
Type: Poster
Abstracts: The goal of Deep Domain Adaptation is to make it possible to use Deep Nets trained in one domain where there is enough annotated training data in another where there is little or none. Most current approaches have focused on learning feature representations that are invariant to the changes that occur when going from one domain to the other, which means using the same network parameters in both domains. While some recent algorithms explicitly model the changes by adapting the network parameters, they either severely restrict the possible domain changes, or significantly increase the number of model parameters. By contrast, we introduce a network architecture that includes auxiliary residual networks, which we train to predict the parameters in the domain with little annotated data from those in the other one. This architecture enables us to flexibly preserve the similarities between domains where they exist and model the differences when necessary. We demonstrate that our approach yields higher accuracy than state-of-the-art methods without undue complexity.
Authors: Artem Rozantsev, EPFL;Mathieu Salzmann, EPFL;Pascal Fua,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Rozantsev_Residual_Parameter_Transfer_CVPR_2018_paper.pdf


335
Title: SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation
Type: Spotlight
Abstracts: We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.
Authors: Weiyue Wang, USC;Ronald Yu, ;Qiangui Huang, U of Southern CA;Ulrich Neumann, USC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf


336
Title: SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation
Type: Poster
Abstracts: We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.
Authors: Weiyue Wang, USC;Ronald Yu, ;Qiangui Huang, U of Southern CA;Ulrich Neumann, USC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf


337
Title: Weakly Supervised Instance Segmentation using Class Peak Response
Type: Spotlight
Authors: UCAS, China;Yi Zhu, UCAS;Qixiang Ye, ;Qiang Qiu, ;Jianbin Jiao,;

338
Title: Weakly Supervised Instance Segmentation using Class Peak Response
Type: Poster
Authors: UCAS, China;Yi Zhu, UCAS;Qixiang Ye, ;Qiang Qiu, ;Jianbin Jiao,;

339
Title: Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network
Type: Poster
Abstracts: While fully-convolutional neural networks are very strong at modeling local features, they fail to aggregate global context due to their constrained receptive field. Modern methods typically address the lack of global context by introducing cascades, pooling, or by fitting a statistical model. In this work, we propose a new approach that introduces global context into a fully-convolutional neural network directly. The key concept is an implicit kernel convolution within the network. The kernel convolution blurs the output of a local-context subnet, which is then refined by a global-context subnet using dilated convolutions. The kernel convolution is crucial for the convergence of the network because it smoothens the gradients and reduces overfitting. In a postprocessing step, a simple PCA-based 2D shape model is fitted to the network output in order to filter outliers. Our experiments demonstrate the effectiveness of our approach, outperforming several state-of-the-art methods in facial landmark detection.
Authors: Daniel Merget, Technical University of Munich;Matthias Rock, TUM;Rigoll Gerhard, TUM;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Merget_Robust_Facial_Landmark_CVPR_2018_paper.pdf


340
Title: Rotation Averaging and Strong Duality
Type: Oral
Abstracts: In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time. We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data.
Authors: Anders Eriksson, ;Fredrik Kahl, Chalmers;Carl Olsson, Lund University;Tat-Jun Chin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.pdf


341
Title: Rotation Averaging and Strong Duality
Type: Poster
Abstracts: In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time. We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data.
Authors: Anders Eriksson, ;Fredrik Kahl, Chalmers;Carl Olsson, Lund University;Tat-Jun Chin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.pdf


342
Title: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning
Type: Poster
Abstracts: This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially ``pack'' multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.
Authors: Arun Mallya, UIUC;Lana Lazebnik,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.pdf


343
Title: Im2Flow: Motion Hallucination from Static Images for Action Recognition
Type: Oral
Authors: Ruohan Gao, University of Texas at Austin;Bo Xiong, UT-Austin;Kristen Grauman,;

344
Title: Im2Flow: Motion Hallucination from Static Images for Action Recognition
Type: Poster
Authors: Ruohan Gao, University of Texas at Austin;Bo Xiong, UT-Austin;Kristen Grauman,;

345
Title: Feature Quantization for Defending Against Distortion of Images
Type: Poster
Abstracts: In this work, we address the problem of improving robustness of convolutional neural networks (CNNs) to image distortion. We argue that higher moment statistics of feature distributions can be shifted due to image distortion, and the shift leads to performance decrease and cannot be reduced by ordinary normalization methods as observed in our experimental analyses. In order to mitigate this effect, we propose an approach base on feature quantization. To be specific, we propose to employ three different types of additional non-linearity in CNNs: i) a floor function with scalable resolution, ii) a power function with learnable exponents, and iii) a power function with data-dependent exponents. In the experiments, we observe that CNNs which employ the proposed methods obtain better performance in both generalization performance and robustness for various distortion types for large scale benchmark datasets. For instance, a ResNet-50 model equipped with the proposed method (+HPOW) obtains 6.95%, 5.26% and 5.61% better accuracy on the ILSVRC-12 classification tasks using images distorted with motion blur, salt and pepper and mixed distortions.
Authors: Zhun Sun, Tohoku University;Mete Ozay, ;Yan Zhang, RIKEN Center for AIP ;Xing Liu, Tohoku University;Takayuki Okatani, Tohoku University/RIKEN AIP;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Feature_Quantization_for_CVPR_2018_paper.pdf


346
Title: End-to-end weakly-supervised semantic alignment
Type: Poster
Authors: Ignacio ROCCO, Inria;Relja Arandjelovic, DeepMind;Josef Sivic,;

347
Title: PointGrid: A Deep Network for 3D Shape Understanding
Type: Spotlight
Abstracts: This paper presents a new deep learning architecture called PointGrid that is designed for 3D model recognition from unorganized point clouds. The new architecture embeds the input point cloud into a 3D grid by a simple, yet effective, sampling strategy and directly learns transformations and features from their raw coordinates. The proposed method is an integration of point and grid, a hybrid model, that leverages the simplicity of grid-based approaches such as VoxelNet while avoid its information loss. PointGrid learns better global information compared with PointNet and is much simpler than PointNet++, Kd-Net, Oct-Net and O-CNN, yet provides comparable recognition accuracy. With experiments on popular shape recognition benchmarks, PointGrid demonstrates competitive performance over existing deep learning methods on both classification and segmentation.
Authors: Truc Le, University of Missouri - Columbia;Ye Duan, University of Missouri - Columbia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf


348
Title: PointGrid: A Deep Network for 3D Shape Understanding
Type: Poster
Abstracts: This paper presents a new deep learning architecture called PointGrid that is designed for 3D model recognition from unorganized point clouds. The new architecture embeds the input point cloud into a 3D grid by a simple, yet effective, sampling strategy and directly learns transformations and features from their raw coordinates. The proposed method is an integration of point and grid, a hybrid model, that leverages the simplicity of grid-based approaches such as VoxelNet while avoid its information loss. PointGrid learns better global information compared with PointNet and is much simpler than PointNet++, Kd-Net, Oct-Net and O-CNN, yet provides comparable recognition accuracy. With experiments on popular shape recognition benchmarks, PointGrid demonstrates competitive performance over existing deep learning methods on both classification and segmentation.
Authors: Truc Le, University of Missouri - Columbia;Ye Duan, University of Missouri - Columbia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf


349
Title: Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts
Type: Poster
Authors: Yizhe Zhu, ;Mohamed Elhoseiny, FAIR;Bingchen Liu, Rutgers;Ahmed Elgammal,;

350
Title: A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds
Type: Poster
Abstracts: This paper proposes a segmentation-free, automatic and efficient procedure to detect general geometric quadric forms in point clouds, where clutter and occlusions are inevitable. Our everyday world is dominated by man-made objects which are designed using 3D primitives (such as planes, cones, spheres, cylinders, etc.). These objects are also omnipresent in industrial environments. This gives rise to the possibility of abstracting 3D scenes through primitives, thereby positions these geometric forms as an integral part of perception and high level 3D scene understanding. As opposed to state-of-the-art, where a tailored algorithm treats each primitive type separately, we propose to encapsulate all types in a single robust detection procedure. At the center of our approach lies a closed form 3D quadric fit, operating in both primal & dual spaces and requiring as low as 4 oriented-points. Around this fit, we design a novel, local null-space voting strategy to reduce the 4-point case to 3. Voting is coupled with the famous RANSAC and makes our algorithm orders of magnitude faster than its conventional counterparts. This is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes. Results on synthetic and real datasets support the validity of our method.
Authors: Tolga Birdal, Technical University of Munich;Benjamin Busam, Framos;Nassir Navab, Technical University of Munich;Slobodan Ilic, Siemens AG;Peter Sturm, INRIA Rhone-Alpes;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Birdal_A_Minimalist_Approach_CVPR_2018_paper.pdf


351
Title: A Benchmark for Articulated Human Pose Estimation and Tracking
Type: Poster
Authors: Mykhaylo Andriluka, MPI Informatics;Umar Iqbal, ;Eldar Insafutdinov, MPI Informatics;Anton Milan, University of Adelaide;Leonid Pishchulin, MPI Informatik;University of Bonn, Germany;Bernt Schiele, MPI Informatics Germany;

352
Title: Boosting Self-Supervised Learning via Knowledge Transfer
Type: Poster
Abstracts: In self-supervised learning one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. A surprising result is that our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from $5.9$ to $2.6$ in object detection on PASCAL VOC 2007.
Authors: Mehdi Noroozi, University of Bern;Ananthachari Kavalkazhani Vinjimoor, UMBC;Hamed Pirsiavash, ;Bern University, Switzerland;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf


353
Title: PPFNet: Global Context Aware Local Features for Robust 3D Point Matching
Type: Spotlight
Abstracts: We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel N-tuple loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.
Authors: Haowen Deng, Technical University of Munich;Tolga Birdal, Technical University of Munich;Slobodan Ilic, Siemens AG;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf


354
Title: PPFNet: Global Context Aware Local Features for Robust 3D Point Matching
Type: Poster
Abstracts: We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel N-tuple loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.
Authors: Haowen Deng, Technical University of Munich;Tolga Birdal, Technical University of Munich;Slobodan Ilic, Siemens AG;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf


355
Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
Type: Spotlight
Authors: Peter Anderson, Australian National University;Qi Wu, University of Adelaide;Damien Teney, Unversity of Adelaide;Jake Bruce, ;Mark Johnson, Macquarie University;Niko S?nderhauf, Queensland University of Technology;Ian Reid, ;Stephen Gould, Australian National University;Anton Van den Hengel, University of Adelaide;

356
Title: Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments
Type: Poster
Authors: Peter Anderson, Australian National University;Qi Wu, University of Adelaide;Damien Teney, Unversity of Adelaide;Jake Bruce, ;Mark Johnson, Macquarie University;Niko S?nderhauf, Queensland University of Technology;Ian Reid, ;Stephen Gould, Australian National University;Anton Van den Hengel, University of Adelaide;

357
Title: Fast Video Object Segmentation by Reference-Guided Mask Propagation
Type: Spotlight
Abstracts: We present an efficient method for the semi-supervised video object segmentation. Our method achieves accuracy competitive with state-of-the-art methods while running in a fraction of time compared to others. To this end, we propose a deep Siamese encoder-decoder network that is designed to take advantage of mask propagation and object detection while avoiding the weaknesses of both approaches. Our network, learned through a two-stage training process that exploits both synthetic and real data, works robustly without any online learning or post-processing. We validate our method on four benchmark sets that cover single and multiple object segmentation. On all the benchmark sets, our method shows comparable accuracy while having the order of magnitude faster runtime. We also provide extensive ablation and add-on studies to analyze and evaluate our framework.
Authors: Seoung Wug Oh, Yonsei Univeristy;Joon-Young Lee, ;Kalyan Sunkavalli, Adobe Systems Inc.;Seon Joo Kim, Yonsei University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Oh_Fast_Video_Object_CVPR_2018_paper.pdf


358
Title: Fast Video Object Segmentation by Reference-Guided Mask Propagation
Type: Poster
Abstracts: We present an efficient method for the semi-supervised video object segmentation. Our method achieves accuracy competitive with state-of-the-art methods while running in a fraction of time compared to others. To this end, we propose a deep Siamese encoder-decoder network that is designed to take advantage of mask propagation and object detection while avoiding the weaknesses of both approaches. Our network, learned through a two-stage training process that exploits both synthetic and real data, works robustly without any online learning or post-processing. We validate our method on four benchmark sets that cover single and multiple object segmentation. On all the benchmark sets, our method shows comparable accuracy while having the order of magnitude faster runtime. We also provide extensive ablation and add-on studies to analyze and evaluate our framework.
Authors: Seoung Wug Oh, Yonsei Univeristy;Joon-Young Lee, ;Kalyan Sunkavalli, Adobe Systems Inc.;Seon Joo Kim, Yonsei University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Oh_Fast_Video_Object_CVPR_2018_paper.pdf


359
Title: Super-Resolving Very Low-Resolution Face Images with Supplementary Attributes
Type: Poster
Authors: Xin Yu, Australian National University;Basura Fernando, ANU Canberra Australia;Richard Hartley, Australian National University Australia;NICTA, Australia;

360
Title: Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding
Type: Poster
Authors: Dapeng Chen, CUHK;Hongsheng Li, ;Tong Xiao, The Chinese University of HK;Shuai Yi, The Chinese University of Hong Kong;Xiaogang Wang, Chinese University of Hong Kong;

361
Title: One-shot Action Localization by Sequence Matching Network
Type: Poster
Authors: Hongtao Yang, Australian National University;Xuming He, ShanghaiTech;NICTA, Australia;

362
Title: Efficient Subpixel Refinement with Symbolic Linear Predictors
Type: Poster
Authors: Vincent Lui, Monash University;Jonathon Geeves, Monash University;Winston Yii, Monash University;Tom Drummond, Monash;

363
Title: Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning
Type: Poster
Authors: Jongchan Park, KAIST;Joon-Young Lee, ;Donggeun Yoo, Lunit;In So Kweon, KAIST;

364
Title: Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification
Type: Oral
Authors: Dapeng Chen, CUHK;Dan Xu, ;Hongsheng Li, ;University of Trento, Italy;Xiaogang Wang, Chinese University of Hong Kong;

365
Title: Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification
Type: Poster
Authors: Dapeng Chen, CUHK;Dan Xu, ;Hongsheng Li, ;University of Trento, Italy;Xiaogang Wang, Chinese University of Hong Kong;

366
Title: Single Image Reflection Separation with Perceptual Losses
Type: Poster
Authors: Xuaner Zhang, UC Berkeley;Qifeng Chen, Intel Labs;

367
Title: AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions
Type: Spotlight
Authors: Chunhui Gu, Google;Chen Sun, Google;David Ross, Google Research;Carl Vondrick, Google;Caroline Pantofaru, Google;Yeqng Li, Google Inc.;Sudheendra Vijayanarasimhan, Google Research;George Toderici, Google;Susanna Ricco, Google;Rahul Sukthankar, Google Research;INRIA Grenoble, France;Jitendra Malik,;

368
Title: AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions
Type: Poster
Authors: Chunhui Gu, Google;Chen Sun, Google;David Ross, Google Research;Carl Vondrick, Google;Caroline Pantofaru, Google;Yeqng Li, Google Inc.;Sudheendra Vijayanarasimhan, Google Research;George Toderici, Google;Susanna Ricco, Google;Rahul Sukthankar, Google Research;INRIA Grenoble, France;Jitendra Malik,;

369
Title: Recognize Actions by Disentangling Components of Dynamics
Type: Poster
Abstracts: Despite the remarkable progress in action recognition over the past several years, existing methods remain limited in efficiency and effectiveness. The methods treating appearance and motion as separate streams are usually subject to the cost of optical flow computation, while those relying on 3D convolution on the original video frames often yield inferior performance in practice. In this paper, we propose a new ConvNet architecture for video representation learning, which can derive disentangled components of dynamics purely from raw video frames, without the need of optical flow estimation. Particularly, the learned representation comprises three components for representing static appearance, apparent motion, and appearance changes. We introduce 3D pooling, cost volume processing, and warped feature differences, respectively for extracting the three components above. These modules are incorporated as three branches in our unified network, which share the underlying features and are learned jointly in an end-to-end manner. On two large datasets UCF101 and Kinetics our method obtained competitive performances with high efficiency, using only the RGB frame sequence as input.
Authors: Yue Zhao, CUHK;Yuanjun Xiong, Amazon ;Dahua Lin, CUHK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Recognize_Actions_by_CVPR_2018_paper.pdf


370
Title: Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains
Type: Poster
Abstracts: Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car.
Authors: Jiahao Pang, SenseTime Group Limited;Wenxiu Sun, SenseTime Group Limited;Chengxi Yang, SenseTime Group Limited;Jimmy Ren, SenseTime Group Limited;Ruichao Xiao, ;Jin Zeng, The Hong Kong University of Science and Technology;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pang_Zoom_and_Learn_CVPR_2018_paper.pdf


371
Title: Attention-aware Compositional Network for Person Re-Identification
Type: Poster
Authors: Jing Xu, SenseNets Technology Limited;Rui Zhao, SenseNets Technology Limited;Feng Zhu, SenseNets Technology Limited;Huaming Wang, SenseNets Technology Limited;Wanli Ouyang, The University of Sydney;

372
Title: HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification
Type: Poster
Authors: Amos Sironi, Prophesee;Manuele Brambilla, Prophesee;Nicolas Bourdis, prophesee;Xavier Lagorce, Prophesee;Ryad Benosman, Universite Pierre et Marie Curie-Paris;

373
Title: Mask-guided Contrastive Attention Model for Person Re-Identification
Type: Poster
Authors: Chunfeng Song, CASIA;Yan Huang, ;Wanli Ouyang, ;Liang Wang, unknown;

374
Title: Pose-Guided Photorealistic Face Rotation
Type: Spotlight
Abstracts: Face rotation provides an effective and cheap way for data augmentation and representation learning of face recognition. It is a challenging generative learning problem due to the large pose discrepancy between two face images. This work focuses on flexible face rotation of arbitrary head poses, including extreme profile views. We propose a novel Couple-Agent Pose-Guided Generative Adversarial Network (CAPG-GAN) to generate both neutral and profile head pose face images. The head pose information is encoded by facial landmark heatmaps. It not only forms a mask image to guide the generator in learning process but also provides a flexible controllable condition during inference. A couple-agent discriminator is introduced to reinforce on the realism of synthetic arbitrary view faces. Besides the generator and conditional adversarial loss, CAPG-GAN further employs identity preserving loss and total variation regularization to preserve identity information and refine local textures respectively. Quantitative and qualitative experimental results on the Multi-PIE and LFW databases consistently show the superiority of our face rotation method over the state-of-the-art.
Authors: CRIPAC, CASIA;Institute of Automation, Chine;Bing Yu, ;Ran He, ;Zhenan Sun, CRIPAC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.pdf


375
Title: Pose-Guided Photorealistic Face Rotation
Type: Poster
Abstracts: Face rotation provides an effective and cheap way for data augmentation and representation learning of face recognition. It is a challenging generative learning problem due to the large pose discrepancy between two face images. This work focuses on flexible face rotation of arbitrary head poses, including extreme profile views. We propose a novel Couple-Agent Pose-Guided Generative Adversarial Network (CAPG-GAN) to generate both neutral and profile head pose face images. The head pose information is encoded by facial landmark heatmaps. It not only forms a mask image to guide the generator in learning process but also provides a flexible controllable condition during inference. A couple-agent discriminator is introduced to reinforce on the realism of synthetic arbitrary view faces. Besides the generator and conditional adversarial loss, CAPG-GAN further employs identity preserving loss and total variation regularization to preserve identity information and refine local textures respectively. Quantitative and qualitative experimental results on the Multi-PIE and LFW databases consistently show the superiority of our face rotation method over the state-of-the-art.
Authors: CRIPAC, CASIA;Institute of Automation, Chine;Bing Yu, ;Ran He, ;Zhenan Sun, CRIPAC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.pdf


376
Title: Automatic 3D Indoor Scene Modeling from Single Panorama
Type: Spotlight
Authors: Yang Yang, University of Delaware;Shi Jin, ShanghaiTech University;Ruiyang Liu, ;Sing Bing Kang, Microsoft Research;University of Delaware, USA;

377
Title: Automatic 3D Indoor Scene Modeling from Single Panorama
Type: Poster
Authors: Yang Yang, University of Delaware;Shi Jin, ShanghaiTech University;Ruiyang Liu, ;Sing Bing Kang, Microsoft Research;University of Delaware, USA;

378
Title: SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-rigid Motion
Type: Spotlight
Authors: Miroslava Slavcheva, Siemens AG;Maximilian Baust, TUM;Slobodan Ilic, Siemens AG;

379
Title: SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-rigid Motion
Type: Poster
Authors: Miroslava Slavcheva, Siemens AG;Maximilian Baust, TUM;Slobodan Ilic, Siemens AG;

380
Title: A Biresolution Spectral framework for Product Quantization
Type: Poster
Authors: Lopamudra Mukherjee, University of Wisc Whitewater;Sathya Ravi, University of Wisconsin-Madison;Jiming Peng, University of Houston;Vikas Singh, University of Wisconsin-Madison;

381
Title: Dynamic Zoom-in Network for Fast Object Detection in Large Images
Type: Poster
Authors: Mingfei Gao, University of Maryland;Ruichi Yu, ;Ang Li, Google DeepMind;Vlad Morariu, University of Maryland;University of Maryland, USA;

382
Title: On the Importance of Label Quality for Semantic Segmentation
Type: Poster
Abstracts: Convolutional networks (ConvNets) have become the dominant approach to semantic image segmentation. Producing accurate, pixel--level labels required for this task is a tedious and time consuming process; however, producing approximate, coarse labels could take only a fraction of the time and effort. We investigate the relationship between the quality of labels and the performance of ConvNets for semantic segmentation. We create a very large synthetic dataset with perfectly labeled street view scenes. From these perfect labels, we synthetically coarsen labels with different qualities and estimate human--hours required for producing them. We perform a series of experiments by training ConvNets with a varying number of training images and label quality. We found that the performance of ConvNets mostly depends on the time spent creating the training labels. That is, a larger coarsely--annotated dataset can yield the same performance as a smaller finely--annotated one. Furthermore, fine--tuning coarsely pre--trained ConvNets with few finely-annotated labels can yield comparable or superior performance to training it with a large amount of finely-annotated labels alone, at a fraction of the labeling cost. We demonstrate that our result is also valid for different network architectures, and various object classes in an urban scene.
Authors: Aleksandar Zlateski, MIT;ronnachai Jaroensri, Massachusetts Institute of Technology;Prafull Sharma, MIT;Fredo Durand,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zlateski_On_the_Importance_CVPR_2018_paper.pdf


383
Title: EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation by Using Epipolar Geometry
Type: Poster
Authors: Changha Shin, Yonsei Univ;Hae-Gon Jeon, KAIST;Youngjin Yoon , ;InSo Kweon, ;Seon Joo Kim, Yonsei University;

384
Title: A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking
Type: Poster
Authors: M. Saquib Sarfraz, KIT;Arne Schumann, KIT;Andreas Eberle, KIT;Rainer Stiefelhagen, Karlsruhe Institute of Technology;

385
Title: Erase or Fill? Deep Joint Recurrent Rain Removal and Reconstruction in Videos
Type: Poster
Abstracts: In this paper, we address the problem of video rain removal by constructing deep recurrent convolutional networks. We visit the rain removal case by considering rain occlusion regions, i.e. light transmittance of rain streaks is low. Different from additive rain streaks, in such rain occlusion regions, the details of background images are completely lost. Therefore, we propose a hybrid rain model to depict both rain streaks and occlusions. With the wealth of temporal redundancy, we build a Joint Recurrent Rain Removal and Reconstruction Network (J4R-Net) that seamlessly integrates rain degradation classification, spatial texture appearances based rain removal and temporal coherence based background details reconstruction. The rain degradation classification provides a binary map that reveals whether a location degraded by linear additive streaks or occlusions. With this side information, the gate of the recurrent unit learns to make a trade-off between rain streak removal and background details reconstruction. Extensive experiments on a series of synthetic and real videos with rain streaks verify the superiority of the proposed method over previous state-of-the-art methods.
Authors: Jiaying Liu, Peking University;Wenhan Yang, Peking University;Shuai Yang, Peking University;Zongming Guo,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Erase_or_Fill_CVPR_2018_paper.pdf


386
Title: Scalable and Effective Deep CCA via Soft Decorrelation
Type: Poster
Abstracts: Recently the widely used multi-view learning model, Canonical Correlation Analysis (CCA) has been generalised to the non-linear setting via deep neural networks. Existing deep CCA models typically first decorrelate the feature dimensions of each view before the different views are maximally correlated in a common latent space. This feature decorrelation is achieved by enforcing an exact decorrelation constraint; these models are thus computationally expensive due to the matrix inversion or SVD operations required for exact decorrelation at each training iteration. Furthermore, the decorrelation step is often separated from the gradient descent based optimisation, resulting in sub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome these problems. Specifically, exact decorrelation is replaced by soft decorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be optimised jointly with the other training objectives. Extensive experiments show that the proposed soft CCA is more effective and efficient than existing deep CCA models. In addition, our SDL loss can be applied to other deep models beyond multi-view learning, and obtains superior performance compared to existing decorrelation losses.
Authors: Xiaobin Chang, Queen Mary Univ. of London;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Scalable_and_Effective_CVPR_2018_paper.pdf


387
Title: High-order tensor regularization with application to attribute ranking
Type: Poster
Authors: Kwang In Kim, University of Bath;Juhyun Park, Lancaster University;James Tompkin, Brown University;

388
Title: 3D-RCNN: Instance-level 3D Scene Understanding via Render-and-Compare
Type: Oral
Authors: Abhijit Kundu, Georgia Institute of Technology;Yin Li, Georgia Tech;James Rehg, Georgia Institute of Technology;

389
Title: 3D-RCNN: Instance-level 3D Scene Understanding via Render-and-Compare
Type: Poster
Authors: Abhijit Kundu, Georgia Institute of Technology;Yin Li, Georgia Tech;James Rehg, Georgia Institute of Technology;

390
Title: FoldingNet: Interpretable Unsupervised Learning on 3D Point Clouds
Type: Spotlight
Authors: Yaoqing Yang, Carnegie Mellon University;Chen Feng, MERL;Yiru Shen, Clemson University;Dong Tian, Mitsubishi Electric Research Laboratories;

391
Title: FoldingNet: Interpretable Unsupervised Learning on 3D Point Clouds
Type: Poster
Authors: Yaoqing Yang, Carnegie Mellon University;Chen Feng, MERL;Yiru Shen, Clemson University;Dong Tian, Mitsubishi Electric Research Laboratories;

392
Title: Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network
Type: Poster
Abstracts: Defocus blur detection (DBD) is the separation of infocus and out-of-focus regions in an image. This process has been paid considerable attention because of its remarkable potential applications. Accurate differentiation of homogeneous regions and detection of low-contrast focal regions, as well as suppression of background clutter, are challenges associated with DBD. To address these issues, we propose a multi-stream bottom-top-bottom fully convolutional network (BTBNet), which is the first attempt to develop an end-to-end deep network for DBD. First, we develop a fully convolutional BTBNet to integrate low-level cues and high-level semantic information. Then, considering that the degree of defocus blur is sensitive to scales, we propose multi-stream BTBNets that handle input images with different scales to improve the performance of DBD. Finally, we design a fusion and recursive reconstruction network to recursively refine the preceding blur detection maps. To promote further study and evaluation of the DBD models, we construct a new database of 500 challenging images and their pixel-wise defocus blur annotations. Experimental results on the existing and our new datasets demonstrate that the proposed method achieves significantly better performance than other state-of-the-art algorithms.
Authors: Wenda Zhao, Dalian University of Technolog;Dong Wang, DUT;Huchuan Lu, Dalian University of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Defocus_Blur_Detection_CVPR_2018_paper.pdf


393
Title: Decorrelated Batch Normalization
Type: Poster
Abstracts: Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.
Authors: Lei Huang, BeiHang university;Dawei Yang, University of Michigan;Bo Lang, Beihang University;Jia Deng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Decorrelated_Batch_Normalization_CVPR_2018_paper.pdf


394
Title: Unsupervised Textual Grounding: Linking Words to Image Concepts
Type: Spotlight
Abstracts: Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively.
Authors: Raymond Yeh, UIUC;Minh Do, University of Illinois at Urbana-Champaign;Alex Schwing,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yeh_Unsupervised_Textual_Grounding_CVPR_2018_paper.pdf


395
Title: Unsupervised Textual Grounding: Linking Words to Image Concepts
Type: Poster
Abstracts: Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively.
Authors: Raymond Yeh, UIUC;Minh Do, University of Illinois at Urbana-Champaign;Alex Schwing,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yeh_Unsupervised_Textual_Grounding_CVPR_2018_paper.pdf


396
Title: Scale-recurrent Network for Deep Image Deblurring
Type: Poster
Authors: Xin Tao, CUHK;Hongyun Gao, ;Yi Wang, The Chinese University of HK;Xiaoyong Shen, CUHK;Jue Wang, Megvii;Jiaya Jia, Chinese University of Hong Kong;

397
Title: Low-Shot Recognition with Imprinted Weights
Type: Poster
Authors: Hang Qi, UCLA;Matthew Brown, ;David Lowe,;

398
Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
Type: Oral
Abstracts: Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.
Authors: Peter Anderson, Australian National University;Xiaodong He, ;Chris Buehler, ;Damien Teney, Unversity of Adelaide;Mark Johnson, Macquarie University;Stephen Gould, Australian National University;Lei Zhang, Microsoft;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf


399
Title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering
Type: Poster
Abstracts: Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.
Authors: Peter Anderson, Australian National University;Xiaodong He, ;Chris Buehler, ;Damien Teney, Unversity of Adelaide;Mark Johnson, Macquarie University;Stephen Gould, Australian National University;Lei Zhang, Microsoft;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf


400
Title: Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation
Type: Poster
Authors: Naoto Inoue, The University of Tokyo;Ryosuke Furuta, The University of Tokyo;Toshihiko Yamasaki, The University of Tokyo;Kiyoharu Aizawa,;

401
Title: Facelet-Bank for Fast Portrait Manipulation
Type: Poster
Abstracts: Digital face manipulation has become a popular and fascinating way to touch images with the prevalence of smart phones and social networks. With a wide variety of user preferences, facial expressions, and accessories, a general and flexible model is necessary to accommodate different types of facial editing. In this paper, we propose a model to achieve this goal based on an end-to-end convolutional neural network that supports fast inference, edit-effect control, and quick partial-model update. In addition, this model learns from unpaired image sets with different attributes. Experimental results show that our framework can handle a wide range of expressions, accessories, and makeup effects. It produces high-resolution and high-quality results in fast speed.
Authors: Ying-Cong Chen, CUHK;Lin Huaijia, the Chinese University of Hong Kong;Ruiyu Li, CUHK;Michelle Shu, ;Xin Tao, CUHK;Yangang Ye, Tencent;Xiaoyong Shen, CUHK;Jiaya Jia, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Facelet-Bank_for_Fast_CVPR_2018_paper.pdf


402
Title: Duplex Generative Adversarial Network for Unsupervised Domain Adaptation
Type: Poster
Abstracts: Domain adaptation attempts to transfer the knowledge obtained from the source domain to the target domain, i.e., the domain where the testing data are. The main challenge lies in the distribution discrepancy between source and target domain. Most existing works endeavor to learn domain invariant representation usually by minimizing a distribution distance, e.g., MMD and the discriminator in the recently proposed generative adversarial network (GAN). Following the similar idea of GAN, this work proposes a novel GAN architecture with duplex adversarial discriminators (referred to as DupGAN), which can achieve domain-invariant representation and domain transformation. Specifically, our proposed network consists of three parts, an encoder, a generator and two discriminators. The encoder embeds samples from both domains into the latent representation, and the generator decodes the latent representation to both source and target domains respectively conditioned on a domain code, i.e., achieves domain transformation. The generator is pitted against duplex discriminators, one for source domain and the other for target, to ensure the reality of domain transformation, the latent representation domain invariant and the category information of it preserved as well. Our proposed work achieves the state-of-the-art performance on unsupervised domain adaptation of digit classification and object recognition.
Authors: ICT, CAS;Meina Kan, ;Shiguang Shan, Chinese Academy of Sciences;Xilin Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Duplex_Generative_Adversarial_CVPR_2018_paper.pdf


403
Title: Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation
Type: Poster
Abstracts: With pervasive applications of medical imaging in healthcare, biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. Since manual annotation suffers limited reproducibility, arduous efforts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), particularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmentation, attaining much improved performance. At the same time, quantization of DNNs has become an active research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing literature on quantization which primarily targets memory and computation complexity reduction, we apply quantization as a method to reduce overfitting in FCNs for better accuracy. Specifically, we focus on a state-of-the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annotation samples from the original training dataset, obtaining an effective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantization for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method have a reduction of up to 6.4x on memory usage.
Authors: Xiaowei Xu, University of Notre Dame;Yiyu Shi, University of Notre Dame;Qing Lu, University of Notre Dame;Lin Yang, University of Notre Dame;Sharon Hu, University of Notre Dame;Danny Chen, University of Notre Dame;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Quantization_of_Fully_CVPR_2018_paper.pdf


404
Title: Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks
Type: Poster
Authors: Shi Xuepeng, ICT;Shiguang Shan, Chinese Academy of Sciences;Meina Kan, ;Shuzhe Wu, Chinese Academy of Sciences;Xilin Chen,;

405
Title: Structure Preserving Video Prediction
Type: Poster
Abstracts: Despite recent emergence of adversarial based methods for video prediction, existing algorithms often produce unsatisfied results in image regions with rich structural information (i.e., object boundary) and detailed motion (i.e., articulated body movement). To this end, we present a structure preserving video prediction framework to explicitly address above issues and enhance video prediction quality. On one hand, our framework contains a two-stream generation architecture which deals with high frequency video content (i.e., detailed object or articulated motion structure) and low frequency video content (i.e., location or moving directions) in two separate streams. On the other hand, we propose a RNN structure for video prediction, which employs temporal-adaptive convolutional kernels to capture time-varying motion patterns as well as the tiny object within a scene. Extensive experiments on diverse scene, ranging from human motion to semantic layout prediction, demonstrate the effectiveness of the proposed video prediction approach.
Authors: Xu Jingwei, Shanghai Jiao Tong University;Bingbing Ni, ;Zefan Li, Shanghai Jiaotong University;Shuo Cheng, SJTU;Xiaokang Yang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Structure_Preserving_Video_CVPR_2018_paper.pdf


406
Title: Tagging Like Humans: Diverse and Distinct Image Annotation
Type: Poster
Abstracts: In this work we propose a new automatic image annotation model, dubbed diverse and distinct image annotation (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. We perform extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets to demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.
Authors: Baoyuan Wu, Tencent AI Lab;Weidong Chen, Tencent;Wei Liu, ;Peng Sun, Tencent;Bernard Ghanem, ;Siwei Lyu, SUNY Albany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Tagging_Like_Humans_CVPR_2018_paper.pdf


407
Title: Learning to Sketch with Shortcut Cycle Consistency
Type: Poster
Authors: Queen Mary, Uni. of London;Kaiyue Pang, QMUL;Yi-Zhe Song, ;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;

408
Title: GroupCap: Group-based Image Captioning with Structured Relevance and Diversity Constraints
Type: Poster
Authors: Fuhai Chen, Xiamen university;Rongrong Ji, ;Xiaoshuai Sun, Harbin Institute of Technology;Jinsong Su, Xiamen university;

409
Title: Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks
Type: Spotlight
Abstracts: Due to the spatially variant blur caused by camera shake and object motions under different scene depths, deblurring images captured from dynamic scenes is challenging. Although recent works based on deep neural networks have shown great progress on this problem, their models are usually large and computationally expensive. In this paper, we propose a novel spatially variant neural network to address the problem. The proposed network is composed of three deep convolutional neural networks (CNNs) and a recurrent neural network (RNN). RNN is used as a deconvolution operator performed on feature maps extracted from the input image by one of the CNNs. Another CNN is used to learn the weights for the RNN at every location. As a result, the RNN is spatially variant and could implicitly model the deblurring process with spatially variant kernels. The third CNN is used to reconstruct the final deblurred feature maps into restored image. The whole network is end-to-end trainable. Our analysis shows that the proposed network has a large receptive field even with a small model size. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of accuracy, speed, and model size.
Authors: Jiawei Zhang, City University of Hong Kong;Jinshan Pan, UC Merced;Jimmy Ren, SenseTime Group Limited;Yibing Song, Tencent AI Lab;Linchao Bao, Tencent AI Lab;Rynson Lau, City University of Hong Kong;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Dynamic_Scene_Deblurring_CVPR_2018_paper.pdf


410
Title: Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks
Type: Poster
Abstracts: Due to the spatially variant blur caused by camera shake and object motions under different scene depths, deblurring images captured from dynamic scenes is challenging. Although recent works based on deep neural networks have shown great progress on this problem, their models are usually large and computationally expensive. In this paper, we propose a novel spatially variant neural network to address the problem. The proposed network is composed of three deep convolutional neural networks (CNNs) and a recurrent neural network (RNN). RNN is used as a deconvolution operator performed on feature maps extracted from the input image by one of the CNNs. Another CNN is used to learn the weights for the RNN at every location. As a result, the RNN is spatially variant and could implicitly model the deblurring process with spatially variant kernels. The third CNN is used to reconstruct the final deblurred feature maps into restored image. The whole network is end-to-end trainable. Our analysis shows that the proposed network has a large receptive field even with a small model size. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of accuracy, speed, and model size.
Authors: Jiawei Zhang, City University of Hong Kong;Jinshan Pan, UC Merced;Jimmy Ren, SenseTime Group Limited;Yibing Song, Tencent AI Lab;Linchao Bao, Tencent AI Lab;Rynson Lau, City University of Hong Kong;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Dynamic_Scene_Deblurring_CVPR_2018_paper.pdf


411
Title: Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning
Type: Poster
Authors: Xingping Dong, Beijing Institute of Technology;Jianbing Shen, Beijing Institute of Technolog;Wenguan Wang, Beijing Institute of Technology;Yu Liu, Beijing Institute of Technology;Ling Shao, University of East Anglia;NICTA, Australia;

412
Title: Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective
Type: Spotlight
Abstracts: The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that ``Is it possible to learn saliency maps without using labeled data while improving the generalization ability?''. To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by ``weak'' and ``noisy'' unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods.
Authors: Jing Zhang, ;Tong Zhang, Australian National University;Yuchao Dai, Australian National University;Mehrtash Harandi, Australian National University;Richard Hartley, Australian National University Australia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Unsupervised_Saliency_CVPR_2018_paper.pdf


413
Title: Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective
Type: Poster
Abstracts: The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that ``Is it possible to learn saliency maps without using labeled data while improving the generalization ability?''. To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by ``weak'' and ``noisy'' unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods.
Authors: Jing Zhang, ;Tong Zhang, Australian National University;Yuchao Dai, Australian National University;Mehrtash Harandi, Australian National University;Richard Hartley, Australian National University Australia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Unsupervised_Saliency_CVPR_2018_paper.pdf


414
Title: NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning
Type: Spotlight
Abstracts: Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods.
Authors: Alexander Richard, University of Bonn;Hilde Kuehne, University of Bonn;Ahsan Iqbal, University of Bonn;University of Bonn, Germany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.pdf


415
Title: NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning
Type: Poster
Abstracts: Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods.
Authors: Alexander Richard, University of Bonn;Hilde Kuehne, University of Bonn;Ahsan Iqbal, University of Bonn;University of Bonn, Germany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.pdf


416
Title: Detecting and Recognizing Human-Object Interactions
Type: Spotlight
Abstracts: To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.
Authors: Georgia Gkioxari, Facebook;Ross Girshick, ;Kaiming He, ;Menlo Park, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.pdf


417
Title: Detecting and Recognizing Human-Object Interactions
Type: Poster
Abstracts: To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.
Authors: Georgia Gkioxari, Facebook;Ross Girshick, ;Kaiming He, ;Menlo Park, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.pdf


418
Title: Augmenting Crowd-Sourced 3D Reconstructions using Semantic Detections
Type: Poster
Authors: True Price, UNC Chapel Hill;Johannes Sch?nberger, ETH Zurich;Zhen Wei, University of North Carolina;Marc Pollefeys, ETH;Jan-Michael Frahm, UNC Chapel Hill;

419
Title: Visual Relationship Learning with a Factorization-based Prior
Type: Poster
Authors: SEONG JAE HWANG, University of Wisconsin - Madison;Zirui Tao , University of Wisconsin - Madi;Vikas Singh, University of Wisconsin-Madison;Hyunwoo Kim, Amazon Lab 126;Sathya Ravi, University of Wisconsin-Madison;Maxwell Collins,;

420
Title: Re-weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation
Type: Poster
Authors: Qingchao Chen, Unviersity College London;Yang Liu, University of Cambridge;Zhaowen Wang, Adobe;Ian Wassell, ;Kevin Chetty,;

421
Title: Flow Guided Recurrent Neural Encoder for Video Salient Object Detection
Type: Poster
Abstracts: Image saliency detection has recently witnessed significant progress due to deep convolutional neural networks. However, extending state-of-the-art saliency detectors from image to video is challenging. The performance of salient object detection suffers from object or camera motion and the dramatic change of the appearance contrast in videos. In this paper, we present flow guided recurrent neural encoder(FGRNE), an accurate and end-to-end learning framework for video salient object detection. It works by enhancing the temporal coherence of the per-frame feature by exploiting both motion information in terms of optical flow and sequential feature evolution encoding in terms of LSTM networks. It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection. Intensive experimental results verify the effectiveness of each part of FGRNE and confirm that our proposed method significantly outperforms state-of-the-art methods on the public benchmarks of DAVIS and FBMS.
Authors: Guanbin Li, ;Yuan Xie, ;Tianhao Wei, ;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Flow_Guided_Recurrent_CVPR_2018_paper.pdf


422
Title: Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment
Type: Poster
Authors: Amit Kumar, University of Maryland;University of Maryland, USA;

423
Title: Progressive Attention Guided Recurrent Network for Salient Object Detection
Type: Poster
Abstracts: Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task. FCN-based methods directly apply multi-level convolutional features without distinction, which leads to sub-optimal results due to the distraction from redundant details. In this paper, we propose a novel attention guided network which selectively integrates multi-level contextual information in a progressive manner. Attentive features generated by our network can alleviate distraction of background thus achieve better performance. On the other hand, it is observed that most of existing algorithms conduct salient object detection by exploiting side-output features of the backbone feature extraction network. However, shallower layers of backbone network lack the ability to obtain global semantic information, which limits the effective feature learning. To address the problem, we introduce multi-path recurrent feedback to enhance our proposed progressive attention driven framework. Through multi-path recurrent connections, global semantic information from the top convolutional layer is transferred to shallower layers, which intrinsically refines the entire network. Experimental results on six benchmark datasets demonstrate that our algorithm performs favorably against the state-of-the-art approaches.
Authors: Xiaoning Zhang, Dalian University of Technolog;TIANTIAN WANG, Dalian University of Technolog;Jinqing Qi, ;Huchuan Lu, Dalian University of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Progressive_Attention_Guided_CVPR_2018_paper.pdf


424
Title: Answer with Grounding Snippets: Focal Visual-Text Attention for Visual Question Answering
Type: Spotlight
Authors: Junwei Liang, Carnegie Mellon University;Lu Jiang, ;Liangliang Cao, ;Alexander Hauptmann,;

425
Title: Answer with Grounding Snippets: Focal Visual-Text Attention for Visual Question Answering
Type: Poster
Authors: Junwei Liang, Carnegie Mellon University;Lu Jiang, ;Liangliang Cao, ;Alexander Hauptmann,;

426
Title: Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints
Type: Poster
Authors: Reza Mahjourian, University of Texas at Austin;Martin Wicke, Google Brain;Anelia Angelova, Google Brain;

427
Title: Repulsion Loss: Detecting Pedestrians in a Crowd
Type: Poster
Abstracts: Detecting individual pedestrians in a crowd remains a challenging problem since the pedestrians often gather together and occlude each other in real-world scenarios. In this paper, we first explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion via experimentation, providing insights into the crowd occlusion problem. Then, we propose a novel bounding box regression loss specifically designed for crowd scenes, termed repulsion loss. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. The repulsion term prevents the proposal from shifting to surrounding objects thus leading to more crowd-robust localization. Our detector trained by repulsion loss outperforms the state-of-the-art methods with a significant improvement in occlusion cases.
Authors: Xinlong Wang, Tongji University;Tete Xiao, Peking University;Yuning Jiang, Megvii inc.;Shuai Shao, Megvii;Jian Sun, ;Chunhua Shen, University of Adelaide;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf


428
Title: PU-Net: Point Cloud Upsampling Network
Type: Poster
Abstracts: Learning and analyzing 3D point clouds with deep networks is challenging due to the sparseness and irregularity of the data. In this paper, we present a data-driven point cloud upsampling technique. The key idea is to learn multi-level features per point and expand the point set via a multi-branch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. We conduct various experiments using synthesis and scan data to evaluate our method and demonstrate its superiority over some baseline methods and an optimization-based method. Results show that our upsampled points have better uniformity and are located closer to the underlying surfaces.
Authors: Lequan Yu, The Chinese University of Hong;XIANZHI LI, CUHK;Chi-Wing Fu, ;Daniel Cohen-Or, ;Pheng-Ann Heng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.pdf


429
Title: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF
Type: Spotlight
Authors: Linchao Bao, Tencent AI Lab;Baoyuan Wu, Tencent AI Lab;Wei Liu,;

430
Title: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF
Type: Poster
Authors: Linchao Bao, Tencent AI Lab;Baoyuan Wu, Tencent AI Lab;Wei Liu,;

431
Title: PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection
Type: Poster
Authors: Nian Liu, Northwestern Polytechnical University;Junwei Han, Northwestern Polytechnical U.;Ming-Hsuan Yang, UC Merced;

432
Title: Gated Fusion Network for Single Image Dehazing
Type: Poster
Abstracts: In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale based approach so that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.
Authors: Wenqi Ren, Chinese Academy of Sciences;Lin Ma, Tencent AI Lab;Jiawei Zhang, City University of Hong Kong;Jinshan Pan, UC Merced;Xiaochun Cao, Chinese Academy of Sciences;Wei Liu, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Gated_Fusion_Network_CVPR_2018_paper.pdf


433
Title: Interleaved Structured Sparse Convolutional Neural Networks
Type: Spotlight
Abstracts: In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels,the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g. , Xception), has been attracting increasing interests. Motivated by the observation that the convolutions contained in a group convolution in IGC can be further decomposed in the same manner, we present a modularized building block, {IGC-V2:}interleaved structured sparse convolutions. It generalizes interleaved group convolutions, which is composed of two structured sparse kernels, to the product of more structured sparse kernels, further eliminating the redundancy. We present the complementary condition and the balance condition to guide the design of structured sparse kernels, obtaining a balance between three aspects: model size and computation complexity and classification accuracy. Experimental results demonstrate the advantage on the balance between these three aspects compared to interleaved group convolutions and Xception and competitive performance with other state-of-the-art architecture design methods.
Authors: Guotian Xie, Sun Yat-Sen University;Ting Zhang, Microsoft Research Asia;Jianhuang Lai, Sun Yat-sen University;Jingdong Wang, Microsoft Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.pdf


434
Title: Interleaved Structured Sparse Convolutional Neural Networks
Type: Poster
Abstracts: In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels,the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g. , Xception), has been attracting increasing interests. Motivated by the observation that the convolutions contained in a group convolution in IGC can be further decomposed in the same manner, we present a modularized building block, {IGC-V2:}interleaved structured sparse convolutions. It generalizes interleaved group convolutions, which is composed of two structured sparse kernels, to the product of more structured sparse kernels, further eliminating the redundancy. We present the complementary condition and the balance condition to guide the design of structured sparse kernels, obtaining a balance between three aspects: model size and computation complexity and classification accuracy. Experimental results demonstrate the advantage on the balance between these three aspects compared to interleaved group convolutions and Xception and competitive performance with other state-of-the-art architecture design methods.
Authors: Guotian Xie, Sun Yat-Sen University;Ting Zhang, Microsoft Research Asia;Jianhuang Lai, Sun Yat-sen University;Jingdong Wang, Microsoft Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.pdf


435
Title: Where and Why Are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks
Type: Poster
Abstracts: This paper addresses a new problem - jointly inferring human attention, intentions, and tasks from videos. Given an RGB-D video where a human performs a task, we answer three questions simultaneously: 1) where the human is looking - attention prediction; 2) why the human is looking there - intention prediction; and 3) what task the human is performing - task recognition. We propose a hierarchical model of human-attention-object (HAO) which represents tasks, intentions, and attention under a unified framework. A task is represented as sequential intentions which transition to each other. An intention is composed of the human pose, attention, and objects. A beam search algorithm is adopted for inference on the HAO graph to output the attention, intention, and task results. We built a new video dataset of tasks, intentions, and attention. It contains 14 task classes, 70 intention categories, 28 object classes, 809 videos, and approximately 330,000 frames. Experiments show that our approach outperforms existing approaches.
Authors: Ping Wei, Xi'an Jiaotong University;Yang Liu, UCLA;University of California, Los Angeles;Nanning Zheng, Xi'an Jiaotong University;Song-Chun Zhu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Where_and_Why_CVPR_2018_paper.pdf


436
Title: End-to-end Flow Correlation Tracking with Spatial-temporal Attention
Type: Poster
Authors: Institute of Automation, CAS;Wei Wu, ;Wei Zou, ;Junjie Yan,;

437
Title: Left/Right Asymmetric Layer Skippable Networks
Type: Poster
Authors: Changmao Cheng, Fudan University;Yanwei Fu, fudan;Yu-Gang Jiang, Fudan University;Wei Liu, ;wenlian Lu, Fudan;Jianfeng Feng, fudan university;Xiangyang Xue,;

438
Title: Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation
Type: Oral
Authors: Henghui Ding, Nanyang Technological University;Xudong Jiang, Nanyang Technological University;Bing Shuai, ;Ai Qun Liu, Nanyang Technological University;Gang Wang,;

439
Title: Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation
Type: Poster
Authors: Henghui Ding, Nanyang Technological University;Xudong Jiang, Nanyang Technological University;Bing Shuai, ;Ai Qun Liu, Nanyang Technological University;Gang Wang,;

440
Title: VITAL: VIsual Tracking via Adversarial Learning
Type: Spotlight
Abstracts: The tracking-by-detection framework consists of two stages, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage. The performance of existing tracking-by-detection trackers using deep classification networks is limited by two aspects. First, the positive samples in each frame are highly spatially overlapped, and they fail to capture rich appearance variations. Second, there exists severe class imbalance between positive and negative samples. This paper presents the VITAL algorithm to address these two problems via adversarial learning. To augment positive samples, we use a generative network to randomly generate masks, which are applied to input features to capture a variety of appearance changes. With the use of adversarial learning, our network identifies the mask that maintains the most robust features of the target objects over a long temporal span. In addition, to handle the issue of class imbalance, we propose a high-order cost sensitive loss to decrease the effect of easy negative samples to facilitate training the classification network. Extensive experiments on benchmark datasets demonstrate that the proposed tracker performs favorably against state-of-the-art approaches.
Authors: Yibing Song, Tencent AI Lab;Chao Ma, ;Xiaohe Wu, Harbin Institute of technology;Lijun Gong, City University of Hong Kong;Linchao Bao, Tencent AI Lab;Wangmeng Zuo, Harbin Institute of Technology;Chunhua Shen, University of Adelaide;Rynson Lau, City University of Hong Kong;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.pdf


441
Title: VITAL: VIsual Tracking via Adversarial Learning
Type: Poster
Abstracts: The tracking-by-detection framework consists of two stages, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage. The performance of existing tracking-by-detection trackers using deep classification networks is limited by two aspects. First, the positive samples in each frame are highly spatially overlapped, and they fail to capture rich appearance variations. Second, there exists severe class imbalance between positive and negative samples. This paper presents the VITAL algorithm to address these two problems via adversarial learning. To augment positive samples, we use a generative network to randomly generate masks, which are applied to input features to capture a variety of appearance changes. With the use of adversarial learning, our network identifies the mask that maintains the most robust features of the target objects over a long temporal span. In addition, to handle the issue of class imbalance, we propose a high-order cost sensitive loss to decrease the effect of easy negative samples to facilitate training the classification network. Extensive experiments on benchmark datasets demonstrate that the proposed tracker performs favorably against state-of-the-art approaches.
Authors: Yibing Song, Tencent AI Lab;Chao Ma, ;Xiaohe Wu, Harbin Institute of technology;Lijun Gong, City University of Hong Kong;Linchao Bao, Tencent AI Lab;Wangmeng Zuo, Harbin Institute of Technology;Chunhua Shen, University of Adelaide;Rynson Lau, City University of Hong Kong;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.pdf


442
Title: RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints
Type: Poster
Authors: Asako Kanezaki, National Institute of Advanced;Yasuyuki Matsushita, Osaka University;Yoshifumi Nishida, National Institute of Advanced Industrial Science and Technology (AIST);

443
Title: Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints
Type: Spotlight
Authors: Alexander Richard, University of Bonn;Hilde Kuehne, University of Bonn;University of Bonn, Germany;

444
Title: Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints
Type: Poster
Authors: Alexander Richard, University of Bonn;Hilde Kuehne, University of Bonn;University of Bonn, Germany;

445
Title: Squeeze-and-Excitation Networks
Type: Oral
Abstracts: Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ∼25% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.
Authors: Jie Hu, Momenta;Li Shen, University of Oxford;Gang Sun, Momenta;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf


446
Title: Squeeze-and-Excitation Networks
Type: Poster
Abstracts: Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ∼25% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.
Authors: Jie Hu, Momenta;Li Shen, University of Oxford;Gang Sun, Momenta;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf


447
Title: Edit Probability for Scene Text Recognition
Type: Poster
Abstracts: We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance.
Authors: Fan Bai, Fudan University;Zhanzhan Cheng, Hikvision Research Institute;Yi Niu, Hikvision Research Institute;Shiliang Pu, ;Shuigeng Zhou, Fudan University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Edit_Probability_for_CVPR_2018_paper.pdf


448
Title: Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning
Type: Spotlight
Authors: Jingwen Wang, SCUT;Wenhao Jiang, Tencent AI Lab;Lin Ma, Tencent AI Lab;Wei Liu, ;Yong Xu, South China University of Technology;

449
Title: Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning
Type: Poster
Authors: Jingwen Wang, SCUT;Wenhao Jiang, Tencent AI Lab;Lin Ma, Tencent AI Lab;Wei Liu, ;Yong Xu, South China University of Technology;

450
Title: Exploit the Unknown Gradually:~ One-Shot Video-Based Person Re-Identification by Stepwise Learning
Type: Poster
Authors: Yu Wu, University of technology sydne;Yutian Lin, ;Xuanyi Dong, UTS;Yan Yan, UTS;Wanli Ouyang, The University of Sydney;Yi Yang,;

451
Title: Learning to Localize Sound Source in Visual Scenes
Type: Poster
Abstracts: Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We show that even with a few supervision, i.e., semi-supervised setup, false conclusion is able to be corrected effectively.
Authors: Arda Senocak, KAIST;Junsik Kim, Korea Advanced Institute of Science and Technology (KAIST);Tae-Hyun Oh, MIT;Ming-Hsuan Yang, UC Merced;In So Kweon, KAIST;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Senocak_Learning_to_Localize_CVPR_2018_paper.pdf


452
Title: Dynamic Few-Shot Visual Learning without Forgetting
Type: Poster
Authors: Spyros Gidaris, Ecole des Ponts ParisTech ;Nikos Komodakis,;

453
Title: Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features
Type: Poster
Abstracts: Weakly-supervised semantic segmentation under image tags supervision is a challenging task as it directly associates high-level semantic to low-level appearance. To bridge this gap, in this paper, we propose an iterative bottom-up and top-down framework which alternatively expands object regions and optimizes segmentation network. We start from initial localization produced by classification networks. While classification networks are only responsive to small and coarse discriminative object regions, we argue that, these regions contain significant common features about objects. So in the bottom-up step, we mine common object features from the initial localization and expand object regions with the mined features. To supplement non-discriminative regions, saliency maps are then considered under Bayesian framework to refine the object regions. Then in the top-down step, the refined object regions are used as supervision to train the segmentation network and to predict object masks. These object masks provide more accurate localization and contain more regions of object. Further, we take these object masks as initial localization and mine common object features from them. These processes are conducted iteratively to progressively produce fine object masks and optimize segmentation networks. Experimental results on Pascal VOC 2012 dataset demonstrate that the proposed method outperforms previous state-of-the-art methods by a large margin.
Authors: Xiang Wang, Tsinghua University;Shaodi You, Data61;Xi Li, Tsinghua University;Huimin Ma, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf


454
Title: SINT++: Robust Visual Tracking via Adversarial Hard Positive Generation
Type: Poster
Authors: Xiao Wang, Anhui university;Chenglong Li, Anhui University;Bin Luo, ;Jin Tang,;

455
Title: Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer
Type: Poster
Authors: Amir Atapour-Abarghouei, Durham University;Toby Breckon, Durham University;

456
Title: Fast and Accurate Single Image Super-Resolution via Information Distillation Network
Type: Poster
Abstracts: Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few number of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance.
Authors: Zheng Hui, Xidian university;Xiumei Wang, Xidian university;Xinbo Gao,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_Fast_and_Accurate_CVPR_2018_paper.pdf


457
Title: Low-Latency Video Semantic Segmentation
Type: Spotlight
Abstracts: Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components:(1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.
Authors: Yule Li, Ict;Jianping Shi, SenseTime;Dahua Lin, CUHK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Low-Latency_Video_Semantic_CVPR_2018_paper.pdf


458
Title: Low-Latency Video Semantic Segmentation
Type: Poster
Abstracts: Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components:(1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.
Authors: Yule Li, Ict;Jianping Shi, SenseTime;Dahua Lin, CUHK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Low-Latency_Video_Semantic_CVPR_2018_paper.pdf


459
Title: Domain Adaptive Faster R-CNN for Object Detection in the Wild
Type: Poster
Abstracts: Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.
Authors: Yuhua Chen, CVL@ETHZ;Wen Li, ETH;Luc Van Gool, KTH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.pdf


460
Title: DoubleFusion: Real-time Capture of Human Performance with Inner Body Shape from a Single Depth Sensor
Type: Oral
Authors: Tao Yu, Beihang University;Zerong Zheng, Tsinghua University;Kaiwen Guo, Google;Jianhui Zhao, Beihang University;Qionghai Dai, ;Hao Li, ;Gerard Pons-Moll, Max Planck for Informatics;Yebin Liu, Tsinghua University;

461
Title: DoubleFusion: Real-time Capture of Human Performance with Inner Body Shape from a Single Depth Sensor
Type: Poster
Authors: Tao Yu, Beihang University;Zerong Zheng, Tsinghua University;Kaiwen Guo, Google;Jianhui Zhao, Beihang University;Qionghai Dai, ;Hao Li, ;Gerard Pons-Moll, Max Planck for Informatics;Yebin Liu, Tsinghua University;

462
Title: Lean Multiclass Crowdsourcing
Type: Spotlight
Abstracts: We introduce a method for efficiently crowdsourcing multiclass annotations in challenging, real world image datasets. Our method is designed to minimize the number of human annotations that are necessary to achieve a desired level of confidence on class labels. It is based on combining models of worker behavior with computer vision. Our method is general: it can handle a large number of classes, worker labels that come from a taxonomy rather than a flat list, and can model the dependence of labels when workers can see a history of previous annotations. Our method may be used as a drop-in replacement for the majority vote algorithms used in online crowdsourcing services that aggregate multiple human annotations into a final consolidated label. In experiments conducted on two real-life applications we find that our method can reduce the number of required annotations by as much as a factor of 5.4 and can reduce the residual annotation error by up to 90% when compared with majority voting. Furthermore, the online risk estimates of the models may be used to sort the annotated collection and minimize subsequent expert review effort.
Authors: Grant van Horn, California Institute of Technology;California Institute of Technology, USA;Serge Belongie,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_Lean_Multiclass_Crowdsourcing_CVPR_2018_paper.pdf


463
Title: Lean Multiclass Crowdsourcing
Type: Poster
Abstracts: We introduce a method for efficiently crowdsourcing multiclass annotations in challenging, real world image datasets. Our method is designed to minimize the number of human annotations that are necessary to achieve a desired level of confidence on class labels. It is based on combining models of worker behavior with computer vision. Our method is general: it can handle a large number of classes, worker labels that come from a taxonomy rather than a flat list, and can model the dependence of labels when workers can see a history of previous annotations. Our method may be used as a drop-in replacement for the majority vote algorithms used in online crowdsourcing services that aggregate multiple human annotations into a final consolidated label. In experiments conducted on two real-life applications we find that our method can reduce the number of required annotations by as much as a factor of 5.4 and can reduce the residual annotation error by up to 90% when compared with majority voting. Furthermore, the online risk estimates of the models may be used to sort the annotated collection and minimize subsequent expert review effort.
Authors: Grant van Horn, California Institute of Technology;California Institute of Technology, USA;Serge Belongie,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_Lean_Multiclass_Crowdsourcing_CVPR_2018_paper.pdf


464
Title: Tell Me Where To Look: Guided Attention Inference Network
Type: Spotlight
Authors: Kunpeng Li, Northeastern University;Ziyan Wu, Siemens Corporation;Kuan-Chuan Peng, Siemens Corporation;Jan Ernst, Siemens Corporation;Yun Fu, Northeastern University;

465
Title: Tell Me Where To Look: Guided Attention Inference Network
Type: Poster
Authors: Kunpeng Li, Northeastern University;Ziyan Wu, Siemens Corporation;Kuan-Chuan Peng, Siemens Corporation;Jan Ernst, Siemens Corporation;Yun Fu, Northeastern University;

466
Title: Residual Dense Network for Image Super-Resolution
Type: Spotlight
Abstracts: In this paper, we propose dense feature fusion (DFF) for image super-resolution (SR). As the same content in different natural images often have various scales and angles of view, jointly leaning hierarchical features is essential for image SR. On the other hand, very deep convolutional neural network (CNN) has recently achieved great success for image SR and offered hierarchical features as well. However, most of deep CNN based SR models neglect to jointly make full use of the hierarchical features. In addition, dense connected layers would allow the network to be deeper, efficient to train, and more powerful. To embrace these observations, in our proposed DFF model, we fully exploit all the meaningful convolutional features in local and global manners. Specifically, we use dense connected convolutional layers to extract abundant local features. We use local feature fusion to adaptively learn more efficient features from preceding and current local features. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets show that our DFF achieves favorable performance against state-of-the-art methods quantitatively and visually.
Authors: Yulun Zhang, Northeastern University;Yapeng Tian, University of rochester;Yu Kong, Northeastern University;Bineng Zhong, Huaqiao University;Yun Fu, Northeastern University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf


467
Title: Residual Dense Network for Image Super-Resolution
Type: Poster
Abstracts: In this paper, we propose dense feature fusion (DFF) for image super-resolution (SR). As the same content in different natural images often have various scales and angles of view, jointly leaning hierarchical features is essential for image SR. On the other hand, very deep convolutional neural network (CNN) has recently achieved great success for image SR and offered hierarchical features as well. However, most of deep CNN based SR models neglect to jointly make full use of the hierarchical features. In addition, dense connected layers would allow the network to be deeper, efficient to train, and more powerful. To embrace these observations, in our proposed DFF model, we fully exploit all the meaningful convolutional features in local and global manners. Specifically, we use dense connected convolutional layers to extract abundant local features. We use local feature fusion to adaptively learn more efficient features from preceding and current local features. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets show that our DFF achieves favorable performance against state-of-the-art methods quantitatively and visually.
Authors: Yulun Zhang, Northeastern University;Yapeng Tian, University of rochester;Yu Kong, Northeastern University;Bineng Zhong, Huaqiao University;Yun Fu, Northeastern University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf


468
Title: Look at Boundary: A Boundary-Aware Face Alignment Algorithm
Type: Poster
Abstracts: We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundary-aware face alignment algorithm achieves 3.49% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92% mean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model are publicly available at https://wywu.github.io/projects/LAB/LAB.html
Authors: Wayne Wu, SenseTime;Chen Qian, SenseTime;Shuo Yang, ;Quan Wang, SenseTime;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Look_at_Boundary_CVPR_2018_paper.pdf


469
Title: Imagination-IQA: No-reference Image Quality Assessment via Adversarial Learning
Type: Poster
Authors: Kwan-Yee Lin, Peking University;

470
Title: Memory Matching Networks for One-Shot Image Recognition
Type: Poster
Abstracts: In this paper, we introduce the new ideas of augmenting Convolutional Neural Networks (CNNs) with Memory and learning to learn the network parameters for the unlabelled images on the fly in one-shot learning. Specifically, we present Memory Matching Networks (MM-Net) --- a novel deep architecture that explores the training procedure, following the philosophy that training and test conditions must match. Technically, MM-Net writes the features of a set of labelled images (support set) into memory and reads from memory when performing inference to holistically leverage the knowledge in the set. Meanwhile, a Contextual Learner employs the memory slots in a sequential manner to predict the parameters of CNNs for unlabelled images. The whole architecture is trained by once showing only a few examples per class and switching the learning from minibatch to minibatch, which is tailored for one-shot learning when presented with a few examples of new categories at test time. Unlike the conventional one-shot learning approaches, our MM-Net could output one unified model irrespective of the number of shots and categories. Extensive experiments are conducted on two public datasets, i.e., Omniglot and emph{mini}ImageNet, and superior results are reported when compared to state-of-the-art approaches. More remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95% to 99.28% and from 49.21% to 53.37% on emph{mini}ImageNet.
Authors: Qi Cai, University of Science and Technology of China;Yingwei Pan, University of Science and Technology of China;Ting Yao, Microsoft Research Asia;Hangzhou Dianzi University, China;Tao Mei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf


471
Title: 3D Human Pose Estimation in the Wild by Adversarial Learning
Type: Poster
Abstracts: Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor have been demonstrated through extensive experiments on two widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.
Authors: Wei Yang, The Chinese University of Hong Kong ;Wanli Ouyang, The University of Sydney;Xiaolong Wang, Carnegie Mellon University;Xiaogang Wang, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_3D_Human_Pose_CVPR_2018_paper.pdf


472
Title: Unsupervised Training for 3D Morphable Model Regression
Type: Spotlight
Abstracts: We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.
Authors: Kyle Genova, Princeton University;Forrester Cole, Google;Aaron Maschinot, Google;Daniel Vlasic, Google;Aaron Sarna, Google;William Freeman, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Genova_Unsupervised_Training_for_CVPR_2018_paper.pdf


473
Title: Unsupervised Training for 3D Morphable Model Regression
Type: Poster
Abstracts: We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.
Authors: Kyle Genova, Princeton University;Forrester Cole, Google;Aaron Maschinot, Google;Daniel Vlasic, Google;Aaron Sarna, Google;William Freeman, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Genova_Unsupervised_Training_for_CVPR_2018_paper.pdf


474
Title: Scalable Dense Non-rigid Structure-from-Motion: A Grassmannian Perspective
Type: Poster
Authors: Suryansh Kumar, Australian National University;Anoop Cherian, ;Yuchao Dai, Australian National University;Hongdong Li, Australian National University;

475
Title: IQA: Visual Question Answering in Interactive Environments
Type: Poster
Abstracts: We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: “Are there any apples in the fridge?” The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98.
Authors: Daniel Gordon, University of Washington;Ali Farhadi, ;Aniruddha Kembhavi, Allen Institute for Artificial Intelligence;Dieter Fox, University of Washington;Mohammad Rastegari, AI2;Joe Redmon, University of Washington;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf


476
Title: Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking
Type: Poster
Abstracts: Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Compared with SRDCF, STRCF with hand-crafted features provides a 5× speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF with deep features also performs favorably against state-of-the-art trackers and achieves an AUC score of 68.3% on OTB-2015.
Authors: Feng Li, Harbin Institute of Technology;Cheng Tian, Harbin Institute of Technology;Wangmeng Zuo, Harbin Institute of Technology;Lei Zhang, The Hong Kong Polytechnic University;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.pdf


477
Title: Low-shot Learning from Imaginary Data
Type: Spotlight
Authors: Yu-Xiong Wang, Carnegie Mellon University;Ross Girshick, ;Martial Hebert, ;Bharath Hariharan, Cornell University;

478
Title: Low-shot Learning from Imaginary Data
Type: Poster
Authors: Yu-Xiong Wang, Carnegie Mellon University;Ross Girshick, ;Martial Hebert, ;Bharath Hariharan, Cornell University;

479
Title: Deep Regression Forests for Age Estimation
Type: Poster
Abstracts: Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with inhomogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.
Authors: Wei Shen, Shanghai University;Yilu Guo, Shanghai University;Yan Wang, JHU;KAI ZHAO, Nankai University;Bo Wang, HikVision USA Inc.;Alan Yuille,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Regression_Forests_CVPR_2018_paper.pdf


480
Title: Partial Transfer Learning with Selective Adversarial Networks
Type: Spotlight
Authors: Zhangjie Cao, Tsinghua University;Mingsheng Long, Tsinghua University;Jianmin Wang,;

481
Title: Partial Transfer Learning with Selective Adversarial Networks
Type: Poster
Authors: Zhangjie Cao, Tsinghua University;Mingsheng Long, Tsinghua University;Jianmin Wang,;

482
Title: A Bi-directional Message Passing Model for Salient Object Detection
Type: Poster
Authors: Lu Zhang, Dalian University of Technolog;Ju Dai, Dalian University of Technolog;Huchuan Lu, Dalian University of Technology;You He, ;Gang Wang,;

483
Title: Transductive Unbiased Embedding for Zero-Shot Learning
Type: Poster
Abstracts: Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3~24.5% following generalized ZSL settings, and by a large margin of 0.2~16.2% following conventional ZSL settings.
Authors: Jie Song, Zhejiang University;Chengchao Shen, Zhejiang University;Yezhou Yang, Arizona State University;Yang Liu, ;Mingli Song, Zhejiang University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Transductive_Unbiased_Embedding_CVPR_2018_paper.pdf


484
Title: Scale-Transferrable Object Detection
Type: Poster
Abstracts: Scale problem lies in the heart of object detection. In this work, we develop a novel Scale-Transferrable Detection Network (STDN) for detecting multi-scale objects in images. In contrast to previous methods that simply combine object predictions from multiple feature maps from different network depths, the proposed network is equipped with embedded super-resolution layers (named as scale-transfer layer/module in this work) to explicitly explore the inter-scale consistency nature across multiple detection scales. Scale-transfer module naturally fits the base network with little computational cost. This module is further integrated with a dense convolutional network (DenseNet) to yield a one-stage object detector. We evaluate our proposed architecture on PASCAL VOC 2007 and MS COCO benchmark tasks and STDN obtains significant improvements over the comparable state-of-the-art detection models.
Authors: Peng Zhou, Sjtu;Bingbing Ni, ;Cong Geng, sjtu;jianguo Hu, Minivision;Yi Xu, Shanghai Jiao Tong University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Scale-Transferrable_Object_Detection_CVPR_2018_paper.pdf


485
Title: Crowd Counting with Deep Negative Correlation Learning
Type: Poster
Authors: Zenglin Shi, University of Bern;Le Zhang, Advanced Digital Sciences Cent;XiaoFeng Cao, university of technology sydney;Yun Liu, Nankai University;Zhengzhou University, China;Guoyan Zheng, University of Bern;

486
Title: Deep Cauchy Hashing for Hamming Space Retrieval
Type: Poster
Abstracts: Due to its computation efficiency and retrieval quality, hashing has been widely applied to approximate nearest neighbor search for large-scale image retrieval, while deep hashing further improves the retrieval quality by end-to-end representation learning and hash coding. With compact hash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a given Hamming radius to each query, by hash table lookups instead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small Hamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming space retrieval. This work presents Deep Cauchy Hashing (DCH), a novel deep hashing model that generates compact and concentrated binary hash codes to enable efficient and effective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs with Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate that DCH can generate highly concentrated hash codes and yield state-of-the-art Hamming space retrieval performance on three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.
Authors: Yue Cao, Tsinghua University;Mingsheng Long, Tsinghua University;Bin Liu, Tsinghua University;Jianmin Wang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.pdf


487
Title: Demo2Vec: Reasoning Object Affordances from Online Videos
Type: Poster
Authors: Te-Lin Wu, USC;Kuan Fang, Stanford University;Daniel Yang, University of Southern California;Joseph Lim, University of Southern California;

488
Title: GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition
Type: Poster
Abstracts: 3D shape recognition has attracted much attention recently. Its recent advances advocate the usage of deep features and achieve the state-of-the-art performance. However, existing deep features for 3D shape recognition are restricted to a view-to-shape setting, which learns the shape descriptor from the view-level feature directly. Despite the exciting progress on view-based 3D shape description, the intrinsic hierarchical correlation and discriminability among views have not been well exploited, which is important for 3D shape representation. To tackle this issue, in this paper, we propose a group-view convolutional neural network (GVCNN) framework for hierarchical correlation modeling towards discriminative 3D shape description. The proposed GVCNN framework is composed of a hierarchical view-group-shape architecture, i.e., from the view level, the group level and the shape level, which are organized using a grouping strategy. Concretely, we first use an expanded CNN to extract a view level descriptor. Then, a grouping module is introduced to estimate the content discrimination of each view, based on which all views can be splitted into different groups according to their discriminative level. A group level description can be further generated by pooling from view descriptors. Finally, all group level descriptors are combined into the shape level descriptor according to their discriminative weights. Experimental results and comparison with state-of-the-art methods show that our proposed GVCNN method can achieve a significant performance gain on both the 3D shape classification and retrieval tasks.
Authors: Yifan Feng, Xidian university;Zizhao Zhang, ;xibin Zhao, ;Rongrong Ji, ;Yue Gao, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.pdf


489
Title: An End-to-End TextSpotter with Explicit Alignment and Attention
Type: Poster
Authors: Tong He, The University of Adelaide;SIAT, CAS;Weilin Huang, The University of Oxford;Chunhua Shen, University of Adelaide;Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences;Changming Sun, CSIRO Data61;

490
Title: Stereoscopic Neural Style Transfer
Type: Poster
Abstracts: This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical real-time solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-to-end network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively.
Authors: Dongdong Chen, ;Lu Yuan, Microsoft Research Asia;Jing Liao, ;Nenghai Yu, ;Gang Hua, Microsoft Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Stereoscopic_Neural_Style_CVPR_2018_paper.pdf


491
Title: Bootstrapping the Performance of Webly Supervised Semantic Segmentation
Type: Poster
Abstracts: Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which are expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively.
Authors: Tong Shen, The University of Adelaide;Guosheng Lin, Nanyang Technological Universi;Chunhua Shen, University of Adelaide;Ian Reid,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Bootstrapping_the_Performance_CVPR_2018_paper.pdf


492
Title: Learning Markov Clustering Networks for Scene Text Detection
Type: Poster
Abstracts: A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is $1.5 imes$ speedup when compared with the fastest scene text detection algorithm.
Authors: ZICHUAN LIU, Nanyang Technological Universi;Guosheng Lin, Nanyang Technological Universi;Sheng Yang, Nanyang Technological University;Jiashi Feng, ;Weisi Lin, Nanyang Technological University;Wangling Goh, Nanyang Technological University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Markov_Clustering_CVPR_2018_paper.pdf


493
Title: Collaborative and Adversarial Network for Unsupervised domain adaptation
Type: Spotlight
Authors: Weichen Zhang, The University of Sydney;Wanli Ouyang, The University of Sydney;Dong Xu, ;Wen Li, ETH;

494
Title: Collaborative and Adversarial Network for Unsupervised domain adaptation
Type: Poster
Authors: Weichen Zhang, The University of Sydney;Wanli Ouyang, The University of Sydney;Dong Xu, ;Wen Li, ETH;

495
Title: Reflection Removal for Large-Scale 3D Point Clouds
Type: Poster
Abstracts: Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often exhibit reflection artifacts by glasses, which degrade the performance of related computer vision techniques. In this paper, we propose an efficient reflection removal algorithm for LS3DPCs. We first partition the unit sphere into local surface patches which are then classified into the ordinary patches and the glass patches according to the number of echo pulses from emitted laser pulses. Then we estimate the glass region of dominant reflection artifacts by measuring the reliability. We also detect and remove the virtual points using the conditions of the reflection symmetry and the geometric similarity. We test the performance of the proposed algorithm on LS3DPCs capturing real-world outdoor scenes, and show that the proposed algorithm estimates valid glass regions faithfully and removes the virtual points caused by reflection artifacts successfully.
Authors: Jae-Seong Yun, UNIST;Jae-Young Sim, UNIST;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yun_Reflection_Removal_for_CVPR_2018_paper.pdf


496
Title: Pose Transferrable Person Re-Identification
Type: Poster
Abstracts: Person re-identification (ReID) is an important task in the field of intelligent security. A key challenge is how to capture human pose variations, while existing benchmarks (i.e., Market1501, DukeMTMC-reID, CUHK03, etc.) do NOT provide sufficient pose coverage to train a robust ReID system. To address this issue, we propose a pose-transferrable person ReID framework which utilizes pose-transferred sample augmentations (i.e., with ID supervision) to enhance ReID model training. On one hand, novel training samples with rich pose variations are generated via transferring pose instances from MARS dataset, and they are added into the target dataset to facilitate robust training. On the other hand, in addition to the conventional discriminator of GAN (i.e., to distinguish between REAL/FAKE samples), we propose a novel guider sub-network which encourages the generated sample (i.e., with novel pose) towards better satisfying the ReID loss (i.e., cross-entropy ReID loss, triplet ReID loss). In the meantime, an alternative optimization procedure is proposed to train the proposed Generator-Guider-Discriminator network. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 show that our method achieves great performance improvement, and outperforms most state-of-the-art methods without elaborate designing the ReID model.
Authors: Jinxian Liu, Shanghai Jiao Tong University;Yichao Yan, Shanghai Jiao Tong University;Bingbing Ni, ;Peng Zhou, Sjtu;Shuo Cheng, SJTU;jianguo Hu, Minivision;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf


497
Title: Learning to Adapt Structured Output Space for Semantic Segmentation
Type: Spotlight
Abstracts: Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.
Authors: Yi-Hsuan Tsai, NEC Labs America;University of California, Merced;Samuel Schulter, NEC Labs;Kihyuk Sohn, NEC Laboratories America;Ming-Hsuan Yang, UC Merced;Manmohan Chandraker, NEC Labs America;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf


498
Title: Learning to Adapt Structured Output Space for Semantic Segmentation
Type: Poster
Abstracts: Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.
Authors: Yi-Hsuan Tsai, NEC Labs America;University of California, Merced;Samuel Schulter, NEC Labs;Kihyuk Sohn, NEC Laboratories America;Ming-Hsuan Yang, UC Merced;Manmohan Chandraker, NEC Labs America;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf


499
Title: Efficient Diverse Ensemble for Discriminative Co-Tracking
Type: Poster
Abstracts: Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks.
Authors: Kourosh Meshgi, Kyoto University;Shigeyuki Oba, Kyoto University;Shin Ishii, Kyoto University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.pdf


500
Title: Learning a Single Convolutional Super-Resolution Network for Multiple Degradations
Type: Poster
Abstracts: Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to non-blindly deal with multiple degradations. To address these issues, we propose a general framework with dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.
Authors: Kai Zhang, Harbin Institute of Technology;Wangmeng Zuo, Harbin Institute of Technology;Lei Zhang, The Hong Kong Polytechnic University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_a_Single_CVPR_2018_paper.pdf


501
Title: Probabilistic Plant Modeling via Multi-View Image-to-Image Translation
Type: Poster
Abstracts: This paper describes a method for inferring three-dimensional (3D) plant branch structures that are hidden under leaves from multi-view observations. Unlike previous geometric approaches that heavily rely on the visibility of the branches or use parametric branching models, our method makes statistical inferences of branch structures in a probabilistic framework. By inferring the probability of branch existence using a Bayesian extension of image-to-image translation applied to each of multi-view images, our method generates a probabilistic plant 3D model, which represents the 3D branching pattern that cannot be directly observed. Experiments demonstrate the usefulness of the proposed approach in generating convincing branch structures in comparison to prior approaches.
Authors: Takahiro Isokane, Osaka university;Fumio Okura, Osaka University;Ayaka Ide, Osaka University;Yasuyuki Matsushita, Osaka University;Yasushi Yagi, Osaka University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Isokane_Probabilistic_Plant_Modeling_CVPR_2018_paper.pdf


502
Title: Learning to Parse Wireframes in Images of Man-Made Environments
Type: Poster
Abstracts: In this paper, we propose a learning-based approach to the task of automatically extracting a "wireframe" representation for images of cluttered man-made environments. The wireframe contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than state-of-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation.
Authors: Kun Huang, Shanghaitech University;Yifan Wang, ShanghaiTech University;Zihan Zhou, Penn State University;Tianjiao Ding, ;Shenghua Gao, ShanghaiTech University;EECS, UC Berkeley;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf


503
Title: A Variational U-Net for Conditional Appearance and Shape Generation
Type: Spotlight
Abstracts: Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art.
Authors: IWR,Heidelberg University;Patrick Esser, Heidelberg University;Bjorn Ommer, Heidelberg;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Esser_A_Variational_U-Net_CVPR_2018_paper.pdf


504
Title: A Variational U-Net for Conditional Appearance and Shape Generation
Type: Poster
Abstracts: Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art.
Authors: IWR,Heidelberg University;Patrick Esser, Heidelberg University;Bjorn Ommer, Heidelberg;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Esser_A_Variational_U-Net_CVPR_2018_paper.pdf


505
Title: Learning to Find Good Correspondences
Type: Oral
Abstracts: We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while embedding global information in it, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data.
Authors: Kwang Moo Yi, EPFL;Eduard Trulls, ;Yuki Ono, Sony;Vincent Lepetit, TU Graz;Mathieu Salzmann, EPFL;Pascal Fua,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yi_Learning_to_Find_CVPR_2018_paper.pdf


506
Title: Learning to Find Good Correspondences
Type: Poster
Abstracts: We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while embedding global information in it, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data.
Authors: Kwang Moo Yi, EPFL;Eduard Trulls, ;Yuki Ono, Sony;Vincent Lepetit, TU Graz;Mathieu Salzmann, EPFL;Pascal Fua,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yi_Learning_to_Find_CVPR_2018_paper.pdf


507
Title: Actor and Action Video Segmentation from a Sentence
Type: Oral
Authors: Kirill Gavrilyuk, University of Amsterdam;Amir Ghodrati, University of Amsterdam;zhenyang Li, University of Amsterdam;Cees Snoek, University of Amsterdam;

508
Title: Actor and Action Video Segmentation from a Sentence
Type: Poster
Authors: Kirill Gavrilyuk, University of Amsterdam;Amir Ghodrati, University of Amsterdam;zhenyang Li, University of Amsterdam;Cees Snoek, University of Amsterdam;

509
Title: Towards a Mathematical Understanding of the Difficulty in Learning with Feedforward Neural Networks
Type: Poster
Authors: Hao Shen, Fortiss GmbH;

510
Title: Weakly-supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity Estimation
Type: Poster
Authors: Yong Zhang, CASIA;Weiming Dong, ;Bao-Gang Hu, CASIA;Qiang Ji, RPI;

511
Title: Maximum Classifier Discrepancy for Unsupervised Domain Adaptation
Type: Oral
Abstracts: In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics. To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at url{https://github.com/mil-tokyo/MCD_DA}
Authors: Kuniaki Saito, The University of Tokyo;Kohei Watanabe, ;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf


512
Title: Maximum Classifier Discrepancy for Unsupervised Domain Adaptation
Type: Poster
Abstracts: In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics. To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at url{https://github.com/mil-tokyo/MCD_DA}
Authors: Kuniaki Saito, The University of Tokyo;Kohei Watanabe, ;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf


513
Title: The power of ensembles for active learning in image classification
Type: Poster
Authors: William Beluch, Bosch Center for Artificial Intelligence;Tim Genewein, Robert Bosch Center for AI;Andreas N?rnberger, Otto-von-Guericke-Universit?t Magdeburg ;Jan K?hler, Bosch Center for AI;

514
Title: Memory Based Online Learning of Deep Representations from Video Streams
Type: Poster
Authors: Federico Pernici, MICC University of Florence;federico Bartoli, Micc - University of Florence;Matteo Bruni, Micc - University of Florence;Alberto Del Bimbo, University of Florence;

515
Title: Correlation Tracking via Joint Discrimination and Reliability Learning
Type: Poster
Abstracts: For visual tracking, an ideal filter learned by the correlation filter (CF) method should take both discrimination and reliability information. However, existing attempts usually focus on the former one while pay less attention to reliability learning. This may make the learned filter be dominated by the unexpected salient regions on the feature map, thereby resulting in model degradation. To address this issue, we propose a novel CF-based optimization problem to jointly model the discrimination and reliability information. First, we treat the filter as the element-wise product of a base filter and a reliability term. The base filter is aimed to learn the discrimination information between the target and backgrounds, and the reliability term encourages the final filter to focus on more reliable regions. Second, we introduce a local response consistency regular term to emphasize equal contributions of different regions and avoid the tracker being dominated by unreliable regions. The proposed optimization problem can be solved using the alternating direction method and speeded up in the Fourier domain. We conduct extensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to evaluate the proposed tracker. Experimental results show that our tracker performs favorably against other state-of-the-art trackers.
Authors: Chong Sun, DalianUniversityofTechnology;Dong Wang, DUT;Huchuan Lu, Dalian University of Technology;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Correlation_Tracking_via_CVPR_2018_paper.pdf


516
Title: Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks
Type: Poster
Abstracts: Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128 imes 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.
Authors: Wei Xiong, University of Rochester;Wenhan Luo, Tencent AI Lab;Lin Ma, Tencent AI Lab;Wei Liu, ;Jiebo Luo, University of Rochester;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xiong_Learning_to_Generate_CVPR_2018_paper.pdf


517
Title: Learning Discriminative Evaluation Metrics for Image Captioning
Type: Poster
Authors: Yin Cui, CornellTech;Guandao Yang, Cornell University;Andreas Veit, Cornel Tech ;Xun Huang, ;Serge Belongie,;

518
Title: Large Scale Fine-Grained Categorization and the Effectiveness of Domain-Specific Transfer Learning
Type: Poster
Authors: Yin Cui, CornellTech;Yang Song, Google;Chen Sun, Google;Andrew Howard, Google;Serge Belongie,;

519
Title: Curve Reconstruction via the Global Statistics of Natural Curves
Type: Poster
Abstracts: Reconstructing the missing parts of a curve has been the subject of much computational research, with applications in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually plausible completions. In this work we focus on reconstructing the underlying physically likely shape by utilizing the global statistics of natural curves. More specifically, we develop a reconstruction model that seeks the mean physical curve for a given inducer configuration. This simple model is both straightforward to compute and it is receptive to diverse additional information, but it requires enough samples for all curve configurations, a practical requirement that limits its effective utilization. To address this practical issue we explore and exploit statistical geometrical properties of natural curves, and in particular, we show that in many cases the mean curve is scale invariant and often times it is extensible. This, in turn, allows to boost the number of examples and thus the robustness of the statistics and its applicability. The reconstruction results are not only more physically plausible but they also lead to important insights on the reconstruction problem, including an elegant explanation why certain inducer configurations are more likely to yield consistent perceptual completions than others.
Authors: Ehud Barnea, Ben-Gurion University;Ohad Ben-Shahar, Ben-Gurion University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Barnea_Curve_Reconstruction_via_CVPR_2018_paper.pdf


520
Title: LAMV: Learning to align and match videos with kernelized temporal layers
Type: Poster
Authors: Lorenzo Baraldi, University of Modena;Matthijs Douze, ;Rita Cucchiara, ;Herve Jegou, Facebook AI Research;

521
Title: Attentive Generative Adversarial Network for Raindrop Removal from A Single Image
Type: Spotlight
Authors: Rui Qian, Peking University;Electrical and Computer Engineering, NUS;Wenhan Yang, Peking University;Jiajun Su, Peking University;Jiaying Liu, Peking University;

522
Title: Attentive Generative Adversarial Network for Raindrop Removal from A Single Image
Type: Poster
Authors: Rui Qian, Peking University;Electrical and Computer Engineering, NUS;Wenhan Yang, Peking University;Jiajun Su, Peking University;Jiaying Liu, Peking University;

523
Title: Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment
Type: Poster
Authors: Li Ding, MIT;Chenliang Xu, University of Rochester;

524
Title: Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers
Type: Poster
Abstracts: In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.
Authors: Stephan Richter, TU Darmstadt;Stefan Roth,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Richter_Matryoshka_Networks_Predicting_CVPR_2018_paper.pdf


525
Title: Deep Semantic Face Deblurring
Type: Poster
Abstracts: In this paper, we present an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (CNNs). As face images are highly structured and share several key semantic components (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. As such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep CNN. We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed.
Authors: Ziyi Shen, Beijing Institute of Technology;University of California, Merced;Tingfa Xu, Beijing Institute of Technology;Jan Kautz, NVIDIA;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Semantic_Face_CVPR_2018_paper.pdf


526
Title: Detail-Preserving Pooling in Deep Networks
Type: Oral
Abstracts: Most convolutional neural networks use some method for gradually downscaling the size of the hidden layers. This is commonly referred to as pooling, and is applied to reduce the number of parameters, improve invariance to certain distortions, and increase the receptive field size. Since pooling by nature is a lossy process, it is crucial that each such layer maintains the portion of the activations that is most important for the network's discriminability. Yet, simple maximization or averaging over blocks, max or average pooling, or plain downsampling in the form of strided convolutions are the standard. In this paper, we aim to leverage recent results on image downscaling for the purposes of deep learning. Inspired by the human visual system, which focuses on local spatial changes, we propose detail-preserving pooling (DPP), an adaptive pooling method that magnifies spatial changes and preserves important structural detail. Importantly, its parameters can be learned jointly with the rest of the network. We analyze some of its theoretical properties and show its empirical benefits on several datasets and networks, where DPP consistently outperforms previous pooling approaches.
Authors: Faraz Saeedan, TU Darmstadt;Nicolas Weber, ;Michael Goesele, TU Darmstadt;Stefan Roth,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Saeedan_Detail-Preserving_Pooling_in_CVPR_2018_paper.pdf


527
Title: Detail-Preserving Pooling in Deep Networks
Type: Poster
Abstracts: Most convolutional neural networks use some method for gradually downscaling the size of the hidden layers. This is commonly referred to as pooling, and is applied to reduce the number of parameters, improve invariance to certain distortions, and increase the receptive field size. Since pooling by nature is a lossy process, it is crucial that each such layer maintains the portion of the activations that is most important for the network's discriminability. Yet, simple maximization or averaging over blocks, max or average pooling, or plain downsampling in the form of strided convolutions are the standard. In this paper, we aim to leverage recent results on image downscaling for the purposes of deep learning. Inspired by the human visual system, which focuses on local spatial changes, we propose detail-preserving pooling (DPP), an adaptive pooling method that magnifies spatial changes and preserves important structural detail. Importantly, its parameters can be learned jointly with the rest of the network. We analyze some of its theoretical properties and show its empirical benefits on several datasets and networks, where DPP consistently outperforms previous pooling approaches.
Authors: Faraz Saeedan, TU Darmstadt;Nicolas Weber, ;Michael Goesele, TU Darmstadt;Stefan Roth,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Saeedan_Detail-Preserving_Pooling_in_CVPR_2018_paper.pdf


528
Title: Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation
Type: Spotlight
Abstracts: While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.
Authors: Yen-Cheng Liu, National Taiwan University;Yu-Ying Yeh, National Taiwan University;Tzu-Chien Fu, Northwestern University;Wei-Chen Chiu, National Chiao Tung University;Sheng-De Wang, National Taiwan University;Yu-Chiang Frank Wang, Academia Sinica;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf


529
Title: Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation
Type: Poster
Abstracts: While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.
Authors: Yen-Cheng Liu, National Taiwan University;Yu-Ying Yeh, National Taiwan University;Tzu-Chien Fu, Northwestern University;Wei-Chen Chiu, National Chiao Tung University;Sheng-De Wang, National Taiwan University;Yu-Chiang Frank Wang, Academia Sinica;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf


530
Title: Visual to Sound: Generating Natural Sound for Videos in the Wild
Type: Poster
Abstracts: As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.
Authors: Yipin Zhou, UNC-Chapel Hill;Zhaowen Wang, Adobe;Chen Fang, Adobe Research;Trung Bui, ;Tamara Berg, University on North carolina;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Visual_to_Sound_CVPR_2018_paper.pdf


531
Title: Deep Reinforcement Learning of Region Proposal Networks for Object Detection
Type: Poster
Abstracts: We propose drl-RPN, a deep reinforcement learning-based visual recognition model consisting of a sequential region proposal network (RPN) and an object detector. In contrast to typical RPNs, where candidate object regions (RoIs) are selected greedily via class-agnostic NMS, drl-RPN optimizes an objective closer to the final detection task. This is achieved by replacing the greedy RoI selection process with a sequential attention mechanism which is trained via deep reinforcement learning (RL). Our model is capable of accumulating class-specific evidence over time, potentially affecting subsequent proposals and classification scores, and we show that such context integration significantly boosts detection accuracy. Moreover, drl-RPN automatically decides when to stop the search process and has the benefit of being able to jointly learn the parameters of the policy and the detector, both represented as deep networks. Our model can further learn to search over a wide range of exploration-accuracy trade-offs making it possible to specify or adapt the exploration extent at test time. The resulting search trajectories are image- and category-dependent, yet rely only on a single policy over all object categories. Results on the MS COCO and PASCAL VOC challenges show that our approach outperforms established, typical state-of-the-art object detection pipelines.
Authors: Aleksis Pirinen, Lund University;Cristian Sminchisescu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.pdf


532
Title: When will you do what? - Anticipating Temporal Occurrences of Activities
Type: Poster
Authors: Alexander Richard, University of Bonn;University of Bonn, Germany;Yazan Abu Farha, University of Bonn;

533
Title: Pixel-Wise Metric Learning for Blazingly Fast Video Object Segmentation
Type: Poster
Authors: Yuhua Chen, CVL@ETHZ;Jordi Pont-Tuset, ETHZ;Alberto Montes, ETHZ;Luc Van Gool, KTH;

534
Title: Global versus Localized Generative Adversarial Nets
Type: Poster
Authors: Guo-Jun Qi, University of Central Florida;Liheng Zhang, University of Central Florida;Hao Hu, University of Central Florida;

535
Title: SeGAN: Segmenting and Generating the Invisible
Type: Spotlight
Abstracts: Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder-occludee relations, our method can infer depth layering.
Authors: KIANA EHSANI, 1993;Roozbeh Mottaghi, Allen Institute for Artificial Intelligence;Ali Farhadi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ehsani_SeGAN_Segmenting_and_CVPR_2018_paper.pdf


536
Title: SeGAN: Segmenting and Generating the Invisible
Type: Poster
Abstracts: Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder-occludee relations, our method can infer depth layering.
Authors: KIANA EHSANI, 1993;Roozbeh Mottaghi, Allen Institute for Artificial Intelligence;Ali Farhadi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ehsani_SeGAN_Segmenting_and_CVPR_2018_paper.pdf


537
Title: Name-removed-for-review: A Multi-camera HD Dataset for Dense Unscripted Pedestrian Detection
Type: Poster
Authors: Tatjana Chavdarova, Idiap and EPFL;Pierre Baqu?, EPFL;Andrii Maksai, ;ST?PHANE BOUQUET, EPFL;Cijo Jose, Idiap and EPFL;Louis Lettry, ETH Z?rich;Francois Fleuret, Idiap Research Institute;Pascal Fua, ;Luc Van Gool, KTH;

538
Title: DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion
Type: Poster
Authors: Zhishuai Zhang, Johns Hopkins University;Cihang Xie, JHU;Jianyu Wang, ;Lingxi Xie, UCLA;Alan Yuille, JHU;

539
Title: Data Distillation: Towards Omni-Supervised Learning
Type: Poster
Abstracts: We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.
Authors: Ilija Radosavovic, Facebook AI Research;Menlo Park, USA;Ross Girshick, ;Georgia Gkioxari, Facebook;Kaiming He,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.pdf


540
Title: Deep Photo Enhancer: Unsupervised Learning of Image Enhancement from Photographs with GANs
Type: Spotlight
Authors: Yu-Sheng Chen, National Taiwan University;Yu-Ching Wang, National Taiwan University;Man-Hsin Kao, National Taiwan University;Yung-Yu Chuang, National Taiwan University;

541
Title: Deep Photo Enhancer: Unsupervised Learning of Image Enhancement from Photographs with GANs
Type: Poster
Authors: Yu-Sheng Chen, National Taiwan University;Yu-Ching Wang, National Taiwan University;Man-Hsin Kao, National Taiwan University;Yung-Yu Chuang, National Taiwan University;

542
Title: Neighbors Do Help: Deeply Exploiting Local Structures of Point Clouds
Type: Poster
Authors: Yiru Shen, Clemson University;Chen Feng, MERL;Yaoqing Yang, Carnegie Mellon University;Dong Tian, Mitsubishi Electric Research Laboratories;

543
Title: Controllable Video Generation with Sparse Trajectories
Type: Poster
Authors: Zekun Hao, ;Xun Huang, ;Serge Belongie,;

544
Title: Context Embedding Networks
Type: Spotlight
Abstracts: Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. Similarity is a multi-dimensional concept that varies from individual to individual. However, existing models for learning crowd embeddings typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs). In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the attributes highlighted by a set of images. Experiments on three noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.
Authors: Kun ho Kim, Caltech;Oisin Mac Aodha, Caltech;California Institute of Technology, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Context_Embedding_Networks_CVPR_2018_paper.pdf


545
Title: Context Embedding Networks
Type: Poster
Abstracts: Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. Similarity is a multi-dimensional concept that varies from individual to individual. However, existing models for learning crowd embeddings typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs). In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the attributes highlighted by a set of images. Experiments on three noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.
Authors: Kun ho Kim, Caltech;Oisin Mac Aodha, Caltech;California Institute of Technology, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Context_Embedding_Networks_CVPR_2018_paper.pdf


546
Title: PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image
Type: Spotlight
Authors: Chen Liu, WUSTL;Jimei Yang, ;Duygu Ceylan, ;Ersin Yumer, Argo AI;Yasutaka Furukawa,;

547
Title: PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image
Type: Poster
Authors: Chen Liu, WUSTL;Jimei Yang, ;Duygu Ceylan, ;Ersin Yumer, Argo AI;Yasutaka Furukawa,;

548
Title: Multi-Task Adversarial Network for Disentangled Feature Learning
Type: Spotlight
Abstracts: We address the problem of image feature learning for the applications where multiple factors exist in the image generation process and only some factors are of our interest. We present a novel multi-task adversarial network based on an encoder-discriminator-generator architecture. The encoder extracts a disentangled feature representation for the factors of interest. The discriminators classify each of the factors as individual tasks. The encoder and the discriminators are trained cooperatively on factors of interest, but in an adversarial way on factors of distraction. The generator provides further regularization on the learned feature by reconstructing images with shared factors as the input image. We design a new optimization scheme to stabilize the adversarial optimization process when multiple distributions need to be aligned. The experiments on face recognition and font recognition tasks show that our method outperforms the state-of-the-art methods in terms of both recognizing the factors of interest and generalization to images with unseen variations.
Authors: Yang Liu, University of Cambridge;Zhaowen Wang, Adobe;Hailin Jin, ;Ian Wassell,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.pdf


549
Title: Multi-Task Adversarial Network for Disentangled Feature Learning
Type: Poster
Abstracts: We address the problem of image feature learning for the applications where multiple factors exist in the image generation process and only some factors are of our interest. We present a novel multi-task adversarial network based on an encoder-discriminator-generator architecture. The encoder extracts a disentangled feature representation for the factors of interest. The discriminators classify each of the factors as individual tasks. The encoder and the discriminators are trained cooperatively on factors of interest, but in an adversarial way on factors of distraction. The generator provides further regularization on the learned feature by reconstructing images with shared factors as the input image. We design a new optimization scheme to stabilize the adversarial optimization process when multiple distributions need to be aligned. The experiments on face recognition and font recognition tasks show that our method outperforms the state-of-the-art methods in terms of both recognizing the factors of interest and generalization to images with unseen variations.
Authors: Yang Liu, University of Cambridge;Zhaowen Wang, Adobe;Hailin Jin, ;Ian Wassell,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.pdf


550
Title: Low-shot learning with large-scale diffusion
Type: Poster
Authors: Matthijs Douze, ;Arthur Szlam, Facebook AI Research;Bharath Hariharan, Cornell University;Herve Jegou, Facebook AI Research;

551
Title: Learning from Synthetic Data: Semantic Segmentation using Generative Adversarial Networks
Type: Spotlight
Authors: Swami Sankaranarayanan, University of Maryland;Yogesh Balaji, University of Maryland;Arpit Jain, ;Ser-Nam Lim, GE Global Research;University of Maryland, USA;

552
Title: Learning from Synthetic Data: Semantic Segmentation using Generative Adversarial Networks
Type: Poster
Authors: Swami Sankaranarayanan, University of Maryland;Yogesh Balaji, University of Maryland;Arpit Jain, ;Ser-Nam Lim, GE Global Research;University of Maryland, USA;

553
Title: Sketch-a-Classifier: Sketch-based Photo Classifier Generation
Type: Spotlight
Authors: Conghui Hu, Queen Mary University of Londo;Da Li, ;Yi-Zhe Song, ;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;

554
Title: Sketch-a-Classifier: Sketch-based Photo Classifier Generation
Type: Poster
Authors: Conghui Hu, Queen Mary University of Londo;Da Li, ;Yi-Zhe Song, ;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;

555
Title: VizWiz Grand Challenge: Answering Visual Questions from Blind People
Type: Spotlight
Authors: Danna Gurari, University of Texas at Austin;Qing Li, USTC;Abigale Stangl, ;Anhong Guo, ;Chi Lin, ;Kristen Grauman, ;Jiebo Luo, University of Rochester;Jeffrey Bigham,;

556
Title: VizWiz Grand Challenge: Answering Visual Questions from Blind People
Type: Poster
Authors: Danna Gurari, University of Texas at Austin;Qing Li, USTC;Abigale Stangl, ;Anhong Guo, ;Chi Lin, ;Kristen Grauman, ;Jiebo Luo, University of Rochester;Jeffrey Bigham,;

557
Title: Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks
Type: Poster
Abstracts: It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if an agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned "look around" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments.
Authors: Dinesh Jayaraman, UT Austin ;Kristen Grauman,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jayaraman_Learning_to_Look_CVPR_2018_paper.pdf


558
Title: Direct Shape Regression Networks for End-to-End Face Alignment
Type: Poster
Abstracts: Face alignment has been extensively studied in computer vision community due to its fundamental role in facial analysis, but it remains an unsolved problem. The major challenges lie in the highly nonlinear relationship between face images and associated facial shapes, which is coupled by underlying correlation of landmarks. Existing methods mainly rely on cascaded regression, suffering from intrinsic shortcomings, e.g., strong dependency on initialization and failure to exploit landmark correlations. In this paper, we propose the direct shape regression network (DSRN) for end-to-end face alignment by jointly handling the aforementioned challenges in a unified framework. Specifically, by deploying doubly convolutional layer and by using the Fourier feature pooling layer proposed in this paper, DSRN efficiently constructs strong representations to disentangle highly nonlinear relationships between images and shapes; by incorporating a linear layer of low-rank learning, DSRN effectively encodes correlations of landmarks to improve performance. DSRN leverages the strengths of kernels for nonlinear feature extraction and neural networks for structured prediction, and provides the first end-to-end learning architecture for direct face alignment. Its effectiveness and generality are validated by extensive experiments on five benchmark datasets, including AFLW, 300W, CelebA, MAFL, and 300VW. All empirical results demonstrate that DSRN consistently produces high performance and in most cases surpasses state-of-the-art.
Authors: Xin Miao, UT Arlington;Xiantong Zhen, Beihang University;Vassilis Athitsos, University of Texas at Arlington;Xianglong Liu, Beihang University;Cheng Deng, Xidian University;Heng Huang, University of Pittsburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Miao_Direct_Shape_Regression_CVPR_2018_paper.pdf


559
Title: Multi-scale Location-aware Kernel Representation for Object Detection
Type: Poster
Authors: Hao Wang, Harbin Institute of Technology;Qilong Wang, ;Mingqi Gao, Harbin Institute of Technology;Peihua Li, ;Wangmeng Zuo, Harbin Institute of Technology;

560
Title: Multistage Adversarial Losses for Pose-Based Human Image Synthesis
Type: Spotlight
Abstracts: Human image synthesis has extensive practical applications e.g. person re-identification and data augmentation for human pose estimation. However, it is much more challenging than rigid object synthesis, e.g. cars and chairs, due to the variability of human posture. In this paper, we propose a pose-based human image synthesis method which can keep the human posture unchanged in novel viewpoints. Furthermore, we adopt multistage adversarial losses separately for the foreground and background generation, which fully exploits the multi-modal characteristics of generative loss to generate more realistic looking images. We perform extensive experiments on the Human3.6M dataset and verify the effectiveness of each stage of our method. The generated human images not only keep the same pose as the input image, but also have clear detailed foreground and background. The quantitative comparison results illustrate that our approach achieves much better results than several state-of-the-art methods.
Authors: Institute of Automation, Chine;Wei Wang, ;Liang Wang, unknown;Tieniu Tan, NLPR China;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.pdf


561
Title: Multistage Adversarial Losses for Pose-Based Human Image Synthesis
Type: Poster
Abstracts: Human image synthesis has extensive practical applications e.g. person re-identification and data augmentation for human pose estimation. However, it is much more challenging than rigid object synthesis, e.g. cars and chairs, due to the variability of human posture. In this paper, we propose a pose-based human image synthesis method which can keep the human posture unchanged in novel viewpoints. Furthermore, we adopt multistage adversarial losses separately for the foreground and background generation, which fully exploits the multi-modal characteristics of generative loss to generate more realistic looking images. We perform extensive experiments on the Human3.6M dataset and verify the effectiveness of each stage of our method. The generated human images not only keep the same pose as the input image, but also have clear detailed foreground and background. The quantitative comparison results illustrate that our approach achieves much better results than several state-of-the-art methods.
Authors: Institute of Automation, Chine;Wei Wang, ;Liang Wang, unknown;Tieniu Tan, NLPR China;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.pdf


562
Title: MoCoGAN: Decomposing Motion and Content for Video Generation
Type: Poster
Abstracts: Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan.
Authors: Sergey Tulyakov, ;Ming-Yu Liu, NVIDIA;Xiaodong Yang, NVIDIA;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf


563
Title: Joint Pose and Expression Modeling for Facial Expression Recognition
Type: Poster
Abstracts: Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.
Authors: Feifei Zhang, Jiangsu University;Tianzhu Zhang, CASIA;Department of Computer Science and Communication Engineering, Jiangsu University;Changsheng Xu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Joint_Pose_and_CVPR_2018_paper.pdf


564
Title: Triplet-Center Loss for Multi-View 3D Object Retrieval
Type: Poster
Abstracts: Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared to the state-of-the-arts.
Authors: Xinwei He, HUST;Yang Zhou, Huazhong University of Science and Technology;Zhichao Zhou, Huazhong University of Science and Technology;Song Bai, HUST;Xiang Bai, Huazhong University of Science and Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/He_Triplet-Center_Loss_for_CVPR_2018_paper.pdf


565
Title: Beyond Holistic Object Recognition: Enriching Image Understanding with Part States
Type: Poster
Authors: Cewu Lu, Shanghai Jiao Tong University;hao Su, ;CK Tang, HKUST;

566
Title: Recurrent Residual Module for Fast Inference in Videos
Type: Poster
Abstracts: Deep convolutional neural networks (CNNs) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection. However, running CNN inference on video requires numerous computation and is usually slow. In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames to largely reduce the redundant computation. One unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed. The experiments show that, while maintaining the similar recognition performance, our RRM yields averagely 2× acceleration on the commonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8−12× faster than the original dense models on the efﬁcient inference engine), and impressively 9× acceleration on some binary networks such as XNOR-Nets (thus 500× faster than the original model). We further verify the effectiveness of the RRM on speeding CNNs for video pose estimation and video object detection.
Authors: Bowen Pan, Shanghai Jiao Tong University;Wuwei Lin, Shanghai Jiao Tong University;Xiaolin Fang, Zhejiang University;Chaoqin Huang, Shanghai Jiaotong University;Bolei Zhou, Massachuate Institute of Technology;Cewu Lu, Shanghai Jiao Tong University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Recurrent_Residual_Module_CVPR_2018_paper.pdf


567
Title: Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines
Type: Spotlight
Authors: Shuqin Xie, SJTU;Cewu Lu, Shanghai Jiao Tong University;Zitian Chen, Fudan University;Chao Xu, Shanghai Jiao Tong University;

568
Title: Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines
Type: Poster
Authors: Shuqin Xie, SJTU;Cewu Lu, Shanghai Jiao Tong University;Zitian Chen, Fudan University;Chao Xu, Shanghai Jiao Tong University;

569
Title: Separating Style and Content for Generalized Style Transfer
Type: Spotlight
Abstracts: Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.
Authors: Yexun Zhang, Shanghai Jiao Tong University;Ya Zhang, ;Wenbin Cai,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Separating_Style_and_CVPR_2018_paper.pdf


570
Title: Separating Style and Content for Generalized Style Transfer
Type: Poster
Abstracts: Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.
Authors: Yexun Zhang, Shanghai Jiao Tong University;Ya Zhang, ;Wenbin Cai,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Separating_Style_and_CVPR_2018_paper.pdf


571
Title: LiDAR-Video Driving Dataset: Learning Driving Policies Effectively
Type: Poster
Abstracts: Learning autonomous-driving policies is one of the most challenging but promising tasks for computer vision. Most researchers believe that future research and applications should combine cameras, video recorders and laser scanners to obtain comprehensive semantic understanding of real traffic. However, current approaches only learn from large-scale videos, due to the lack of benchmarks that consist of precise laser-scanner data. In this paper, we are the first to propose a LiDAR-Video dataset, which provides large-scale high-quality point clouds scanned by a Velodyne laser, videos recorded by a dashboard camera and standard drivers' behaviors. Extensive experiments demonstrate that extra depth information help networks to determine driving policies indeed.
Authors: Yiping Chen, Xiamen University;Jingkang Wang, Shanghai Jiao Tong University;Cewu Lu, Shanghai Jiao Tong University;Zhipeng Luo, Xiamen University;Jonathan Li, University of Waterloo;Han Xue, Shanghai Jiao Tong University;Cheng Wang, Xiamen University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf


572
Title: Geometry-Aware Scene Text Detection with Instance Transformation Network
Type: Poster
Authors: Fangfang Wang, Zhejiang University;Liming Zhao, Zhejiang University;Xi Li, Zhejiang University;Xinchao Wang, ;Dacheng Tao, University of Sydney;

573
Title: Temporal Hallucinating for Action Recognition with Few Still Images
Type: Poster
Authors: Lei Zhou, ;SIAT, CAS;Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences;

574
Title: Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons
Type: Poster
Abstracts: Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous in virtually all machine learning and computer vision challenges; however, advancements in CNNs have arguably reached an engineering saturation point where incremental novelty results in minor performance gains. Although there is evidence that object classification has reached human levels on narrowly defined tasks, for general applications, the biological visual system is far superior to that of any computer. Research reveals there are numerous missing components in feed-forward deep neural networks that are critical in mammalian vision. The brain does not work solely in a feed-forward fashion, but rather all of the neurons are in competition with each other; neurons are integrating information in a bottom up and top down fashion and incorporating expectation and feedback in the modeling process. Furthermore, our visual cortex is working in tandem with our parietal lobe, integrating sensory information from various modalities. In our work, we sought to improve upon the standard feed-forward deep learning model by augmenting them with biologically inspired concepts of sparsity, top down feedback, and lateral inhibition. We define our model as a sparse coding problem using hierarchical layers. We solve the sparse coding problem with an additional top down feedback error driving the dynamics of the neural network. While building and observing the behavior of our model, we were fascinated that multimodal, invariant neurons naturally emerged that mimicked, "Halle Berry neurons" found in the human brain. These neurons trained in our sparse model learned to respond to high level concepts from multiple modalities, which is not the case with a standard feed-forward autoencoder. Furthermore, our sparse representation of multimodal signals demonstrates qualitative and quantitative superiority to the standard feed-forward joint embedding in common vision and machine learning tasks.
Authors: Edward Kim, ;Darryl Hannan, ;Garrett Kenyon,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Deep_Sparse_Coding_CVPR_2018_paper.pdf


575
Title: Learning Spatial-Aware Regressions for Visual Tracking
Type: Spotlight
Abstracts: In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method.
Authors: Chong Sun, DalianUniversityofTechnology;Dong Wang, DUT;Huchuan Lu, Dalian University of Technology;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Learning_Spatial-Aware_Regressions_CVPR_2018_paper.pdf


576
Title: Learning Spatial-Aware Regressions for Visual Tracking
Type: Poster
Abstracts: In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method.
Authors: Chong Sun, DalianUniversityofTechnology;Dong Wang, DUT;Huchuan Lu, Dalian University of Technology;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Learning_Spatial-Aware_Regressions_CVPR_2018_paper.pdf


577
Title: Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes
Type: Poster
Abstracts: While people tracking has been greatly improved over the recent years, crowd scenes remain particularly challenging for people tracking due to heavy occlusions, high crowd density, and significant appearance variation. To address these challenges, we first design a Sparse Kernelized Correlation Filter (S-KCF) to suppress target response variations caused by occlusions and illumination changes, and spurious responses due to similar distractor objects. We then propose a people tracking framework that fuses the S-KCF response map with an estimated crowd density map using a convolutional neural network (CNN), yielding a refined response map. To train the fusion CNN, we propose a two-stage strategy to gradually optimize the parameters. The first stage is to train a preliminary model in batch mode with image patches selected around the targets, and the second stage is to fine-tune the preliminary model using the real frame-by-frame tracking process. Our density fusion framework can significantly improves people tracking in crowd scenes, and can also be combined with other trackers to improve the tracking performance. We validate our framework on two crowd video datasets: UCSD and PETS2009.
Authors: Weihong Ren, City University of Hong Kong;Di Kang, ;Shenyang Institute of Automation, Chinese Academy of Sciences;City University of Hong Kong, Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Fusing_Crowd_Density_CVPR_2018_paper.pdf


578
Title: Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation
Type: Spotlight
Abstracts: Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn’t need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84:3% on ICDAR2015 and 81:5% on MSRA-TD500.
Authors: Pengyuan Lyu, Huazhong University of Science and Technology;Cong Yao, Huazhong University of Science and Technology;Wenhao Wu, Megvii;Shuicheng Yan, National University of Singapore;Xiang Bai, Huazhong University of Science and Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lyu_Multi-Oriented_Scene_Text_CVPR_2018_paper.pdf


579
Title: Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation
Type: Poster
Abstracts: Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn’t need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84:3% on ICDAR2015 and 81:5% on MSRA-TD500.
Authors: Pengyuan Lyu, Huazhong University of Science and Technology;Cong Yao, Huazhong University of Science and Technology;Wenhao Wu, Megvii;Shuicheng Yan, National University of Singapore;Xiang Bai, Huazhong University of Science and Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lyu_Multi-Oriented_Scene_Text_CVPR_2018_paper.pdf


580
Title: Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis
Type: Poster
Abstracts: We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches.
Authors: Seunghoon Hong, POSTECH;Dingdong Yang, University of Michigan;Jongwook Choi, University of Michigan;University of Michigan, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.pdf


581
Title: Optimal Structured Light a la Carte
Type: Spotlight
Authors: Parsa Mirdehghan, University of Toronto;Wenzheng Chen, UofT;Kyros Kutulakos,;

582
Title: Optimal Structured Light a la Carte
Type: Poster
Authors: Parsa Mirdehghan, University of Toronto;Wenzheng Chen, UofT;Kyros Kutulakos,;

583
Title: FOTS: Fast Oriented Text Spotting with a Unified Network
Type: Poster
Authors: Xuebo Liu, SenseTime Group Ltd.;Ding Liang, Sensetime;Shi Yan, SenseTime;Dagui Chen, SenseTime;Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences;Junjie Yan,;

584
Title: Deep Marching Cubes: Learning Explicit Surface Representations
Type: Poster
Abstracts: Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., TSDF) from which 3D surface meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.
Authors: Yiyi Liao, Zhejiang University;Simon Donn?, Ghent University;Andreas Geiger, MPI Tuebingen / ETH Zuerich;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Deep_Marching_Cubes_CVPR_2018_paper.pdf


585
Title: Learning 3D Shape Completion from Point Clouds with Weak Supervision
Type: Poster
Authors: David Stutz, MPI Saarbruecken;Andreas Geiger, MPI Tuebingen / ETH Zuerich;

586
Title: A General Two-Step Quantization Approach for Low-bit Neural Networks with High Accuracy
Type: Poster
Authors: Peisong Wang, CASIA;Qinghao Hu, Chinese Academy of Sciences;Yifan Zhang, CASIA;Jian Cheng, Chinese Academy of Sciences;

587
Title: Clinical Skin Lesion Diagnosis using Representations Inspired by Dermatologist Criteria
Type: Poster
Authors: Jufeng Yang, Nankai University;Xiaoxiao Sun, ;Jie Liang, ;Paul Rosin,;

588
Title: RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials
Type: Spotlight
Authors: Despoina Paschalidou, MPI Tuebingen;Carolin Schmitt, MPI Tuebingen;Osman Ulusoy, microsoft corporation;Luc Van Gool, KTH;Andreas Geiger, MPI Tuebingen / ETH Zuerich;

589
Title: RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials
Type: Poster
Authors: Despoina Paschalidou, MPI Tuebingen;Carolin Schmitt, MPI Tuebingen;Osman Ulusoy, microsoft corporation;Luc Van Gool, KTH;Andreas Geiger, MPI Tuebingen / ETH Zuerich;

590
Title: Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition
Type: Poster
Authors: Jinmian Ye, University of Electronic Science and Technology of China;Linnan Wang, Brown;Guangxi Li, UESTC;Di Chen, ;School of Computing, University of Utah;Zenglin Xu, University of Electronic Science and Technology of China;

591
Title: Convolutional Neural Networks with Alternately Updated Clique
Type: Oral
Authors: Yibo Yang, Peking Univ.;Zhisheng Zhong, ;Tiancheng Shen, ;Peking University, China;

592
Title: Convolutional Neural Networks with Alternately Updated Clique
Type: Poster
Authors: Yibo Yang, Peking Univ.;Zhisheng Zhong, ;Tiancheng Shen, ;Peking University, China;

593
Title: Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition
Type: Poster
Authors: Yansong Tang, Tsinghua University;Yi Tian, ;Peiyang Li, ;Jiwen Lu, Tsinghua University;Jie Zhou,;

594
Title: Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present
Type: Poster
Authors: Xinpeng Chen, Wuhan University;Lin Ma, Tencent AI Lab;Wenhao Jiang, Tencent AI Lab;Jian Yao, ;Wei Liu,;

595
Title: Dimensionalitys Blessing: Detecting the distributions underlying images
Type: Poster
Authors: Wen-Yan Lin, ADSC;Yasuyuki Matsushita, Osaka University;Siying Liu, I2r.a-star.edu.sg;Jianhuang Lai, Sun Yat-sen University;

596
Title: Learning to Promote Saliency Detectors
Type: Poster
Abstracts: The categories and appearance of salient objects vary from image to image, therefore, saliency detection is an image-specific task. Due to lack of large-scale saliency training data, using deep neural networks (DNNs) with pre-training is difficult to precisely capture the image-specific saliency cues. To solve this issue, we formulate a zero-shot learning problem to promote existing saliency detectors. Concretely, a DNN is trained as an embedding function to map pixels and the attributes of the salient/background regions of an image into the same metric space, in which an image-specific classifier is learned to classify the pixels. Since the image-specific task is performed by the classifier, the DNN embedding effectively plays the role of a general feature extractor. Compared with transferring the learning to a new recognition task using limited data, this formulation makes the DNN learn more effectively from small data. Extensive experiments on five data sets show that our method significantly improves accuracy of existing methods and compares favorably against state-of-the-art approaches.
Authors: Yu Zeng, Dalian University of Technology;Huchuan Lu, Dalian University of Technology;Lihe Zhang, Dalian University of Technology;DUT, student;Ali Borji, UCF;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zeng_Learning_to_Promote_CVPR_2018_paper.pdf


597
Title: Fully Convolutional Adaptation Networks for Semantic Segmentation
Type: Poster
Abstracts: The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the ``style" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting.
Authors: Yiheng Zhang, University of Science and Technology of China;Zhaofan Qiu, University of Science and Technology of China;Ting Yao, Microsoft Research Asia;Dong Liu, Univ Sci Tech China;Tao Mei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.pdf


598
Title: Object Referring in Videos with Language and Human Gaze
Type: Poster
Authors: Arun Balajee Vasudevan , ETH Zurich;Dengxin Dai, ETH Zurich;Luc Van Gool, KTH;

599
Title: Learning Pose Specific Representations by Predicting different Views
Type: Spotlight
Authors: Georg Poier, Graz University of Technology;David Schinagl, ;Horst Bischof,;

600
Title: Learning Pose Specific Representations by Predicting different Views
Type: Poster
Authors: Georg Poier, Graz University of Technology;David Schinagl, ;Horst Bischof,;

601
Title: Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images
Type: Poster
Authors: Mahdi Rad, TUG;Markus Oberweger, ;Vincent Lepetit, TU Graz;

602
Title: A Papier-M?ch? Approach to Learning 3D Surface Generation
Type: Spotlight
Authors: Thibault GROUEIX, ?cole des ponts ParisTech;Bryan Russell, Adobe;Mathew Fisher, Adobe Systems;Mathieu Aubry, ;Vladimir Kim, Adobe Research;

603
Title: A Papier-M?ch? Approach to Learning 3D Surface Generation
Type: Poster
Authors: Thibault GROUEIX, ?cole des ponts ParisTech;Bryan Russell, Adobe;Mathew Fisher, Adobe Systems;Mathieu Aubry, ;Vladimir Kim, Adobe Research;

604
Title: Deep PhaseNet for Video Frame Interpolation
Type: Poster
Authors: Simone Meyer, ETH Zurich;Abdelaziz Djelouah, The Walt Disney Company;Christopher Schroers, Disney Research Zurich;Brian McWilliams, ;Alexander Sorkine-Hornung, ;Markus Gross,;

605
Title: Non-blind Deblurring: Handling Kernel Uncertainty with CNNs
Type: Poster
Authors: Subeesh Vasu, IIT Madras;Venkatesh Reddy Maligireddy, IIT Madras;A.N. Rajagopalan, IIT Madras;

606
Title: CosFace: Large Margin Cosine Loss for Deep Face Recognition
Type: Poster
Abstracts: Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.
Authors: Hao Wang, ;Yitong Wang, Tencent AI Lab;Zheng Zhou, ;xing Ji, ;Dihong Gong, ;Zhifeng Li, ;Jingchao Zhou, ;Wei Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_CosFace_Large_Margin_CVPR_2018_paper.pdf


607
Title: Lightweight Probabilistic Deep Networks
Type: Poster
Abstracts: Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.
Authors: Jochen Gast, TU Darmstadt;Stefan Roth,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.pdf


608
Title: Occlusion-Aware Rolling Shutter Rectification of 3D Scenes
Type: Poster
Abstracts: A vast majority of contemporary cameras employ rolling shutter (RS) mechanism to capture images. Due to the sequential mechanism, images acquired with a moving camera are subjected to rolling shutter effect which manifests as geometric distortions. In this work, we consider the specific scenario of a fast moving camera wherein the rolling shutter distortions not only are predominant but also become depth-dependent which in turn results in intra-frame occlusions. To this end, we develop a first-of-its-kind pipeline to recover the latent image of a 3D scene from a set of such RS distorted images. The proposed approach sequentially recovers both the camera motion and scene structure while accounting for RS and occlusion effects. Subsequently, we perform depth and occlusion-aware rectification of RS images to yield the desired latent image. Our experiments on synthetic and real image sequences reveal that the proposed approach achieves state-of-the-art results.
Authors: Subeesh Vasu, IIT Madras;Mahesh Mohan M R, IIT Madras;A.N. Rajagopalan, IIT Madras;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Vasu_Occlusion-Aware_Rolling_Shutter_CVPR_2018_paper.pdf


609
Title: Disentangled Person Image Generation
Type: Spotlight
Abstracts: Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.
Authors: Liqian Ma, KU Leuven;Qianru Sun, MPI for Informatics;Stamatios Georgoulis, KU Leuven;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;Luc Van Gool, KU Leuven;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf


610
Title: Disentangled Person Image Generation
Type: Poster
Abstracts: Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.
Authors: Liqian Ma, KU Leuven;Qianru Sun, MPI for Informatics;Stamatios Georgoulis, KU Leuven;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;Luc Van Gool, KU Leuven;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf


611
Title: CRRN: Multi-Scale Guided Concurrent Reflection Removal Network
Type: Poster
Abstracts: Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks. Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections. In this paper, we propose the Concurrent Reflection Removal Network (CRRN) to tackle this problem in a unified framework. Our network integrates image appearance information and multi-scale gradient information with human perception inspired loss function, and is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes. Extensive experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods.
Authors: Renjie Wan, Nanyang Technological Universi;Boxin Shi, Peking University;Ling-Yu Duan, ;Ah-Hwee Tan, ;Alex Kot,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_CRRN_Multi-Scale_Guided_CVPR_2018_paper.pdf


612
Title: Natural and Effective Obfuscation by Head Inpainting
Type: Poster
Abstracts: As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers[17]. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers.
Authors: Qianru Sun, MPI for Informatics;Liqian Ma, KU Leuven;Seong Joon Oh, MPI-INF;Saarbrucken, Germany;Luc Van Gool, KU Leuven;Bernt Schiele, MPI Informatics Germany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf


613
Title: Deep Learning of Graph Matching
Type: Oral
Abstracts: The problem of graph matching under node and pair-wise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.
Authors: Andrei Zanfir, IMAR and Lund University;Cristian Sminchisescu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Deep_Learning_of_CVPR_2018_paper.pdf


614
Title: Deep Learning of Graph Matching
Type: Poster
Abstracts: The problem of graph matching under node and pair-wise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.
Authors: Andrei Zanfir, IMAR and Lund University;Cristian Sminchisescu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Deep_Learning_of_CVPR_2018_paper.pdf


615
Title: What do Deep Networks Like to See?
Type: Poster
Authors: Sebastian Palacio, DFKI;Joachim Folz, DFKI;Andreas Dengel, DFKI;J?rn Hees, DFKI;Federico Raue, DFKI;

616
Title: FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors
Type: Spotlight
Authors: Yu Chen, NUST;Ying Tai, Tencent;Xiaoming Liu, Michigan State University;Chunhua Shen, University of Adelaide;Jian Yang, Nanjing University of Science and Technology;

617
Title: FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors
Type: Poster
Authors: Yu Chen, NUST;Ying Tai, Tencent;Xiaoming Liu, Michigan State University;Chunhua Shen, University of Adelaide;Jian Yang, Nanjing University of Science and Technology;

618
Title: Person Re-identification with Cascaded Pairwise Convolutions
Type: Poster
Authors: Yicheng Wang, ;Zhenzhong Chen, Wuhan University;Feng Wu, ;Gang Wang,;

619
Title: DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Network
Type: Poster
Authors: Shuang Ma, SUNY Buffalo;Jianlong Fu, ;Chang Chen, ;Tao Mei, Microsoft Research Asia;

620
Title: Deep Cocktail Networks: Multi-source Unsupervised Domain Adaptation with Category Shift
Type: Poster
Authors: Ruijia Xu, Sun Yat-sen University;Ziliang Chen, Sun Yat-sen University;Wangmeng Zuo, Harbin Institute of Technology;Junjie Yan, ;Liang Lin,;

621
Title: Learning deep structured active contours end-to-end
Type: Spotlight
Authors: Diego Marcos, ;Devis Tuia, Wageningen University;Benjamin Kellenberger, Wageningen University and Research;Lisa Zhang, University of Toronto;Min Bai, ;Renjie Liao, ;Raquel Urtasun, University of Toronto;

622
Title: Learning deep structured active contours end-to-end
Type: Poster
Authors: Diego Marcos, ;Devis Tuia, Wageningen University;Benjamin Kellenberger, Wageningen University and Research;Lisa Zhang, University of Toronto;Min Bai, ;Renjie Liao, ;Raquel Urtasun, University of Toronto;

623
Title: Unsupervised Domain Adaptation with Similarity-Based Classifier
Type: Poster
Authors: Pedro Pinheiro, EPFL;

624
Title: Tags2Parts: Discovering Semantic Regions from Shape Tags
Type: Poster
Authors: Sanjeev Muralikrishnan, IIT Bombay;Vladimir Kim, Adobe Research;Siddhartha Chaudhuri, IIT Bombay;

625
Title: A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation
Type: Poster
Abstracts: In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthe- sis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye ge- ometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermedi- ate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model’s effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields.
Authors: Kang Wang, RPI;Rui Zhao, Rensselaer Polytechnic Institu;Qiang Ji, RPI;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_A_Hierarchical_Generative_CVPR_2018_paper.pdf


626
Title: Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks
Type: Poster
Authors: Nick Johnston, Google;Damien Vincent, google.com;David Minnen, google.com;Michele Covell, google.com;Saurabh Singh, Univ. of Illinois at Urbana-Champaign;Sung Jin Hwang, google.com;George Toderici, Google;Troy Chinen, google.com;Joel Shor, google.com;

627
Title: Neural Sign Language Translation
Type: Poster
Abstracts: Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar. We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language. To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T. It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively.
Authors: Necati Cihan Camgoz, CVSSP;Simon Hadfield, ;Richard Bowden, University of Surrey UK;Oscar Koller, ;Hermann Ney,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Camgoz_Neural_Sign_Language_CVPR_2018_paper.pdf


628
Title: 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild
Type: Poster
Abstracts: We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.
Authors: Alexander Grabner, Graz University of Technology;Peter Roth, Graz University of Technology;Vincent Lepetit, TU Graz;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Grabner_3D_Pose_Estimation_CVPR_2018_paper.pdf


629
Title: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs
Type: Poster
Authors: Loic Landrieu, IGN;Universite Paris Est, ENPC;

630
Title: The iNaturalist Species Classification and Detection Dataset
Type: Spotlight
Authors: Grant van Horn, California Institute of Technology;Oisin Mac Aodha, Caltech;Yang Song, Google;Yin Cui, CornellTech;Chen Sun, Google;Alex Shepard, iNaturalist;Hartwig Adam, Google;California Institute of Technology, USA;Serge Belongie,;

631
Title: The iNaturalist Species Classification and Detection Dataset
Type: Poster
Authors: Grant van Horn, California Institute of Technology;Oisin Mac Aodha, Caltech;Yang Song, Google;Yin Cui, CornellTech;Chen Sun, Google;Alex Shepard, iNaturalist;Hartwig Adam, Google;California Institute of Technology, USA;Serge Belongie,;

632
Title: Teaching Categories to Human Learners with Visual Explanations
Type: Spotlight
Authors: Oisin Mac Aodha, Caltech;Shihan Su, Caltech;Yuxin Chen, Caltech;California Institute of Technology, USA;Yisong Yue,;

633
Title: Teaching Categories to Human Learners with Visual Explanations
Type: Poster
Authors: Oisin Mac Aodha, Caltech;Shihan Su, Caltech;Yuxin Chen, Caltech;California Institute of Technology, USA;Yisong Yue,;

634
Title: Motion-Appearance Co-Memory Networks for Video Question Answering
Type: Poster
Abstracts: Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.
Authors: Jiyang Gao, ;Runzhou Ge, Univ. of Southern California;Kan Chen, Univ. of Southern California;Ram Nevatia,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.pdf


635
Title: Temporal Deformable Residual Networks for Action Segmentation in Videos
Type: Poster
Abstracts: This paper is about temporal segmentation of human actions in videos. We introduce a new model -- temporal deformable residual network (TDRN) -- aimed at analyzing video intervals at multiple temporal scales for labeling video frames. Our TDRN computes two parallel temporal streams: i) Residual stream that analyzes video information at its full temporal resolution, and ii) Pooling/unpooling stream that captures long-range video information at different scales. The former facilitates local, fine-scale action segmentation, and the latter uses multiscale context for improving accuracy of frame classification. These two streams are computed by a set of temporal residual modules with deformable convolutions, and fused by temporal residuals at the full video resolution. Our evaluation on the University of Dundee 50 Salads, Georgia Tech Egocentric Activities, and JHU-ISI Gesture and Skill Assessment Working Set demonstrates that TDRN outperforms the state of the art in frame-wise segmentation accuracy, segmental edit score, and segmental overlap F1 score.
Authors: Peng Lei, Oregon State University;Sinisa Todorovic,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.pdf


636
Title: Actor and Observer: Joint Modeling of First and Third-Person Videos
Type: Spotlight
Abstracts: Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain.
Authors: Gunnar Sigurdsson, CMU;INRIA Grenoble, France;Ali Farhadi, ;Abhinav Gupta, ;Karteek Alahari,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf


637
Title: Actor and Observer: Joint Modeling of First and Third-Person Videos
Type: Poster
Abstracts: Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain.
Authors: Gunnar Sigurdsson, CMU;INRIA Grenoble, France;Ali Farhadi, ;Abhinav Gupta, ;Karteek Alahari,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf


638
Title: Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push
Type: Spotlight
Authors: Siavash Gorji, McGill University;James Clark, McGill University;

639
Title: Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push
Type: Poster
Authors: Siavash Gorji, McGill University;James Clark, McGill University;

640
Title: Spatially-Adaptive Filter Units for Deep Neural Networks
Type: Poster
Abstracts: Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter.
Authors: Domen Tabernik, University of Ljubljana;Matej Kristan, University of Ljubljana;University of Birmingham, UK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tabernik_Spatially-Adaptive_Filter_Units_CVPR_2018_paper.pdf


641
Title: Boundary Flow: A Siamese Network that Predicts Boundary Motion without Training on Motion
Type: Poster
Authors: Peng Lei, Oregon State University;Fuxin Li, Oregon State University;Sinisa Todorovic,;

642
Title: DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks
Type: Poster
Abstracts: We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem -- object detection on (de-)blurred images. The method is 5 times faster than the closest competitor -- DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation. The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN
Authors: Orest Kupyn, Ukrainian Catholic University;Volodymyr Budzan, Ukrainian Catholic University;Mykola Mykhailych, UCU;Dmytro Mishkin, Czech Technical University;Jiri Matas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf


643
Title: Discriminability objective for training descriptive captions
Type: Poster
Authors: Ruotian Luo, Toyota Technological Institute;Scott Cohen, ;Brian Price, ;Greg Shakhnarovich,;

644
Title: Rolling Shutter and Radial Distortion are Features for High Frame Rate Multi-camera Tracking
Type: Poster
Authors: Akash Bapat, UNC Chapel Hill;Jan-Michael Frahm, UNC Chapel Hill;True Price, UNC Chapel Hill;

645
Title: Language-Based Image Editing with Recurrent attentive Models
Type: Spotlight
Authors: Yelong Shen, Microsoft;Jianbo Chen, UC Berkeley;Jianfeng Gao, ;JingJing Liu, Microsoft;Xiaodong Liu, Microsoft;

646
Title: Language-Based Image Editing with Recurrent attentive Models
Type: Poster
Authors: Yelong Shen, Microsoft;Jianbo Chen, UC Berkeley;Jianfeng Gao, ;JingJing Liu, Microsoft;Xiaodong Liu, Microsoft;

647
Title: SBNet: Sparse Block?s Network for Fast Inference
Type: Spotlight
Authors: Mengye Ren, Uber ATG;Andrei Pokrovsky, Uber ATG;Uber ATG, UofT;Raquel Urtasun, University of Toronto;

648
Title: SBNet: Sparse Block?s Network for Fast Inference
Type: Poster
Authors: Mengye Ren, Uber ATG;Andrei Pokrovsky, Uber ATG;Uber ATG, UofT;Raquel Urtasun, University of Toronto;

649
Title: Learning Compositional Visual Concepts with Mutual Consistency
Type: Spotlight
Authors: Yunye Gong, Cornell University;Srikrishna Karanam, Siemens Corporate Technology;Ziyan Wu, Siemens Corporation;Kuan-Chuan Peng, Siemens Corporation;Jan Ernst, Siemens Corporation;Peter Doerschuk, Cornell University;

650
Title: Learning Compositional Visual Concepts with Mutual Consistency
Type: Poster
Authors: Yunye Gong, Cornell University;Srikrishna Karanam, Siemens Corporate Technology;Ziyan Wu, Siemens Corporation;Kuan-Chuan Peng, Siemens Corporation;Jan Ernst, Siemens Corporation;Peter Doerschuk, Cornell University;

651
Title: Learning Deep Sketch Abstraction
Type: Poster
Abstracts: Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.
Authors: Umar Riaz Muhammad, Queen Mary Uni of London;Yongxin Yang, Queen Mary University of London;Yi-Zhe Song, ;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Muhammad_Learning_Deep_Sketch_CVPR_2018_paper.pdf


652
Title: Learning to Extract a Video Sequence from a Single Motion-Blurred Image
Type: Spotlight
Authors: University of Bern, Switzerlan;University of Bern, Switzerland;Bern University, Switzerland;

653
Title: Learning to Extract a Video Sequence from a Single Motion-Blurred Image
Type: Poster
Authors: University of Bern, Switzerlan;University of Bern, Switzerland;Bern University, Switzerland;

654
Title: Synthesizing Images of Humans in Unseen Poses
Type: Oral
Abstracts: We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.
Authors: Guha Balakrishnan, MIT;Adrian Dalca, ;Amy Zhao, MIT;Fredo Durand, ;John Guttag,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_Synthesizing_Images_of_CVPR_2018_paper.pdf


655
Title: Synthesizing Images of Humans in Unseen Poses
Type: Poster
Abstracts: We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.
Authors: Guha Balakrishnan, MIT;Adrian Dalca, ;Amy Zhao, MIT;Fredo Durand, ;John Guttag,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_Synthesizing_Images_of_CVPR_2018_paper.pdf


656
Title: Learning to See in the Dark
Type: Poster
Abstracts: Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can lead to blurry images and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.
Authors: Chen Chen, UIUC;Qifeng Chen, Intel Labs;Jia Xu, Tencent AI Lab;Vladlen Koltun, Intel Labs;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Learning_to_See_CVPR_2018_paper.pdf


657
Title: Neural Inverse Kinematics for Unsupervised Motion Retargetting
Type: Oral
Authors: Ruben Villegas, University of Michigan;Jimei Yang, ;Duygu Ceylan, ;University of Michigan, USA;

658
Title: Neural Inverse Kinematics for Unsupervised Motion Retargetting
Type: Poster
Authors: Ruben Villegas, University of Michigan;Jimei Yang, ;Duygu Ceylan, ;University of Michigan, USA;

659
Title: Eliminating Background-bias for Robust Person Re-identification
Type: Poster
Authors: Maoqing Tian, Sensetime Limited;Shuai Yi, The Chinese University of Hong Kong;Hongsheng Li, ;Shihua Li, ;Xuesen Zhang, SenseTime;Jianping Shi, SenseTime;Junjie Yan, ;Xiaogang Wang, Chinese University of Hong Kong;

660
Title: Uncalibrated Photometric Stereo under Natural Illumination
Type: Poster
Authors: Zhipeng Mo, ;Boxin Shi, Peking University;Feng Lu, U. Tokyo;Sai-Kit Yeung, ;Yasuyuki Matsushita, Osaka University;

661
Title: A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping
Type: Poster
Abstracts: Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.
Authors: Debang Li, CASIA;Huikai Wu, CASIA;Junge Zhang, ;Kaiqi Huang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_A2-RL_Aesthetics_Aware_CVPR_2018_paper.pdf


662
Title: Weakly Supervised Action Localization by Sparse Temporal Pooling Network
Type: Poster
Abstracts: We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.
Authors: University of California, Irvine;Google, Inc.;Google, Inc.;Bohyung Han, Seoul National University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Weakly_Supervised_Action_CVPR_2018_paper.pdf


663
Title: Very Large-Scale Global SfM by Distributed Motion Averaging
Type: Poster
Abstracts: Global Structure-from-Motion (SfM) techniques have demonstrated superior efficiency and accuracy than the conventional incremental approach in many recent studies. This work proposes a divide-and-conquer framework to solve very large global SfM at the scale of millions of images. Specifically, we first divide all images into multiple partitions that preserve strong data association for well posed and parallel local motion averaging. Then, we solve a global motion averaging that determines cameras at partition boundaries and a similarity transformation per partition to register all cameras in a single coordinate frame. Finally, local and global motion averaging are iterated until convergence. Since local camera poses are fixed during the global motion average, we can avoid caching the whole reconstruction in memory at once. This distributed framework significantly enhances the efficiency and robustness of large-scale motion averaging.
Authors: Siyu Zhu, HKUST;Runze Zhang, HKUST;Lei Zhou, HKUST;Tianwei Shen, HKUST;Tian Fang, HKUST;Ping Tan, ;The Hong Kong University of Science and Technology, Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Very_Large-Scale_Global_CVPR_2018_paper.pdf


664
Title: ID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis
Type: Poster
Authors: Dept. of IE, CUHK;Ping Luo, The Chinese University of Hong Kong;Junjie Yan, ;Xiaogang Wang, Chinese University of Hong Kong;Xiaoou Tang, Chinese University of Hong Kong;

665
Title: DenseASPP: Densely Connected Networks for Semantic Segmentation
Type: Spotlight
Authors: Maoke Yang, DeepMotion;Kun Yu, DeepMotion;Kuiyuan Yang, DeepMotion;

666
Title: DenseASPP: Densely Connected Networks for Semantic Segmentation
Type: Poster
Authors: Maoke Yang, DeepMotion;Kun Yu, DeepMotion;Kuiyuan Yang, DeepMotion;

667
Title: DVQA: Understanding Data Visualization via Question Answering
Type: Poster
Authors: Kushal Kafle, ;Brian Price, ;Scott Cohen, ;Christopher Kanan, RIT;

668
Title: iVQA: Inverse Visual Question Answering
Type: Spotlight
Authors: Feng Liu, Southeast Univeristy;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;Wankou Yang, Southeast University;Changyin Sun, Southeast University;

669
Title: iVQA: Inverse Visual Question Answering
Type: Poster
Authors: Feng Liu, Southeast Univeristy;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;Wankou Yang, Southeast University;Changyin Sun, Southeast University;

670
Title: Globally Optimal Inlier Set Maximization for Atlanta Frame Estimation
Type: Poster
Abstracts: In this work, we describe man-made structures via an appropriate structure assumption, called Atlanta world, which contains a vertical direction (typically the gravity direction) and a set of horizontal directions orthogonal to the vertical direction. Contrary to the commonly used Manhattan world assumption, the horizontal directions in Atlanta world are not necessarily orthogonal to each other. While Atlanta world permits to encompass a wider range of scenes, this makes the solution space larger and the problem more challenging. Given a set of inputs, such as lines in a calibrated image or surface normals, we propose the first globally optimal method of inlier set maximization for Atlanta direction estimation. We define a novel search space for Atlanta world, as well as its parameterization, and solve this challenging problem by a branch-and-bound framework. Experimental results with synthetic and real-world datasets have successfully confirmed the validity of our approach.
Authors: Kyungdon Joo, ;Tae-Hyun Oh, MIT;In So Kweon, KAIST;Jean-Charles Bazin, KAIST;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Globally_Optimal_Inlier_CVPR_2018_paper.pdf


671
Title: Recurrent Slice Networks for 3D Segmentation on Point Clouds
Type: Spotlight
Authors: Qiangui Huang, U of Southern CA;Weiyue Wang, USC;Ulrich Neumann, USC;

672
Title: Recurrent Slice Networks for 3D Segmentation on Point Clouds
Type: Poster
Authors: Qiangui Huang, U of Southern CA;Weiyue Wang, USC;Ulrich Neumann, USC;

673
Title: End-to-end Convolutional Semantic Embeddings
Type: Poster
Authors: Quanzeng You, Microsoft;Zhengyou Zhang, Microsoft Research;Jiebo Luo, University of Rochester;

674
Title: The Easy
Type: Spotlight
Authors: Swami Sankaranarayanan, University of Maryland;Yogesh Balaji, University of Maryland;Carlos Castillo, ;University of Maryland, USA;

675
Title: The Easy
Type: Poster
Authors: Swami Sankaranarayanan, University of Maryland;Yogesh Balaji, University of Maryland;Carlos Castillo, ;University of Maryland, USA;

676
Title: Visual Question Answering with Memory-Augmented Networks
Type: Poster
Authors: Chao Ma, ;Chunhua Shen, University of Adelaide;Anthony Dick, University of Adelaide;Qi Wu, University of Adelaide;Peng Wang, The University of Adelaide;Anton Van den Hengel, University of Adelaide;Ian Reid,;

677
Title: InLoc: Indoor Visual Localization with Dense Matching and View Synthesis
Type: Spotlight
Authors: Hajime Taira, Tokyo Institute of Technology;Masatoshi Okutomi, Tokyo Institute of Technology;Torsten Sattler, ETH Zurich;Mircea Cimpoi, Czech Institute of Informatics;Marc Pollefeys, ETH;Josef Sivic, ;Tomas Pajdla, ;Akihiko Torii, Tokyo Institute of Technology;

678
Title: InLoc: Indoor Visual Localization with Dense Matching and View Synthesis
Type: Poster
Authors: Hajime Taira, Tokyo Institute of Technology;Masatoshi Okutomi, Tokyo Institute of Technology;Torsten Sattler, ETH Zurich;Mircea Cimpoi, Czech Institute of Informatics;Marc Pollefeys, ETH;Josef Sivic, ;Tomas Pajdla, ;Akihiko Torii, Tokyo Institute of Technology;

679
Title: MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition
Type: Poster
Abstracts: Human actions in videos are three-dimensional (3D) signals. Recent attempts use 3D convolutional neural networks (CNNs) to explore spatio-temporal information for human action recognition. Though promising, 3D CNNs have not achieved high performanceon on this task with respect to their well-established two-dimensional (2D) counterparts for visual recognition in still images. We argue that the high training complexity of spatio-temporal fusion and the huge memory cost of 3D convolution hinder current 3D CNNs, which stack 3D convolutions layer by layer, by outputting deeper feature maps that are crucial for high-level tasks. We thus propose a Mixed Convolutional Tube (MiCT) that integrates 2D CNNs with the 3D convolution module to generate deeper and more informative feature maps, while reducing training complexity in each round of spatio-temporal fusion. A new end-to-end trainable deep 3D network, MiCT-Net, is also proposed based on the MiCT to better explore spatio-temporal information in human actions. Evaluations on three well-known benchmark datasets (UCF101, Sport-1M and HMDB-51) show that the proposed MiCT-Net significantly outperforms the original 3D CNNs. Compared with state-of-the-art approaches for action recognition on UCF101 and HMDB51, our MiCT-Net yields the best performance.
Authors: Yizhou Zhou, Univ of Scienc.&Tech. of China;Xiaoyan Sun, Microsoft;Zheng-Jun Zha, ;Wenjun Zeng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.pdf


680
Title: Content-Sensitive Supervoxels via Uniform Tessellations on Video Manifolds
Type: Poster
Abstracts: Supervoxels are perceptually meaningful atomic regions in videos, obtained by grouping voxels that exhibit coherence in both appearance and motion. In this paper, we propose content-sensitive supervoxels (CSS), which are regularly-shaped 3D primitive volumes that possess the following characteristic: they are typically larger and longer in content-sparse regions (i.e., with homogeneous appearance and motion), and smaller and shorter in content-dense regions (i.e., with high variation of appearance and/or motion). To compute CSS, we map a video X to a 3-dimensional manifold M embedded in R^6, whose volume elements give a good measure of the content density in X. We propose an efficient Lloyd-like method with a splitting-merging scheme to compute a uniform tessellation on M, which induces the CSS in X. Theoretically our method has a good competitive ratio O(1). We also present a simple extension of CSS to stream CSS for processing long videos that cannot be loaded into main memory at once. We evaluate CSS, stream CSS and seven representative supervoxel methods on four video datasets. The results show that our method outperforms existing supervoxel methods.
Authors: Ran Yi, Tsinghua University;Yong-Jin Liu, ;Yu-Kun Lai, Cardiff University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yi_Content-Sensitive_Supervoxels_via_CVPR_2018_paper.pdf


681
Title: Weakly Supervised Coupled Networks for Visual Sentiment Analysis
Type: Spotlight
Abstracts: Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions on-line. In this paper, we solve the problem of visual sentiment analysis using the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image appearance. However, different image regions can have a different influence on the intended expression. This paper presents a weakly supervised coupled convolutional network with two branches to leverage the localized information. The first branch detects a sentiment specific soft map by training a fully convolutional network with the cross spatial pooling strategy, which only requires image-level labels, thereby significantly reducing the annotation burden. The second branch utilizes both the holistic and localized information by coupling the sentiment map with deep features for robust classification. We integrate the sentiment detection and classification branches into a unified deep framework and optimize the network in an end-to-end manner. Extensive experiments on six benchmark datasets demonstrate that the proposed method performs favorably against the state-ofthe-art methods for visual sentiment analysis.
Authors: Jufeng Yang, Nankai University;Dongyu She, ;Yu-Kun Lai, Cardiff University;Paul Rosin, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Weakly_Supervised_Coupled_CVPR_2018_paper.pdf


682
Title: Weakly Supervised Coupled Networks for Visual Sentiment Analysis
Type: Poster
Abstracts: Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions on-line. In this paper, we solve the problem of visual sentiment analysis using the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image appearance. However, different image regions can have a different influence on the intended expression. This paper presents a weakly supervised coupled convolutional network with two branches to leverage the localized information. The first branch detects a sentiment specific soft map by training a fully convolutional network with the cross spatial pooling strategy, which only requires image-level labels, thereby significantly reducing the annotation burden. The second branch utilizes both the holistic and localized information by coupling the sentiment map with deep features for robust classification. We integrate the sentiment detection and classification branches into a unified deep framework and optimize the network in an end-to-end manner. Extensive experiments on six benchmark datasets demonstrate that the proposed method performs favorably against the state-ofthe-art methods for visual sentiment analysis.
Authors: Jufeng Yang, Nankai University;Dongyu She, ;Yu-Kun Lai, Cardiff University;Paul Rosin, ;Ming-Hsuan Yang, UC Merced;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Weakly_Supervised_Coupled_CVPR_2018_paper.pdf


683
Title: 3D Semantic Trajectory Reconstruction from 3D Pixel Continuum
Type: Poster
Authors: Jae Yoon, ;Ziwei Li, UMN;Hyun Park,;

684
Title: End-to-End Learning of Motion Representation for Video Understanding
Type: Spotlight
Abstracts: Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.
Authors: Lijie Fan, Tsinghua University;Wenbing Huang, Tencent AI Lab;Chuang Gan, Tsinghua University;Stefano Ermon, Stanford University;Junzhou Huang, UT Arlingtron;Boqing Gong, University of Central Florida;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_End-to-End_Learning_of_CVPR_2018_paper.pdf


685
Title: End-to-End Learning of Motion Representation for Video Understanding
Type: Poster
Abstracts: Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.
Authors: Lijie Fan, Tsinghua University;Wenbing Huang, Tencent AI Lab;Chuang Gan, Tsinghua University;Stefano Ermon, Stanford University;Junzhou Huang, UT Arlingtron;Boqing Gong, University of Central Florida;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_End-to-End_Learning_of_CVPR_2018_paper.pdf


686
Title: Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships
Type: Poster
Abstracts: Context is important for accurate visual recognition. In this work we propose an object detection algorithm that not only considers object visual appearance, but also makes use of two kinds of context including scene contextual information and object relationships within a single image. Therefore, object detection is regarded as both a cognition problem and a reasoning problem when leveraging these structured information. Specifically, this paper formulates object detection as a problem of graph structure inference, where given an image the objects are treated as nodes in a graph and relationships between the objects are modeled as edges in such graph. To this end, we present a so-called Structure Inference Network (SIN), a detector that incorporates into a typical detection framework (e.g. Faster R-CNN) with a graphical model which aims to infer object state. Comprehensive experiments on PASCAL VOC and MS COCO datasets indicate that scene context and object relationships truly improve the performance of object detection with more desirable and reasonable outputs.
Authors: Yong Liu, ICT;Institute of Computing Technology, Chinese Academy of Sciences;Shiguang Shan, Chinese Academy of Sciences;Xilin Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Structure_Inference_Net_CVPR_2018_paper.pdf


687
Title: Feature Selective Networks for Object Detection
Type: Poster
Abstracts: Objects for detection usually have distinct characteristics in different sub-regions and different aspect ratios. However, in prevalent two-stage object detection methods, Region-of-Interest (RoI) features are extracted by RoI pooling with little emphasis on these translation-variant feature components. We present feature selective networks to reform the feature representations of RoIs by exploiting their disparities among sub-regions and aspect ratios. Our network produces the sub-region attention bank and aspect ratio attention bank for the whole image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from the banks, and then used to refine the original RoI features for RoI classification. Equipped with a light-weight detection subnetwork, our network gets a consistent boost in detection performance based on general ConvNet backbones (ResNet-101, GoogLeNet and VGG-16). Without bells and whistles, our detectors equipped with ResNet-101 achieve more than 3% mAP improvement compared to counterparts on PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO datasets.
Authors: Yao Zhai, University of Science and Technology of China;Jingjing Fu, ;Yan Lu, ;Houqiang Li,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhai_Feature_Selective_Networks_CVPR_2018_paper.pdf


688
Title: High-speed Tracking with Multi-kernel Correlation Filters
Type: Poster
Authors: IA, CAS;IA, CAS;Fan Zhang, BUPT;Jinqiao Wang,;

689
Title: Weakly Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer
Type: Spotlight
Authors: Hao-Shu Fang, Shanghai Jiao Tong University;Guansong Lu, Shanghai Jiao Tong University;Xiaolin Fang, Zhejiang University;Yu-Wing Tai, Tencent YouTu;Cewu Lu, Shanghai Jiao Tong University;

690
Title: Weakly Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer
Type: Poster
Authors: Hao-Shu Fang, Shanghai Jiao Tong University;Guansong Lu, Shanghai Jiao Tong University;Xiaolin Fang, Zhejiang University;Yu-Wing Tai, Tencent YouTu;Cewu Lu, Shanghai Jiao Tong University;

691
Title: Semantic Video Segmentation by Gated Recurrent Flow Propagation
Type: Poster
Abstracts: Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology for video segmentation that is capable of leveraging the information present in unlabeled data, besides sparsely labeled frames, in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that is able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our experiments in the challenging CityScapes and Camvid datasets, and for multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.
Authors: David Nilsson, Lund University;Cristian Sminchisescu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Nilsson_Semantic_Video_Segmentation_CVPR_2018_paper.pdf


692
Title: A Constrained Deep Neural Network for Ordinal Regression
Type: Poster
Abstracts: Ordinal regression is a supervised learning problem aiming to classify instances into ordinal categories. It is challenging to automatically extract high-level features for representing intraclass information and interclass ordinal relationship simultaneously. This paper proposes a constrained optimization formulation for the ordinal regression problem which minimizes the negative loglikelihood for multiple categories constrained by the order relationship between instances. Mathematically, it is equivalent to an unconstrained formulation with a pairwise regularizer. An implementation based on the CNN framework is proposed to solve the problem such that high-level features can be extracted automatically, and the optimal solution can be learned through the traditional back-propagation method. The proposed pairwise constraints make the algorithm work even on small datasets, and a proposed efficient implementation make it be scalable for large datasets. Experimental results on four real-world benchmarks demonstrate that the proposed algorithm outperforms the traditional deep learning approaches and other state-of-the-art approaches based on hand-crafted features.
Authors: Yanzhu Liu, Nanyang Technological Universi;Adams Kong, NTU Singapore ;Chi Keong Goh, Rolls-Royce Advanced Technology Centre;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_A_Constrained_Deep_CVPR_2018_paper.pdf


693
Title: Encoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction
Type: Poster
Authors: Yanyu Xu, Shanghaitech University;Zhixin Piao, ;Shenghua Gao, ShanghaiTech University;

694
Title: Spline Error Weighting for Robust Visual-Inertial Fusion
Type: Poster
Abstracts: In this paper we derive and test a probability-based weighting that can balance residuals of different types in spline fitting. In contrast to previous formulations, the proposed spline error weighting scheme also incorporates a prediction of the approximation error of the spline fit. We demonstrate the effectiveness of the prediction in a synthetic experiment, and apply it to visual-inertial fusion on rolling shutter cameras. This results in a method that can estimate 3D structure with metric scale on generic first-person videos. We also propose a quality measure for spline fitting, that can be used to automatically select the knot spacing. Experiments verify that the obtained trajectory quality corresponds well with the requested quality. Finally, by linearly scaling the weights, we show that the proposed spline error weighting minimizes the estimation errors on real sequences, in terms of scale and end-point errors.
Authors: Hannes Ovr?n, Link?ping University;Per-Erik Forssen, Linkoping University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ovren_Spline_Error_Weighting_CVPR_2018_paper.pdf


695
Title: Mean-Variance Loss for Deep Age Estimation from a Face
Type: Poster
Authors: Hongyu Pan, Institute of Computing Technol;Hu Han, ;Shiguang Shan, Chinese Academy of Sciences;Xilin Chen,;

696
Title: Pose-Robust Face Recognition via Deep Residual Equivariant Mapping
Type: Poster
Abstracts: Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead.
Authors: Kaidi Cao, Tsinghua University;Yu Rong, CUHK;Cheng Li, SenseTime;Chen-Change Loy, the Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Pose-Robust_Face_Recognition_CVPR_2018_paper.pdf


697
Title: Viewpoint-aware Video Summarization
Type: Spotlight
Authors: Atsushi Kanehira, University of Tokyo;Luc Van Gool, KTH;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;

698
Title: Viewpoint-aware Video Summarization
Type: Poster
Authors: Atsushi Kanehira, University of Tokyo;Luc Van Gool, KTH;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;

699
Title: Statistical Tomography of Microscopic Life
Type: Poster
Abstracts: We achieve tomography of 3D volumetric natural objects, where each projected 2D image corresponds to a different specimen. Each specimen has unknown random 3D orientation, location, and scale. This imaging scenario is relevant to microscopic and mesoscopic organisms, aerosols and hydrosols viewed naturally by a microscope. In-class scale variation inhibits prior single-particle reconstruction methods. We thus generalize tomographic recovery to account for all degrees of freedom of a similarity transformation. This enables geometric self-calibration in imaging of transparent objects. We make the computational load manageable and reach good quality reconstruction in a short time. This enables extraction of statistics that are important for a scientific study of specimen populations, specifically size distribution parameters. We apply the method to study of plankton.
Authors: Aviad Levis, Technion Institute of Technology;Ronen Talmon, Technion - Israel Institute of Technology;Technion Haifa, Israel;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Levis_Statistical_Tomography_of_CVPR_2018_paper.pdf


700
Title: Divide and Conquer for Full-Resolution Light Field Deblurring
Type: Poster
Abstracts: The increasing popularity of computational light field (LF) cameras has necessitated the need for tackling motion blur which is a ubiquitous phenomenon in hand-held photography. The state-of-the-art method for blind deblurring of LFs of general 3D scenes is limited to handling only downsampled LF, both in spatial and angular resolution. This is due to the computational overhead involved in processing data-hungry full-resolution 4D LF altogether. Moreover, the method warrants high-end GPUs for optimization and is ineffective for wide-angle settings and irregular camera motion. In this paper, we introduce a new blind motion deblurring strategy for LFs which alleviates these limitations significantly. Our model achieves this by isolating 4D LF motion blur across the 2D subaperture images, thus paving the way for independent deblurring of these subaperture images. Furthermore, our model accommodates common camera motion parameterization across the subaperture images. Consequently, blind deblurring of any single subaperture image elegantly paves the way for cost-effective non-blind deblurring of the other subaperture images. Our approach is CPU-efficient computationally and can effectively deblur full-resolution LFs.
Authors: Mahesh Mohan M R, IIT Madras;A.N. Rajagopalan, IIT Madras;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mohan_Divide_and_Conquer_CVPR_2018_paper.pdf


701
Title: Conditional Probability Models for Deep Image Compression
Type: Poster
Abstracts: Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.
Authors: Eirikur Agustsson, ETH Zurich;Fabian Mentzer, ETHZ Z?rich;Michael Tschannen, ETH Zurich;Radu Timofte, ETH Zurich;Luc Van Gool, KTH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.pdf


702
Title: Direction-aware Spatial Context Features for Shadow Detection
Type: Oral
Authors: Xiaowei Hu, CUHK;Lei Zhu, ;Chi-Wing Fu, ;Jing Qin, The Hong Kong Polytechnic University;Pheng-Ann Heng,;

703
Title: Direction-aware Spatial Context Features for Shadow Detection
Type: Poster
Authors: Xiaowei Hu, CUHK;Lei Zhu, ;Chi-Wing Fu, ;Jing Qin, The Hong Kong Polytechnic University;Pheng-Ann Heng,;

704
Title: Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification
Type: Poster
Abstracts: Recently, substantial research effort has focused on how to apply CNNs or RNNs to better capture temporal patterns in videos, so as to improve the accuracy of video classification. In this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common trimmed video classification datasets. We investigate the potential of a purely attention based local feature integration. Accounting for the characteristics of such features in video classification, we propose a local feature integration framework based on attention clusters, and introduce a shifting operation to capture more diverse signals. We carefully analyze and compare the effect of different attention mechanisms, cluster sizes, and the use of the shifting operation, and also investigate the combination of attention clusters for multimodal integration. We demonstrate the effectiveness of our framework on three real-world video classification datasets. Our model achieves competitive results across all of these. In particular, on the large-scale Kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5 accuracy on the validation set.
Authors: Xiang Long, Tsinghua University;Chuang Gan, Tsinghua University;Gerard De Melo, Rutgers University;Jiajun Wu, MIT;Xiao Liu, ;Shilei Wen, Baidu Research;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Long_Attention_Clusters_Purely_CVPR_2018_paper.pdf


705
Title: Occluded Pedestrian Detection through Guided Attention in CNNs
Type: Poster
Authors: Shanshan Zhang, MPI;Jian Yang, Nanjing University of Science and Technology;Bernt Schiele, MPI Informatics Germany;

706
Title: SO-Net: Self-Organizing Network for Point Cloud Analysis
Type: Poster
Abstracts: This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website.
Authors: Jiaxin Li, National University of Singapore;Ben Chen, National Univ of Singapore;Gim Hee Lee, National University of SIngapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf


707
Title: CartoonGAN: Generative Adversarial Networks for Photo Cartoonization
Type: Poster
Abstracts: In this paper, we propose a solution to transforming photos of real-world scenes into cartoon style images, which is valuable and challenging in computer vision and computer graphics. Our solution belongs to learning based methods, which have recently become popular to stylize images in artistic forms such as painting. However, existing methods do not produce satisfactory results for cartoonization, due to the fact that (1) cartoon styles have unique characteristics with high level simplification and abstraction, and (2) cartoon images tend to have clear edges, smooth color shading and relatively simple textures, which exhibit significant challenges for texture-descriptor-based loss functions used in existing methods. In this paper, we propose CartoonGAN, a generative adversarial network (GAN) framework for cartoon stylization. Our method takes unpaired photos and cartoon images for training, which is easy to use. Two novel losses suitable for cartoonization are proposed: (1) a semantic content loss, which is formulated as a sparse regularization in the high-level feature maps of the VGG network to cope with substantial style variation between photos and cartoons, and (2) an edge-promoting adversarial loss for preserving clear edges. We further introduce an initialization phase, to improve the convergence of the network to the target manifold. Our method is also much more efficient to train than existing methods. Experimental results show that our method is able to generate high-quality cartoon images from real-world photos (i.e., following specific artists' styles and with clear edges and smooth shading) and outperforms state-of-the-art methods.
Authors: Yang Chen, Tsinghua University;Yu-Kun Lai, Cardiff University;Yong-Jin Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf

708
Title: Conditional Image-to-Image Translation
Type: Poster
Abstracts: Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes and bags translations. The results demonstrate the effectiveness of our proposed method.
Authors: Jianxin Lin, USTC;Yingce Xia, ;Tao Qin, ;Zhibo Chen, ;Tie-Yan Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Conditional_Image-to-Image_Translation_CVPR_2018_paper.pdf


709
Title: Human Appearance Transfer
Type: Poster
Abstracts: We propose an automatic person-to-person appearance transfer model based on explicit parametric 3d human representations and learned, constrained deep translation network architectures for photographic image synthesis. Given a single source image and a single target image, each corresponding to different human subjects, wearing different clothing and in different poses, our goal is to photo-realistically transfer the appearance from the source image onto the target image while preserving the target shape and clothing segmentation layout. Our solution to this new problem is formulated in terms of a computational pipeline that combines (1) 3d human pose and body shape estimation from monocular images, (2) identifying 3d surface colors elements (mesh triangles) visible in both images, that can be transferred directly using barycentric procedures, and (3) predicting surface appearance missing in the first image but visible in the second one using deep learning-based image synthesis techniques. Our model achieves promising results as supported by a perceptual user study where the participants rated around 65% of our results as good, very good or perfect, as well in automated tests (Inception scores and a Faster-RCNN human detector responding very similarly to real and model generated images). We further show how the proposed architecture can be profiled to automatically generate images of a person dressed with different clothing transferred from a person in another image, opening paths for applications in entertainment and photo-editing (e.g. embodying and posing as friends or famous actors), the fashion industry, or affordable online shopping of clothing.
Authors: Mihai Zanfir, IMAR and Lund University ;Alin-Ionut Popa, IMAR;Andrei Zanfir, IMAR and Lund University;Cristian Sminchisescu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.pdf


710
Title: Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes
Type: Poster
Authors: Elisabeta Marinoiu, IMAR and Lund University;Andrei Zanfir, IMAR and Lund University;Cristian Sminchisescu,;

711
Title: Egocentric Basketball Motion Planning from a Single First-Person Image
Type: Poster
Authors: Gedas Bertasius, University of Pennsylvania;Aaron Chan, U. of Southern California;University of Pennsylvania, USA;

712
Title: SGAN: An Alternative Training of Generative Adversarial Networks
Type: Poster
Abstracts: The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial "local" pairs of networks are trained independently so that a "global" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.
Authors: Tatjana Chavdarova, Idiap and EPFL;Francois Fleuret, Idiap Research Institute;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chavdarova_SGAN_An_Alternative_CVPR_2018_paper.pdf


713
Title: 3D Human Pose Reconstruction and Action Classification in Robot Assisted Therapy of Children with Autism
Type: Poster
Authors: Elisabeta Marinoiu, IMAR and Lund University;Mihai Zanfir, IMAR and Lund University ;Vlad Olaru, ;Cristian Sminchisescu,;

714
Title: Zero-Shot Super-Resolution using Deep Internal Learning
Type: Poster
Authors: Assaf Shocher, Weizmann institut of Science;Michal Irani, Weizmann Institute of Science;Nadav Cohen, Institute for Advanced Study;

715
Title: Deep Diffeomorphic Transformer Networks
Type: Poster
Abstracts: Spatial Transformer layers allow neural networks, at least in principle, to be invariant to large spatial transformations in image data. The model has, however, seen limited uptake as most practical implementations support only transformations that are too restricted, e.g. affine or homographic maps, and/or destructive maps, such as thin plate splines. We investigate the use of ﬂexible diffeomorphic image transformations within such networks and demonstrate that significant performance gains can be attained over currently-used models. The learned transformations are found to be both simple and intuitive, thereby providing insights into individual problem domains. With the proposed framework, a standard convolutional neural network matches state-of-the-art results on face veriﬁcation with only two extra lines of simple TensorFlow code.
Authors: Nicki Skafte Detlefsen, DTU;Oren Freifeld, Ben-Gurion University;Soren Hauberg, Technical University of Denmark;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Detlefsen_Deep_Diffeomorphic_Transformer_CVPR_2018_paper.pdf


716
Title: Single Image Dehazing via Conditional Generative Adversarial Network
Type: Poster
Abstracts: In this paper, we present an algorithm to directly restore a clear image from a hazy image. This problem is highly ill-posed and most existing algorithms often use hand-crafted features, e.g., dark channel, color disparity, maximum contrast, to estimate transmission maps and then atmospheric lights. In contrast, we solve this problem based on a conditional generative adversarial network (cGAN), where the clear image is estimated by an end-to-end trainable neural network. Different from the generative network in basic cGAN, we propose an encoder and decoder architecture so that it can generate better results. To generate realistic clear images, we further modify the basic cGAN formulation by introducing the VGG features and a L_1-regularized gradient prior. We also synthesize a hazy dataset including indoor and outdoor scenes to train and evaluate the proposed algorithm. Extensive experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on both synthetic dataset and real world hazy images.
Authors: Runde Li, NJUST;Jinshan Pan, UC Merced;Zechao Li, Nanjing University of Science and Technology ;Jinhui Tang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Single_Image_Dehazing_CVPR_2018_paper.pdf


717
Title: Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination
Type: Spotlight
Abstracts: This paper presents a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (who’s better?) and overall (who’s best?) ranking of video collections, using supervised deep ranking. We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.
Authors: Hazel Doughty, University of Bristol;Dima Damen, University of Bristol;Walterio Mayol-Cuevas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Doughty_Whos_Better_Whos_CVPR_2018_paper.pdf


718
Title: Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination
Type: Poster
Abstracts: This paper presents a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (who’s better?) and overall (who’s best?) ranking of video collections, using supervised deep ranking. We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.
Authors: Hazel Doughty, University of Bristol;Dima Damen, University of Bristol;Walterio Mayol-Cuevas,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Doughty_Whos_Better_Whos_CVPR_2018_paper.pdf


719
Title: HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization
Type: Spotlight
Abstracts: Although video summarization has achieved great success in recent years, few approaches have realized the influence of video structure on the summarization results. As we know, the video data follow a hierarchical structure, i.e., a video is composed of shots, and a shot is composed of several frames. Generally, shots provide the activity-level information for people to understand the video content. While few existing summarization approaches pay attention to the shot segmentation procedure. They generate shots by some trivial strategies, such as fixed length segmentation, which may destroy the underlying hierarchical structure of video data and further reduce the quality of generated summaries. To address this problem, we propose a structure-adaptive video summarization approach that integrates shot segmentation and video summarization into a Hierarchical Structure-Adaptive RNN, denoted as HSA-RNN. We evaluate the proposed approach on four popular datasets, i.e., SumMe, TVsum, CoSum and VTW. The experimental results have demonstrated the effectiveness of HSA-RNN in the video summarization task.
Authors: Bin Zhao, Northwestern Polytechnical Uni;Xuelong Li, ;Xiaoqiang Lu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_HSA-RNN_Hierarchical_Structure-Adaptive_CVPR_2018_paper.pdf


720
Title: HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization
Type: Poster
Abstracts: Although video summarization has achieved great success in recent years, few approaches have realized the influence of video structure on the summarization results. As we know, the video data follow a hierarchical structure, i.e., a video is composed of shots, and a shot is composed of several frames. Generally, shots provide the activity-level information for people to understand the video content. While few existing summarization approaches pay attention to the shot segmentation procedure. They generate shots by some trivial strategies, such as fixed length segmentation, which may destroy the underlying hierarchical structure of video data and further reduce the quality of generated summaries. To address this problem, we propose a structure-adaptive video summarization approach that integrates shot segmentation and video summarization into a Hierarchical Structure-Adaptive RNN, denoted as HSA-RNN. We evaluate the proposed approach on four popular datasets, i.e., SumMe, TVsum, CoSum and VTW. The experimental results have demonstrated the effectiveness of HSA-RNN in the video summarization task.
Authors: Bin Zhao, Northwestern Polytechnical Uni;Xuelong Li, ;Xiaoqiang Lu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_HSA-RNN_Hierarchical_Structure-Adaptive_CVPR_2018_paper.pdf


721
Title: Detect globally
Type: Poster
Authors: TIANTIAN WANG, Dalian University of Technolog;Lihe Zhang, Dalian University of Technology;Huchuan Lu, Dalian University of Technology;Ali Borji, UCF;

722
Title: Improving Landmark Localization with Semi-Supervised Learning
Type: Poster
Authors: Sina Honari, University of Montreal;Pavlo Molchanov, NVIDIA Research;Jan Kautz, NVIDIA;Stephen Tyree, ;Christopher Pal, Ecole Polytechnique de Montreal;Pascal Vincent, University of Montreal;

723
Title: Reward Learning by Instruction
Type: Poster
Authors: Hsiao-Yu Tung, Carnegie Mellon University;Adam Harley, Carnegie Mellon University;Katerina Fragkiadaki, Carnegie Mellon University;

724
Title: The Lov?sz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks
Type: Poster
Authors: ESAT-PSI, KU Leuven;Amal Rannen Triki, KU Leuven;Matthew Blaschko, KU Leuven;

725
Title: Facial Expression Recognition by De-expression Residue Learning
Type: Poster
Authors: Huiyuan Yang, Binghamton University-SUNY;Umur Ciftci, Binghamton University-SUNY;Lijun Yin, Binghamton University State University of New York;

726
Title: Learning to Understand Image Blur
Type: Poster
Abstracts: While many approaches have been proposed to estimate and remove blur in a photo, few efforts were made to have an algorithm automatically understand the blur desirability: whether the blur is desired or not, and how it affects the quality of the photo. Such a task not only relies on low-level visual features to identify blurry regions, but also requires high-level understanding of the image content as well as user intent during photo capture. In this paper, we propose a unified framework to estimate a spatially-varying blur map and understand its desirability in terms of image quality at the same time. In particular, we use a dilated fully convolutional neural network with pyramid pooling and boundary refinement layers to generate high-quality blur response maps. If blur exists, we classify its desirability to three levels ranging from good to bad, by distilling high-level semantics and learning an attention map to adaptively localize the important content in the image. The whole framework is end-to-end jointly trained with both supervisions of pixel-wise blur responses and image-wise blur desirability levels. Considering the limitations of existing image blur datasets, we collected a new large-scale dataset with both annotations to facilitate training. The proposed methods are extensively evaluated on two datasets and demonstrate state-of-the-art performance on both tasks.
Authors: Shanghang Zhang, ;Xiaohui Shen, Adobe Research;Adobe Systems, Inc.;Radom?r Mech, ;Jo?o Costeira, ;Jose Moura, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_to_Understand_CVPR_2018_paper.pdf


727
Title: Hierarchical Novelty Detection for Visual Object Recognition
Type: Poster
Abstracts: Deep neural networks have achieved impressive success in large-scale visual object recognition tasks with a predefined set of classes. However, recognizing objects of novel classes unseen during training still remains challenging. The problem of detecting such novel classes has been addressed in the literature, but most prior works have focused on providing simple binary or regressive decisions, e.g., the output would be "known," "novel," or corresponding confidence intervals. In this paper, we study more informative novelty detection schemes based on a hierarchical classification framework. For an object of a novel class, we aim for finding its closest super class in the hierarchical taxonomy of known classes. To this end, we propose two different approaches termed top-down and flatten methods, and their combination as well. The essential ingredients of our methods are confidence-calibrated classifiers, data relabeling, and the leave-one-out strategy for modeling novel classes under the hierarchical taxonomy. Furthermore, our method can generate a hierarchical embedding that leads to improved generalized zero-shot learning performance in combination with other commonly-used semantic embeddings.
Authors: Kibok Lee, University of Michigan;Kimin Lee, KAIST;Kyle Min, University of Michigan;Yuting Zhang, University of Michigan;Jinwoo Shin, KAIST;University of Michigan, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.pdf


728
Title: Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition
Type: Poster
Authors: Yaming Wang, University of Maryland;Vlad Morariu, University of Maryland;University of Maryland, USA;

729
Title: Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data
Type: Poster
Abstracts: Paucity of large curated hand labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multitask learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.
Authors: Arghya Pal, Indian Institute of Technology;Vineeth Balasubramanian, IIT Hyderabad;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pal_Adversarial_Data_Programming_CVPR_2018_paper.pdf


730
Title: Compare and Contrast: Learning Prominent Visual Differences
Type: Poster
Abstracts: Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW-10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems.
Authors: Steven Chen, University of Texas at Austin;Kristen Grauman,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Compare_and_Contrast_CVPR_2018_paper.pdf


731
Title: SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis
Type: Poster
Abstracts: Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.
Authors: Wengling Chen, Georgia Institute of Technolog;James Hays, Georgia Tech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.pdf


732
Title: Grounding Referring Expressions in Images by Variational Context
Type: Poster
Abstracts: We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., ``largest elephant standing behind baby elephant''. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context --- visual attributes (e.g., ``largest'', ``baby'') and relationships (e.g., ``behind'') that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings. The code is available at url{https://github.com/yuleiniu/vc/
Authors: Hanwang Zhang, Columbia University;Yulei Niu, Renmin University of China;Shih-Fu Chang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.pdf


733
Title: Multi-Content GAN for Few-Shot Font Style Transfer
Type: Spotlight
Abstracts: In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.
Authors: Samaneh Azadi, UC Berkeley;Matthew Fisher, Adobe;Vladimir Kim, Adobe Research;Zhaowen Wang, Adobe;Eli Shechtman, Adobe Research;UC Berkeley, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.pdf


734
Title: Multi-Content GAN for Few-Shot Font Style Transfer
Type: Poster
Abstracts: In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.
Authors: Samaneh Azadi, UC Berkeley;Matthew Fisher, Adobe;Vladimir Kim, Adobe Research;Zhaowen Wang, Adobe;Eli Shechtman, Adobe Research;UC Berkeley, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.pdf


735
Title: LIME: Live Intrinsic Material Estimation
Type: Spotlight
Abstracts: We present the first end-to-end approach for real-time material estimation for general object shapes with uniform material that only requires a single color image as input. In addition to Lambertian surface properties, our approach fully automatically computes the specular albedo, material shininess, and a foreground segmentation. We tackle this challenging and ill-posed inverse rendering problem using recent advances in image-to-image translation techniques based on deep convolutional encoder–decoder architectures. The underlying core representations of our approach are specular shading, diffuse shading and mirror images, which allow to learn the effective and accurate separation of diffuse and specular albedo. In addition, we propose a novel highly efficient perceptual rendering loss that mimics real world image formation and obtains intermediate results even during run time. The estimation of material parameters at real-time frame rates enables exciting mixed reality applications, such as seamless illumination-consistent integration of virtual objects into realworld scenes, and virtual material cloning.We demonstrate our approach in a live setup, compare it to the state of the art, and demonstrate its effectiveness through quantitative and qualitative evaluation.
Authors: Abhimitra Meka, Max Planck Institute for Infor;Graduate School of Computer Science, Saarland University;Michael Zollh?fer, MPI Informatics;Avishek Chatterjee, Max Planck Institute for Informatics;Hans-Peter Seidel, Max Planck Institute for Informatics;Christian Richardt, University of Bath;Christian Theobalt, MPI Informatics;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Meka_LIME_Live_Intrinsic_CVPR_2018_paper.pdf


736
Title: LIME: Live Intrinsic Material Estimation
Type: Poster
Abstracts: We present the first end-to-end approach for real-time material estimation for general object shapes with uniform material that only requires a single color image as input. In addition to Lambertian surface properties, our approach fully automatically computes the specular albedo, material shininess, and a foreground segmentation. We tackle this challenging and ill-posed inverse rendering problem using recent advances in image-to-image translation techniques based on deep convolutional encoder–decoder architectures. The underlying core representations of our approach are specular shading, diffuse shading and mirror images, which allow to learn the effective and accurate separation of diffuse and specular albedo. In addition, we propose a novel highly efficient perceptual rendering loss that mimics real world image formation and obtains intermediate results even during run time. The estimation of material parameters at real-time frame rates enables exciting mixed reality applications, such as seamless illumination-consistent integration of virtual objects into realworld scenes, and virtual material cloning.We demonstrate our approach in a live setup, compare it to the state of the art, and demonstrate its effectiveness through quantitative and qualitative evaluation.
Authors: Abhimitra Meka, Max Planck Institute for Infor;Graduate School of Computer Science, Saarland University;Michael Zollh?fer, MPI Informatics;Avishek Chatterjee, Max Planck Institute for Informatics;Hans-Peter Seidel, Max Planck Institute for Informatics;Christian Richardt, University of Bath;Christian Theobalt, MPI Informatics;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Meka_LIME_Live_Intrinsic_CVPR_2018_paper.pdf


737
Title: Multi-Agent Diverse Generative Adversarial Networks
Type: Spotlight
Abstracts: We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task.
Authors: Viveka Kulharia, University of Oxford;Arnab Ghosh, University of Oxford;Vinay P. Namboodiri, Indian Institute of Technology Kanpur;Phil Torr, Oxford;Puneet Kumar Dokania, University of Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.pdf


738
Title: Multi-Agent Diverse Generative Adversarial Networks
Type: Poster
Abstracts: We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task.
Authors: Viveka Kulharia, University of Oxford;Arnab Ghosh, University of Oxford;Vinay P. Namboodiri, Indian Institute of Technology Kanpur;Phil Torr, Oxford;Puneet Kumar Dokania, University of Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.pdf


739
Title: Light field intrinsics with a deep encoder-decoder network
Type: Spotlight
Authors: Anna Alperovich, University of Konstanz;Ole Johannsen, University of Konstanz;Michael Strecke, University of Konstanz;Bastian Goldluecke,;

740
Title: Light field intrinsics with a deep encoder-decoder network
Type: Poster
Authors: Anna Alperovich, University of Konstanz;Ole Johannsen, University of Konstanz;Michael Strecke, University of Konstanz;Bastian Goldluecke,;

741
Title: Density Adaptive Point Set Registration
Type: Oral
Abstracts: Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling.
Authors: Felix J?remo Lawin, Link?ping University;Martin Danelljan, ;Linkoping University , Sweden;Per-Erik Forssen, Linkoping University;Michael Felsberg, Link_ping University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf


742
Title: Density Adaptive Point Set Registration
Type: Poster
Abstracts: Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling.
Authors: Felix J?remo Lawin, Link?ping University;Martin Danelljan, ;Linkoping University , Sweden;Per-Erik Forssen, Linkoping University;Michael Felsberg, Link_ping University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf


743
Title: Learning Monocular 3D Human Pose estimation on weakly-supervised Multi-view Images
Type: Spotlight
Authors: Helge Rhodin, epfl.ch;J?rg Sp?rri, Balgrist;EPFL Lausanne, Switzerland;Victor Constantin, EPFL;Fr?d?ric Meyer, ;Erich M?ller, ;Mathieu Salzmann, EPFL;Pascal Fua,;

744
Title: Learning Monocular 3D Human Pose estimation on weakly-supervised Multi-view Images
Type: Poster
Authors: Helge Rhodin, epfl.ch;J?rg Sp?rri, Balgrist;EPFL Lausanne, Switzerland;Victor Constantin, EPFL;Fr?d?ric Meyer, ;Erich M?ller, ;Mathieu Salzmann, EPFL;Pascal Fua,;

745
Title: Emotional Attention: A Study of Image Sentiment and Visual Attention
Type: Spotlight
Abstracts: Image sentiment influences visual perception. Emotion-eliciting stimuli such as happy faces and poisonous snakes are generally prioritized in human attention. However, little research has evaluated the interrelationships of image sentiment and visual saliency. In this paper, we present the first study to focus on the relation between emotional properties of an image and visual attention. We first create the EMOtional attention dataset (EMOd). It is a diverse set of emotion-eliciting images, and each image has (1) eye-tracking data collected from 16 subjects, (2) intensive image context labels including object contour, object sentiment, object semantic category, and high-level perceptual attributes such as image aesthetics and elicited emotions. We perform extensive analyses on EMOd to identify how image sentiment relates to human attention. We discover an emotion prioritization effect: for our images, emotion-eliciting content attracts human attention strongly, but such advantage diminishes dramatically after initial fixation. Aiming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context of the image scene. The proposed network outperforms the state-of-the-art on three benchmark datasets, by effectively capturing the relative importance of human attention within an image. The code, models, and dataset are available online at https://nus-sesame.top/emotionalattention/.
Authors: Shaojing Fan, National University of Singapo;Zhiqi Shen, National University of Singapore;Ming Jiang, University of Minnesota;Bryan Koenig, Southern Utah University;Juan Xu, University of Minnesota;Mohan Kankanhalli, National University of Singapore;Qi Zhao,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Emotional_Attention_A_CVPR_2018_paper.pdf


746
Title: Emotional Attention: A Study of Image Sentiment and Visual Attention
Type: Poster
Abstracts: Image sentiment influences visual perception. Emotion-eliciting stimuli such as happy faces and poisonous snakes are generally prioritized in human attention. However, little research has evaluated the interrelationships of image sentiment and visual saliency. In this paper, we present the first study to focus on the relation between emotional properties of an image and visual attention. We first create the EMOtional attention dataset (EMOd). It is a diverse set of emotion-eliciting images, and each image has (1) eye-tracking data collected from 16 subjects, (2) intensive image context labels including object contour, object sentiment, object semantic category, and high-level perceptual attributes such as image aesthetics and elicited emotions. We perform extensive analyses on EMOd to identify how image sentiment relates to human attention. We discover an emotion prioritization effect: for our images, emotion-eliciting content attracts human attention strongly, but such advantage diminishes dramatically after initial fixation. Aiming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context of the image scene. The proposed network outperforms the state-of-the-art on three benchmark datasets, by effectively capturing the relative importance of human attention within an image. The code, models, and dataset are available online at https://nus-sesame.top/emotionalattention/.
Authors: Shaojing Fan, National University of Singapo;Zhiqi Shen, National University of Singapore;Ming Jiang, University of Minnesota;Bryan Koenig, Southern Utah University;Juan Xu, University of Minnesota;Mohan Kankanhalli, National University of Singapore;Qi Zhao,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Emotional_Attention_A_CVPR_2018_paper.pdf


747
Title: Geometry-Guided CNN for Self-supervised Video Representation learning
Type: Poster
Authors: Chuang Gan, Tsinghua University;Boqing Gong, University of Central Florida;Kun Liu, Beijing University of Posts and Telecommunications;hao Su, ;Leonidas J. Guibas,;

748
Title: Multi-Level Fusion based 3D Object Detection from Monocular Images
Type: Poster
Authors: Bin Xu, ;Zhenzhong Chen, Wuhan University;

749
Title: Explicit Loss-Error-Aware Quantization for Deep Neural Networks
Type: Poster
Authors: Aojun Zhou, Intel labs china;Anbang Yao,;

750
Title: Generative Adversarial Perturbations
Type: Poster
Abstracts: In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.
Authors: Omid Poursaeed, Cornell University;Isay Katsman, Cornell University;Bicheng Gao, Shanghai Jiao Tong University;Serge Belongie,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf


751
Title: A Hybrid L1-L0 Layer Decomposition Model for Tone Mapping
Type: Poster
Authors: Zhetong Liang, PolyU;Jun Xu, Hong Kong Polytechnic U;David Zhang, Hong Kong Polytechnic University;Zisheng Cao, ;Lei Zhang, The Hong Kong Polytechnic University;

752
Title: Learning Deep Correspondence through Prior and Posterior Feature Constancy
Type: Poster
Authors: Zhengfa Liang, NUDT;Yiliu Feng, NUDT;Yulan Guo, NUDT;Hengzhu Liu, NUDT;Wei Chen, ;Linbo Qiao, ;Li Zhou, NUDT;Jianfeng Zhang, NUDT;

753
Title: Through-Wall Human Pose Estimation Using Radio Signals
Type: Spotlight
Abstracts: This paper demonstrates accurate human pose estimation through walls and occlusions. We leverage the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body. We introduce a deep neural network approach that parses such radio signals to estimate 2D poses. Since humans cannot annotate radio signals, we use state-of-the-art vision model to provide cross-modal supervision. Specifically, during training the system uses synchronized wireless and visual inputs, extracts pose information from the visual stream, and uses it to guide the training process. Once trained, the network uses only the wireless signal for pose estimation. We show that, when tested on visible scenes, the radio-based system is almost as accurate as the vision-based system used to train it. Yet, unlike vision-based pose estimation, the radio-based system can estimate 2D poses through walls despite never trained on such scenarios. Demo videos are available at our website (http://rfpose.csail.mit.edu).
Authors: Mingmin Zhao, MIT;Tianhong Li, MIT;Mohammad Abu Alsheikh, MIT;Yonglong Tian, Massachusetts Institute of Technology;Hang Zhao, MIT;Antonio Torralba, MIT;Dina Katabi, MIT;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf


754
Title: Through-Wall Human Pose Estimation Using Radio Signals
Type: Poster
Abstracts: This paper demonstrates accurate human pose estimation through walls and occlusions. We leverage the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body. We introduce a deep neural network approach that parses such radio signals to estimate 2D poses. Since humans cannot annotate radio signals, we use state-of-the-art vision model to provide cross-modal supervision. Specifically, during training the system uses synchronized wireless and visual inputs, extracts pose information from the visual stream, and uses it to guide the training process. Once trained, the network uses only the wireless signal for pose estimation. We show that, when tested on visible scenes, the radio-based system is almost as accurate as the vision-based system used to train it. Yet, unlike vision-based pose estimation, the radio-based system can estimate 2D poses through walls despite never trained on such scenarios. Demo videos are available at our website (http://rfpose.csail.mit.edu).
Authors: Mingmin Zhao, MIT;Tianhong Li, MIT;Mohammad Abu Alsheikh, MIT;Yonglong Tian, Massachusetts Institute of Technology;Hang Zhao, MIT;Antonio Torralba, MIT;Dina Katabi, MIT;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf


755
Title: End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching
Type: Poster
Authors: Georgios Georgakis, George Mason University;Srikrishna Karanam, Siemens Corporate Technology;Ziyan Wu, Siemens Corporation;Jan Ernst, Siemens Corporation;Jana Kosecka, George Mason Univiversity;

756
Title: Learning Multi-grid Generative ConvNets by Minimal Contrastive Divergence
Type: Spotlight
Authors: Ruiqi Gao, UCLA;Yang Lu, University of California Los Angeles;Junpei Zhou, ;Song-Chun Zhu, ;Yingnian Wu,;

757
Title: Learning Multi-grid Generative ConvNets by Minimal Contrastive Divergence
Type: Poster
Authors: Ruiqi Gao, UCLA;Yang Lu, University of California Los Angeles;Junpei Zhou, ;Song-Chun Zhu, ;Yingnian Wu,;

758
Title: Matching Adversarial Networks
Type: Poster
Abstracts: Generative Adversarial Nets (GANs) and Conditonal GANs (CGANs) show that using a trained network as loss function (discriminator) enables to synthesize highly structured outputs (e.g. natural images). However, applying a discriminator network as a universal loss function for common supervised tasks (e.g. semantic segmentation, line detection, depth estimation) is considerably less successful. We argue that the main difficulty of applying CGANs to supervised tasks is that the generator training consists of optimizing a loss function that does not depend directly on the ground truth labels. To overcome this, we propose to replace the discriminator with a matching network taking into account both the ground truth outputs as well as the generated examples. As a consequence, the generator loss function also depends on the targets of the training examples, thus facilitating learning. We demonstrate on three computer vision tasks that this approach can significantly outperform CGANs achieving comparable or superior results to task-specific solutions and results in stable training. Importantly, this is a general approach that does not require the use of task-specific loss functions.
Authors: Gellert Mattyus, UBER ATG;Raquel Urtasun, University of Toronto;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.pdf


759
Title: Stochastic Variational Inference with Gradient Linearization
Type: Poster
Authors: Tobias Pl?tz, TU Darmstadt;Anne Wannenwetsch, TU Darmstadt;Stefan Roth,;

760
Title: Geometry-aware Deep Network for Single-Image Novel View Synthesis
Type: Poster
Authors: Data61,CSIRO;Xuming He, ShanghaiTech;Mathieu Salzmann, EPFL;

761
Title: Robust Depth Estimation from Auto Bracketed Images
Type: Poster
Authors: Sunghoon Im, KAIST;Hae-Gon Jeon, KAIST;In So Kweon, KAIST;

762
Title: Document Enhancement using Visibility Detection
Type: Poster
Authors: Nati Kligler, Technion;Sagi Katz, Technion;Ayellet Tal, Technion;

763
Title: Co-Occurrence Template Matching
Type: Poster
Authors: Shai Avidan, ;rotal kat, Tel-Aviv University;roy jevnisek, Tel-Aviv University;

764
Title: Intrinsic Image Transformation via Scale Space Decomposition
Type: Poster
Abstracts: We introduce a new network structure for decomposing an image into its intrinsic albedo and shading. We treat this as an image-to-image transformation problem and explore the scale space of the input and output. By expanding the output images (albedo and shading) into their Laplacian pyramid components, we develop a multi-channel network structure that learns the image-to-image transformation function in successive frequency bands in parallel, within each channel is a fully convolutional neural network with skip connections. This network structure is general and extensible, and has demonstrated excellent performance on the intrinsic image decomposition problem. We evaluate the network on two benchmark datasets: the MPI-Sintel dataset and the MIT Intrinsic Images dataset. Both quantitative and qualitative results show our model delivers a clear progression over state-of-the-art.
Authors: Lechao Cheng, ;Chengyi Zhang, Zhejiang University;Zicheng Liao,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Intrinsic_Image_Transformation_CVPR_2018_paper.pdf


765
Title: Depth and Transient Imaging with Compressive SPAD Array Cameras
Type: Poster
Authors: Qilin Sun, KAUST;Xiong Dun, KAUST;Yifan (Evan) Peng, UBC;Wolfgang Heidrich,;

766
Title: Efficient and Deep Person Re-Identification using Multi-Level Similarity
Type: Poster
Authors: Yiluan Guo, SUTD;Ngai-Man Cheung,;

767
Title: Hybrid Camera Pose Estimation
Type: Oral
Abstracts: In this paper, we aim to solve the pose estimation problem of calibrated pinhole and generalized cameras w.r.t. a Structure-from-Motion (SfM) model by leveraging both 2D-3D correspondences as well as 2D-2D correspondences. Traditional approaches either focus on the use of 2D-3D matches, known as structure-based pose estimation or solely on 2D-2D matches (structure-less pose estimation). Absolute pose approaches are limited in their performance by the quality of the 3D point triangulations as well as the completeness of the 3D model. Relative pose approaches, on the other hand, while being more accurate, also tend to be far more computationally costly and often return dozens of possible solutions. This work aims to bridge the gap between these two paradigms. We propose a new RANSAC-based approach that automatically chooses the best type of solver to use at each iteration in a data-driven way. The solvers chosen by our RANSAC can range from pure structure-based or structure-less solvers, to any possible combination of hybrid solvers (i.e. using both types of matches) in between. A number of these new hybrid minimal solvers are also presented in this paper. Both synthetic and real data experiments show our approach to be as accurate as structure-less approaches, while staying close to the efficiency of structure-based methods.
Authors: Federico Camposeco, ETH;Andrea Cohen, ETH Zurich;Marc Pollefeys, ETH;Torsten Sattler, ETH Zurich;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.pdf


768
Title: Hybrid Camera Pose Estimation
Type: Poster
Abstracts: In this paper, we aim to solve the pose estimation problem of calibrated pinhole and generalized cameras w.r.t. a Structure-from-Motion (SfM) model by leveraging both 2D-3D correspondences as well as 2D-2D correspondences. Traditional approaches either focus on the use of 2D-3D matches, known as structure-based pose estimation or solely on 2D-2D matches (structure-less pose estimation). Absolute pose approaches are limited in their performance by the quality of the 3D point triangulations as well as the completeness of the 3D model. Relative pose approaches, on the other hand, while being more accurate, also tend to be far more computationally costly and often return dozens of possible solutions. This work aims to bridge the gap between these two paradigms. We propose a new RANSAC-based approach that automatically chooses the best type of solver to use at each iteration in a data-driven way. The solvers chosen by our RANSAC can range from pure structure-based or structure-less solvers, to any possible combination of hybrid solvers (i.e. using both types of matches) in between. A number of these new hybrid minimal solvers are also presented in this paper. Both synthetic and real data experiments show our approach to be as accurate as structure-less approaches, while staying close to the efficiency of structure-based methods.
Authors: Federico Camposeco, ETH;Andrea Cohen, ETH Zurich;Marc Pollefeys, ETH;Torsten Sattler, ETH Zurich;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.pdf


769
Title: SoS-RSC: A Sum-of-Squares Polynomial Approach to Robustifying Subspace Clustering Algorithms
Type: Poster
Abstracts: This paper addresses the problem of subspace clustering in the presence of outliers. Typically, this scenario is handled through a regularized optimization, whose computational complexity scales polynomially with the size of the data. Further, the regularization terms need to be manually tuned to achieve optimal performance. To circumvent these difficulties, in this paper we propose an outlier removal algorithm based on evaluating a suitable sum-ofsquares polynomial, computed directly from the data. This algorithm only requires performing two singular value decompositions of fixed size, and provides certificates on the probability of misclassifying outliers as inliers.
Authors: Northeastern University, USA;Mario Sznaier,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sznaier_SoS-RSC_A_Sum-of-Squares_CVPR_2018_paper.pdf


770
Title: Alive Caricature from 2D to 3D
Type: Spotlight
Authors: Qianyi Wu, USTC;Juyong Zhang, University of Science and Technology of China;Yu-Kun Lai, Cardiff University;Jianmin Zheng, Nanyang Technological University;Jianfei Cai,;

771
Title: Alive Caricature from 2D to 3D
Type: Poster
Authors: Qianyi Wu, USTC;Juyong Zhang, University of Science and Technology of China;Yu-Kun Lai, Cardiff University;Jianmin Zheng, Nanyang Technological University;Jianfei Cai,;

772
Title: Arbitrary Style Transfer with Deep Feature Reshuffle
Type: Poster
Authors: Shuyang Gu, USTC;Congliang Chen, Peking University;Jing Liao, ;Lu Yuan, Microsoft Research Asia;

773
Title: Self-Supervised Feature Learning by Learning to Spot Artifacts
Type: Spotlight
Abstracts: We introduce a novel self-supervised learning method based on adversarial training. Our objective is to train a discriminator network to distinguish real images from images with synthetic artifacts, and then to extract features from its intermediate layers that can be transferred to other data domains and tasks. To generate images with artifacts, we pre-train a high-capacity autoencoder and then we use a damage and repair strategy: First, we freeze the autoencoder and damage the output of the encoder by randomly dropping its entries. Second, we augment the decoder with a repair network, and train it in an adversarial manner against the discriminator. The repair network helps generate more realistic images by inpainting the dropped feature entries. To make the discriminator focus on the artifacts, we also make it predict what entries in the feature were dropped. We demonstrate experimentally that features learned by creating and spotting artifacts achieve state of the art performance in several benchmarks.
Authors: Simon Jenni, Universit?t Bern;Bern University, Switzerland;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.pdf


774
Title: Self-Supervised Feature Learning by Learning to Spot Artifacts
Type: Poster
Abstracts: We introduce a novel self-supervised learning method based on adversarial training. Our objective is to train a discriminator network to distinguish real images from images with synthetic artifacts, and then to extract features from its intermediate layers that can be transferred to other data domains and tasks. To generate images with artifacts, we pre-train a high-capacity autoencoder and then we use a damage and repair strategy: First, we freeze the autoencoder and damage the output of the encoder by randomly dropping its entries. Second, we augment the decoder with a repair network, and train it in an adversarial manner against the discriminator. The repair network helps generate more realistic images by inpainting the dropped feature entries. To make the discriminator focus on the artifacts, we also make it predict what entries in the feature were dropped. We demonstrate experimentally that features learned by creating and spotting artifacts achieve state of the art performance in several benchmarks.
Authors: Simon Jenni, Universit?t Bern;Bern University, Switzerland;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.pdf


775
Title: Multi-Label Zero-Shot Learning with Structured Knowledge Graphs
Type: Poster
Authors: Chung-Wei Lee, National Taiwan University;Wei Fang, National Taiwan University;Chih-Kuan Yeh, Carnegie Mellon University;Yu-Chiang Frank Wang, Academia Sinica;

776
Title: Towards High Performance Video Object Detection
Type: Spotlight
Abstracts: There has been significant progresses for image object detection recently. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios. Built upon the recent works, this work proposes a unified viewpoint based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope (speed-accuracy tradeoff), towards high performance video object detection.
Authors: Xizhou Zhu, ;Jifeng Dai, Microsoft Research;Lu Yuan, Microsoft Research Asia;Yichen Wei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_High_Performance_CVPR_2018_paper.pdf


777
Title: Towards High Performance Video Object Detection
Type: Poster
Abstracts: There has been significant progresses for image object detection recently. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios. Built upon the recent works, this work proposes a unified viewpoint based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope (speed-accuracy tradeoff), towards high performance video object detection.
Authors: Xizhou Zhu, ;Jifeng Dai, Microsoft Research;Lu Yuan, Microsoft Research Asia;Yichen Wei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_High_Performance_CVPR_2018_paper.pdf


778
Title: Mesoscopic Facial Geometry inference Using Deep Neural Networks
Type: Spotlight
Authors: Loc Huynh, USC ICT;Weikai Chen, USC ICT;Shunsuke Saito, ;Jun Xing, ICT;Pinscreen, Inc;Andrew Jones, USC ICT;Paul Debevec, USC ICT;Hao Li,;

779
Title: Mesoscopic Facial Geometry inference Using Deep Neural Networks
Type: Poster
Authors: Loc Huynh, USC ICT;Weikai Chen, USC ICT;Shunsuke Saito, ;Jun Xing, ICT;Pinscreen, Inc;Andrew Jones, USC ICT;Paul Debevec, USC ICT;Hao Li,;

780
Title: Relation Networks for Object Detection
Type: Oral
Abstracts: Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances extbf{individually}, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects extbf{simultaneously} through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the extbf{first fully end-to-end object detector}.
Authors: Han Hu, ;Jiayuan Gu, Microsoft;Zheng Zhang, Microsoft;Jifeng Dai, Microsoft Research;Yichen Wei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Relation_Networks_for_CVPR_2018_paper.pdf


781
Title: Relation Networks for Object Detection
Type: Poster
Abstracts: Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances extbf{individually}, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects extbf{simultaneously} through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the extbf{first fully end-to-end object detector}.
Authors: Han Hu, ;Jiayuan Gu, Microsoft;Zheng Zhang, Microsoft;Jifeng Dai, Microsoft Research;Yichen Wei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Relation_Networks_for_CVPR_2018_paper.pdf


782
Title: Mobile Video Object Detection with Temporally-Aware Feature Maps
Type: Poster
Authors: Menglong Zhu, ;Mason Liu, Georgia Tech;

783
Title: Free supervision from video games
Type: Poster
Authors: Philipp Krahenbuhl,;

784
Title: Adversarially Learned One-Class Classifier for Novelty Detection
Type: Poster
Abstracts: Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.
Authors: Mohammad Sabokrou, Institute for Research in Fundamental Sciences (IPM);Mohammad Khalooie, ;Mahmood Fathi, ;Ehsan Adeli, Stanford University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf


785
Title: Fast
Type: Poster
Authors: Ariel Gordon, Google;Elad Eban, Google;Bo Chen, Google;ofir Nachum, Google;Tien-Ju Yang, Massachusetts Institute of Technology;Edward Choi, Georgia Institute of Technology;

786
Title: Resource Aware Person Re-identification across Multiple Resolutions
Type: Poster
Authors: Yan Wang, Cornell university;Lequn Wang, Cornell University;yurong you, shang hai jiao tong university;xu zou, tsinghua university;Vincent Chen, cornell university;Serena Li, CORNELL UNIVERSITY;Bharath Hariharan, Cornell University;Gao Huang, ;Kilian Weinberger, Cornell University;

787
Title: Deep Learning under Privileged Information Using Heteroscedastic Dropout
Type: Spotlight
Authors: Ozan Sener, Stanford University;Silvio Savarese, ;John Lambert, Stanford University;

788
Title: Deep Learning under Privileged Information Using Heteroscedastic Dropout
Type: Poster
Authors: Ozan Sener, Stanford University;Silvio Savarese, ;John Lambert, Stanford University;

789
Title: Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks
Type: Poster
Authors: Long Chen, ZJU;Hanwang Zhang, Columbia University;Jun Xiao, ZJU;Wei Liu, ;Shih-Fu Chang,;

790
Title: Learning and Using the Arrow of Time
Type: Poster
Abstracts: We seek to understand the arrow of time in videos -- what makes videos look like they are playing forwards or backwards? Can we visualize the cues? Can the arrow of time be a supervisory signal useful for activity analysis? To this end, we build three large-scale video datasets and apply a learning-based approach to these tasks. To learn the arrow of time efficiently and reliably, we design a ConvNet suitable for extended temporal footprints and for class activation visualization, and study the effect of artificial cues, such as cinematographic conventions, on learning. Our trained model achieves state-of-the-art performance on large-scale real-world video datasets. Through cluster analysis and localization of important regions for the prediction, we examine learned visual cues that are consistent among many samples and show when and where they occur. Lastly, we use the trained ConvNet for two applications: self-supervision for action recognition, and video forensics -- determining whether Hollywood film clips have been deliberately reversed in time, often used as special effects.
Authors: Donglai Wei, MIT;Andrew Zisserman, Oxford;William Freeman, MIT/Google;Joseph Lim, University of Southern California;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Learning_and_Using_CVPR_2018_paper.pdf


791
Title: Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition
Type: Poster
Abstracts: Recognizing facial action units (AUs) during spontaneous facial displays is a challenging problem. Most recently, Convolutional Neural Networks (CNNs) have shown promise for facial AU recognition, where predefined and fixed convolution filter sizes are employed. In order to achieve the best performance, the optimal filter size is often empirically found by conducting extensive experimental validation. Such a training process suffers from expensive training cost, especially as the network becomes deeper. This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters. Specifically, the filter size is defined as a continuous variable, which is optimized by minimizing the training loss. Experimental results on two AU-coded spontaneous databases have shown that the proposed OFS-CNN is capable of estimating optimal filter size for varying image resolution and outperforms traditional CNNs with the best filter size obtained by exhaustive search. The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm.
Authors: Shizhong Han, 1986;zibo Meng, ;Zhiyuan Li, University of South Carolina;JAMES O'REILLY, University of South Carolina;Jie Cai, University of South Carolina;Xiaofeng Wang, University of South Carolina;Yan Tong, University of South Carolina;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Optimizing_Filter_Size_CVPR_2018_paper.pdf


792
Title: Revisiting Salient Object Detection: Simultaneous Detection
Type: Oral
Authors: Md Amirul Islam, University of Manitoba;Mahmoud Kalash, University of Manitoba;Neil D. B. Bruce, University of Manitoba;

793
Title: Revisiting Salient Object Detection: Simultaneous Detection
Type: Poster
Authors: Md Amirul Islam, University of Manitoba;Mahmoud Kalash, University of Manitoba;Neil D. B. Bruce, University of Manitoba;

794
Title: Gaze Prediction in Dynamic $360^\circ$ Immersive Videos
Type: Poster
Authors: Yanyu Xu, Shanghaitech University;Yanbing Dong, ;Junru Wu, ;Zhengzhong Sun, ;Zhiru Shi, ;Jingyi Yu, ;Shenghua Gao, ShanghaiTech University;

795
Title: Weakly-Supervised Semantic Segmentation Network with Deep Seeded Region Growing
Type: Poster
Authors: Zilong Huang, HUST;Xinggang Wang, ;Jiasi Wang, HUST;Wenyu Liu, ;Jingdong Wang, Microsoft Research;

796
Title: Modulated Convolutional Networks
Type: Poster
Abstracts: Despite great effectiveness of very deep and wide Convolutional Neural Networks (CNNs) in various computer vision tasks, the significant cost in terms of storage requirement of such networks impedes the deployment on computationally limited devices. In this paper, we propose new Modulated Convolutional Networks (MCNs) to improve the portability of CNNs via binarized filters. In MCNs, we propose a new loss function which considers the filter loss, center loss and softmax loss in an end-to-end framework. We first introduce modulation filters (M-Filters) to recover the unbinarized filters, which leads to a new architecture to calculate the network model. The convolution operation is further approximated by considering intra-class compactness in the loss function. As a result, our MCNs can reduce the size of required storage space of convolutional filters by a factor of 32, in contrast to the full-precision model, while achieving much better performances than state-of-the-art binarized models. Most importantly, MCNs achieve a comparable performance to the full-precision ResNets and Wide-ResNets. The code will be available publicly soon.
Authors: Xiaodi Wang, Beihang University;Baochang Zhang, ;Ce Li, CUMTB;Rongrong Ji, ;jungong han, ;Xianbin Cao, Beihang University;jianzhuang liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.pdf


797
Title: V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map
Type: Poster
Authors: Gyeongsik Moon, Seoul National University;Ju Yong Chang, Kwangwoon University;Kyoung Mu Lee,;

798
Title: SeedNet : Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation
Type: Poster
Authors: Gwangmo Song, Seoul National University;Heesoo Myeong, Samsung;Kyoung Mu Lee,;

799
Title: Deep Parametric Continuous Convolutional Neural Networks
Type: Spotlight
Abstracts: Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.
Authors: Shenlong Wang, ;Shun Da Suo, ;Wei-Chiu Ma, MIT;Raquel Urtasun, University of Toronto;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf


800
Title: Deep Parametric Continuous Convolutional Neural Networks
Type: Poster
Abstracts: Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.
Authors: Shenlong Wang, ;Shun Da Suo, ;Wei-Chiu Ma, MIT;Raquel Urtasun, University of Toronto;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf


801
Title: Optical Flow Guided Feature: A Motion Representation for Video Action Recognition
Type: Poster
Authors: Shuyang Sun, The University of Sydney;Zhanghui Kuang, Sense Time;Wanli Ouyang, The University of Sydney;Lu Sheng, The Chinese University of HK;Wei Zhang,;

802
Title: Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View
Type: Oral
Authors: Shuran Song, Princeton ;Andy Zeng, Princeton;Angel Chang, Stanford University;Manolis Savva, ;Silvio Savarese, ;Thomas Funkhouser, Princeton;

803
Title: Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View
Type: Poster
Authors: Shuran Song, Princeton ;Andy Zeng, Princeton;Angel Chang, Stanford University;Manolis Savva, ;Silvio Savarese, ;Thomas Funkhouser, Princeton;

804
Title: Preserving Semantic Relations for Zero-Shot Learning
Type: Poster
Abstracts: Zero-shot learning has gained popularity due to its potential to scale recognition models without requiring additional training data. This is usually achieved by associating categories with their semantic information like attributes. However, we believe that the potential offered by this paradigm is not yet fully exploited. In this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. We devise objective functions to preserve these relations in the embedding space, thereby inducing semanticity to the embedding space. Through extensive experimental evaluation on five benchmark datasets, we demonstrate that inducing semanticity to the embedding space is beneficial for zero-shot learning. The proposed approach outperforms the state-of-the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. We also demonstrate how the proposed approach can be useful for making approximate semantic inferences about an image belonging to a category for which attribute information is not available.
Authors: Yashas Annadani, NITK;Soma Biswas, Indian Institute of Science;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Annadani_Preserving_Semantic_Relations_CVPR_2018_paper.pdf


805
Title: What have we learned from deep representations for action recognition?
Type: Poster
Authors: Christoph Feichtenhofer, ;Axel Pinz, Graz University of Technology;Richard Wildes, York University;Andrew Zisserman, Oxford;

806
Title: AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation
Type: Spotlight
Abstracts: Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.
Authors: Jogendra Kundu, Indian Institute of Science;Phani Krishna Uppala, Indian Institute of Science;Anuj Pahuja, Indian Institute of Science;Venkatesh Babu Radhakrishnan, Indian Institute of Science;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.pdf


807
Title: AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation
Type: Poster
Abstracts: Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.
Authors: Jogendra Kundu, Indian Institute of Science;Phani Krishna Uppala, Indian Institute of Science;Anuj Pahuja, Indian Institute of Science;Venkatesh Babu Radhakrishnan, Indian Institute of Science;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.pdf


808
Title: Neural Style Transfer via Meta Networks
Type: Poster
Abstracts: In this paper we propose a noval method to generate the specified network parameters through one feed-forward propagation in the meta networks for neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent, which lacks the generalization ability to new style in the inference stage. To tackle these issues, we build a meta network which takes in the style image and generates a corresponding image transformation network directly. Compared with optimization-based methods for every style, our meta networks can handle an arbitrary new style within 19 milliseconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449 KB, which is capable of real-time running on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models will be released.
Authors: Falong Shen, Peking University;Shuicheng Yan, ;Gang Zeng, Peking University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Neural_Style_Transfer_CVPR_2018_paper.pdf


809
Title: FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis
Type: Spotlight
Abstracts: Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.
Authors: Nitika Verma, INRIA;Edmond Boyer, ;Jakob Verbeek,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Verma_FeaStNet_Feature-Steered_Graph_CVPR_2018_paper.pdf


810
Title: FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis
Type: Poster
Abstracts: Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.
Authors: Nitika Verma, INRIA;Edmond Boyer, ;Jakob Verbeek,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Verma_FeaStNet_Feature-Steered_Graph_CVPR_2018_paper.pdf


811
Title: InverseFaceNet: Deep Monocular Inverse Face Rendering at over 250 Hz
Type: Poster
Authors: Hyeongwoo Kim, MPII;Michael Zollh?fer, MPI Informatics;Ayush Tewari, MPI Informatics;Justus Thies, Technical University of Munich;Christian Richardt, University of Bath;Christian Theobalt, MPI Informatics;

812
Title: People
Type: Poster
Authors: Mark Marsden, Dublin City University;Kevin McGuinness, DCU ;Suzanne Little, DCU;University College Dublin, Ireland;Noel O'Connor, DCU;

813
Title: Multi-Frame Quality Enhancement for Compressed Video
Type: Poster
Abstracts: The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at https://github.com/ryangBUAA/MFQE.git.
Authors: Ren Yang, Beihang University;Mai Xu, Beihang University;Zulin Wang, Beihang University;Tianyi Li, Beihang University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Multi-Frame_Quality_Enhancement_CVPR_2018_paper.pdf


814
Title: Cascade R-CNN: Delving into High Quality Object Detection
Type: Spotlight
Authors: Zhaowei Cai, UC San Diego;UCSD, USA;

815
Title: Cascade R-CNN: Delving into High Quality Object Detection
Type: Poster
Authors: Zhaowei Cai, UC San Diego;UCSD, USA;

816
Title: DiverseNet: When One Right Answer Is Not Enough
Type: Poster
Abstracts: Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy. We introduce a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. Our best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. We demonstrate that our method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction.
Authors: Michael Firman, UCL;Neill Campbell, University of bath;Lourdes Agapito, University College London;Gabriel Brostow, University College London UK;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Firman_DiverseNet_When_One_CVPR_2018_paper.pdf


817
Title: Beyond the Pixel-Wise Loss for Topology-Aware Delineation
Type: Poster
Abstracts: Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological importance of prediction errors. Instead, we propose a new loss term that is aware of the higher-order topological features of the linear structures. We also introduce a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step while keeping the number of parameters and the complexity of the model constant. When combined with the standard pixel-wise loss, both our new loss term and iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained only with the binary cross-entropy. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.
Authors: Agata Mosinska, EPFL;Pablo Marquez Neila, EPFL;Mateusz Kozinski, ;Pascal Fua,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.pdf


818
Title: Polarimetric Dense Monocular SLAM
Type: Oral
Abstracts: This paper presents a novel polarimetric dense monocular SLAM (PDMS) algorithm based on a polarization camera. The algorithm exploits both photometric and polarimetric light information to produce more accurate and complete geometry. The polarimetric information allows us to recover the azimuth angle of surface normals from each video frame to facilitate dense reconstruction, especially at textureless or specular regions. There are two challenges in our approach: 1) surface azimuth angles from the polarization camera are very noisy; and 2) we need a near real-time solution for SLAM. Previous successful methods on polarimetric multi-view stereo are offline and require manually pre-segmented object masks to suppress the effects of erroneous angle information along boundaries. Our fully automatic approach efficiently iterates azimuth-based depth propagations, two-view depth consistency check, and depth optimization to produce a depthmap in real-time, where all the algorithmic steps are carefully designed to enable a GPU implementation. To our knowledge, this paper is the first to propose a photometric and polarimetric method for dense SLAM. We have qualitatively and quantitatively evaluated our algorithm against a few of competing methods, demonstrating the superior performance on various indoor and outdoor scenes.
Authors: Luwei Yang, Simon Farser University;Feitong Tan, Simon Fraser University;Ao Li, Simon Fraser University;Zhaopeng Cui, Simon Fraser University;Yasutaka Furukawa, ;Ping Tan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Polarimetric_Dense_Monocular_CVPR_2018_paper.pdf


819
Title: Polarimetric Dense Monocular SLAM
Type: Poster
Abstracts: This paper presents a novel polarimetric dense monocular SLAM (PDMS) algorithm based on a polarization camera. The algorithm exploits both photometric and polarimetric light information to produce more accurate and complete geometry. The polarimetric information allows us to recover the azimuth angle of surface normals from each video frame to facilitate dense reconstruction, especially at textureless or specular regions. There are two challenges in our approach: 1) surface azimuth angles from the polarization camera are very noisy; and 2) we need a near real-time solution for SLAM. Previous successful methods on polarimetric multi-view stereo are offline and require manually pre-segmented object masks to suppress the effects of erroneous angle information along boundaries. Our fully automatic approach efficiently iterates azimuth-based depth propagations, two-view depth consistency check, and depth optimization to produce a depthmap in real-time, where all the algorithmic steps are carefully designed to enable a GPU implementation. To our knowledge, this paper is the first to propose a photometric and polarimetric method for dense SLAM. We have qualitatively and quantitatively evaluated our algorithm against a few of competing methods, demonstrating the superior performance on various indoor and outdoor scenes.
Authors: Luwei Yang, Simon Farser University;Feitong Tan, Simon Fraser University;Ao Li, Simon Fraser University;Zhaopeng Cui, Simon Fraser University;Yasutaka Furukawa, ;Ping Tan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Polarimetric_Dense_Monocular_CVPR_2018_paper.pdf


820
Title: A Perceptual Measure for Deep Single Image Camera Calibration
Type: Poster
Abstracts: Most current single image camera calibration methods rely on specific image features or user input, and cannot be applied to natural images captured in uncontrolled settings. We propose inferring directly camera calibration parameters from a single image using a deep convolutional neural network. This network is trained using automatically generated samples from a large-scale panorama dataset, and considerably outperforms other methods, including recent deep learning-based approaches, in terms of standard L2 error. However, we argue that in many cases it is more important to consider how humans perceive errors in camera estimation. To this end, we conduct a large-scale human perception study where we ask users to judge the realism of 3D objects composited with and without ground truth camera calibration. Based on this study, we develop a new perceptual measure for camera calibration, and demonstrate that our deep calibration network outperforms other methods on this measure. Finally, we demonstrate the use of our calibration network for a number of applications including virtual object insertion, image retrieval and compositing.
Authors: Yannick Hold-Geoffroy, Universit? Laval;Kalyan Sunkavalli, Adobe Systems Inc.;Jonathan Eisenmann, Adobe Systems;Matthew Fisher, Adobe;Emiliano Gambaretto, Adobe Systems;Sunil Hadap, ;Jean-Francois Lalonde, Laval University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hold-Geoffroy_A_Perceptual_Measure_CVPR_2018_paper.pdf


821
Title: Show Me a Story: Towards Coherent Neural Story Illustration
Type: Poster
Abstracts: We propose an end-to-end network for the visual illustration of a sequence of sentences forming a story. At the core of our model is the ability to model the inter-related nature of the sentences within a story, as well as the ability to learn coherence to support reference resolution. The framework takes the form of an encoder-decoder architecture, where sentences are encoded using a hierarchical two-level sentence-story GRU, combined with an encoding of coherence, and sequentially decoded using predicted feature representation into a consistent illustrative image sequence. We optimize all parameters of our network in an end-to-end fashion with respect to order embedding loss, encoding entailment between images and sentences. Experiments on the VIST storytelling dataset cite{vist} highlight the importance of our algorithmic choices and efficacy of our overall model.
Authors: Hareesh Ravi, Rutgers University;Lezi Wang, Rutgers;Carlos Muniz, Rutgers University;Leonid Sigal, University of British Columbia;Mubbasir Kapadia, Rutgers University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ravi_Show_Me_a_CVPR_2018_paper.pdf


822
Title: Towards Universal Representation for Unseen Action Recognition
Type: Poster
Abstracts: Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover "building-blocks" from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.
Authors: Yi Zhu, University of California Merced;Yang Long, Newcastle University;Yu Guan, Newcastle University;Shawn Newsam, ;Ling Shao, University of East Anglia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_Universal_Representation_CVPR_2018_paper.pdf


823
Title: A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects
Type: Poster
Abstracts: Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causal-effect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions.
Authors: University of California, Los Angeles;Institute of Computing Technology, Chinese Academy of Sciences;Xiaobai Liu, San Diego State University;Song-Chun Zhu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_A_Causal_And-Or_CVPR_2018_paper.pdf


824
Title: LEGO: Learning Edge with Geometry all at Once by Watching Videos
Type: Spotlight
Authors: Zhenheng Yang, ;Peng Wang, Baidu;Yang Wang, Baidu USA;Wei Xu, ;Ram Nevatia,;

825
Title: LEGO: Learning Edge with Geometry all at Once by Watching Videos
Type: Poster
Authors: Zhenheng Yang, ;Peng Wang, Baidu;Yang Wang, Baidu USA;Wei Xu, ;Ram Nevatia,;

826
Title: Iterative Learning with Open-set Noisy Labels
Type: Spotlight
Authors: Yisen Wang, Tsinghua University;Xingjun Ma, The University of Melbourne;Weiyang Liu, Georgia Tech;James Bailey, The University of Melbourne;Hongyuan Zha, Georgia Institute of Technology;Le Song, Georgia Institute of Technology;Shu-Tao Xia, Tsinghua University;

827
Title: Iterative Learning with Open-set Noisy Labels
Type: Poster
Authors: Yisen Wang, Tsinghua University;Xingjun Ma, The University of Melbourne;Weiyang Liu, Georgia Tech;James Bailey, The University of Melbourne;Hongyuan Zha, Georgia Institute of Technology;Le Song, Georgia Institute of Technology;Shu-Tao Xia, Tsinghua University;

828
Title: Sparse Photometric 3D Face Reconstruction Guided by Morphable Models
Type: Poster
Abstracts: We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and latest advances on face registration / modeling from a single image. We observe that 3D morphable faces approach provides a reasonable geometry proxy for light position calibration. Specifically, we develop a robust optimization technique that can calibrate per-pixel lighting direction and illumination at a very high precision without assuming uniform surface albedos. Next, we apply semantic segmentation on input images and the geometry proxy to refine hairy vs. bare skin regions using tailored filter. Experiments on synthetic and real data show that by using a very small set of images, our technique is able to reconstruct fine geometric details such as wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing movie quality productions.
Authors: Xuan Cao, ShanghaiTech University;Zhang Chen, ShanghaiTech University;jingyi Yu, Shanghai Tech University;Anpei Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Sparse_Photometric_3D_CVPR_2018_paper.pdf


829
Title: Deep Adversarial Subspace Clustering
Type: Poster
Abstracts: Most existing subspace clustering methods hinge on self-expression of handcrafted representations and are unaware of potential clustering errors. Thus they perform unsatisfactorily on real data with complex underlying subspaces. To solve this issue, we propose a novel deep adversarial subspace clustering (DASC) model, which learns more favorable sample representations by deep learning for subspace clustering, and more importantly introduces adversarial learning to supervise sample representation learning and subspace clustering. Specifically, DASC consists of a subspace clustering generator and a quality-verifying discriminator, which learn against each other. The generator produces subspace estimation and sample clustering. The discriminator evaluates current clustering performance by inspecting whether the re-sampled data from estimated subspaces have consistent subspace properties, and supervises the generator to progressively improve subspace clustering. Experimental results on the handwritten recognition, face and object clustering tasks demonstrate the advantages of DASC over shallow and few deep subspace clustering models. Moreover, to our best knowledge, this is the first successful application of GAN-alike model for unsupervised subspace clustering, which also paves the way for deep learning to solve other unsupervised learning problems.
Authors: Pan Zhou, National university of singapo;Yunqing Hou, NUS;Jiashi Feng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Deep_Adversarial_Subspace_CVPR_2018_paper.pdf


830
Title: Multimodal Visual Concept Learning with Weakly Supervised Techniques
Type: Poster
Authors: Giorgos Bouritsas, NTUA;Petros Koutras, NTUA;Athanasia Zlatintsi, NTUA;Petros Maragos, NTUA;

831
Title: ICE-BA: Efficient
Type: Poster
Authors: Haomin Liu, Baidu;Mingyu Chen, Baidu;Guofeng Zhang, Zhejiang University;Hujun Bao, Zhejiang University;Yingze Bao, Baidu LLC;

832
Title: KIPPI: KInetic Polygonal Partitioning of Images
Type: Poster
Abstracts: Recent works showed that floating polygons can be an interesting alternative to traditional superpixels, especially for analyzing scenes with strong geometric signatures, as man-made environments. Existing algorithms produce homogeneously-sized polygons that fail to capture thin geometric structures and over-partition large uniform areas. We propose a kinetic approach that brings more flexibility on polygon shape and size. The key idea consists in progressively extending pre-detected line-segments until they meet each other. Our experiments demonstrate that output partitions both contain less polygons and better capture geometric structures than those delivered by existing methods. We also show the applicative potential of the method when used as preprocessing in object contouring.
Authors: Jean-Philippe Bauchet, Inria;Florent Lafarge,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bauchet_KIPPI_KInetic_Polygonal_CVPR_2018_paper.pdf


833
Title: Planar Shape Detection at Structural Scales
Type: Poster
Abstracts: Interpreting 3D data such as point clouds or surface meshes depends heavily on the scale of observation. Yet, existing algorithms for shape detection rely on trial-and-error parameter tunings to output configurations representative of a structural scale. We present a framework to automatically extract a set of representations that capture the shape and structure of man-made objects at different key abstraction levels. A shape-collapsing process first generates a fine-to-coarse sequence of shape representations by exploiting local planarity. This sequence is then analyzed to identify significant geometric variations between successive representations through a supervised energy minimization. Our framework is flexible enough to learn how to detect both existing structural formalisms such as the CityGML Levels Of Details, and expert-specified levels of abstraction. Experiments on different input data and classes of man-made objects, as well as comparisons with existing shape detection methods, illustrate the strengths of our approach in terms of efficiency and flexibility.
Authors: Hao Fang, Inria;Florent Lafarge, ;Mathieu Desbrun, Caltech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Planar_Shape_Detection_CVPR_2018_paper.pdf


834
Title: A Closer Look at Spatiotemporal Convolutions for Action Recognition
Type: Poster
Abstracts: In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ``R(2+1)D'' which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.
Authors: Du Tran, Dartmouth College;heng Wang, ;Darthmout College, USA;Jamie Ray, Facebook;Manohar Paluri,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf


835
Title: Wasserstein Introspective Neural Networks
Type: Oral
Abstracts: We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.
Authors: Kwonjoon Lee, UC San Diego;Weijian Xu, UC San Diego;Fan Fan, UC San Diego;UCSD, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.pdf


836
Title: Wasserstein Introspective Neural Networks
Type: Poster
Abstracts: We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.
Authors: Kwonjoon Lee, UC San Diego;Weijian Xu, UC San Diego;Fan Fan, UC San Diego;UCSD, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.pdf


837
Title: Learning Globally Optimized Object Detector via Policy Gradient
Type: Spotlight
Abstracts: In this paper, we propose a simple yet effective method to learn globally optimized detector for object detection, which is a simple modification to the standard cross-entropy gradient inspired by the REINFORCE algorithm. In our approach, the cross-entropy gradient is adaptively adjusted according to overall mean Average Precision (mAP) of the current state for each detection candidate, which leads to more effective gradient and global optimization of detection results, and brings no computational overhead. Benefiting from more precise gradients produced by the global optimization method, our framework significantly improves state-of-the-art object detectors. Furthermore, since our method is based on scores and bounding boxes without modification on the architecture of object detector, it can be easily applied to off-the-shelf modern object detection frameworks.
Authors: Yongming Rao, ;Dahua Lin, CUHK;Jiwen Lu, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Rao_Learning_Globally_Optimized_CVPR_2018_paper.pdf


838
Title: Learning Globally Optimized Object Detector via Policy Gradient
Type: Poster
Abstracts: In this paper, we propose a simple yet effective method to learn globally optimized detector for object detection, which is a simple modification to the standard cross-entropy gradient inspired by the REINFORCE algorithm. In our approach, the cross-entropy gradient is adaptively adjusted according to overall mean Average Precision (mAP) of the current state for each detection candidate, which leads to more effective gradient and global optimization of detection results, and brings no computational overhead. Benefiting from more precise gradients produced by the global optimization method, our framework significantly improves state-of-the-art object detectors. Furthermore, since our method is based on scores and bounding boxes without modification on the architecture of object detector, it can be easily applied to off-the-shelf modern object detection frameworks.
Authors: Yongming Rao, ;Dahua Lin, CUHK;Jiwen Lu, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Rao_Learning_Globally_Optimized_CVPR_2018_paper.pdf


839
Title: Reconstruction Network for Video Captioning
Type: Poster
Abstracts: In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by encoder-decoder and the reconstruction loss introduced by reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor could boost the encoder-decoder models and leads to significant gains on video caption accuracy.
Authors: Bairui Wang, ;Lin Ma, Tencent AI Lab;Wei Zhang, ;Wei Liu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Reconstruction_Network_for_CVPR_2018_paper.pdf


840
Title: DOTA: A Large-scale Dataset for Object Detection in Aerial Images
Type: Poster
Authors: Gui-Song Xia, Wuhan University;Xiang Bai, Huazhong University of Science and Technology;Jian Ding, Wuhan University;Zhen Zhu, Huazhong University of Science and Technology;Serge Belongie, ;Jiebo Luo, University of Rochester;Mihai Datcu, ;Marcello Pelillo, University of Venice;Liangpei Zhang, Wuhan University;

841
Title: Person Transfer GAN to Bridge Domain Gap for Person Re-Identification
Type: Spotlight
Abstracts: Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.
Authors: Longhui Wei, Peking University;Shiliang Zhang, Peking University;Wen Gao, ;Qi Tian,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Person_Transfer_GAN_CVPR_2018_paper.pdf


842
Title: Person Transfer GAN to Bridge Domain Gap for Person Re-Identification
Type: Poster
Abstracts: Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.
Authors: Longhui Wei, Peking University;Shiliang Zhang, Peking University;Wen Gao, ;Qi Tian,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Person_Transfer_GAN_CVPR_2018_paper.pdf


843
Title: Tight Nonconvex Relaxation of MAP Inference
Type: Poster
Authors: Inria & CentraleSup?lec, Universit? Paris-Saclay;Nikos Paragios, Ecole Centrale de Paris;

844
Title: Weakly Supervised Phrase Localization with Multi-Scale Anchored Transformer Network
Type: Poster
Authors: Fang Zhao, National University of Singapore;Jianshu Li, National University of Singapo;Jian Zhao, NUS;Jiashi Feng,;

845
Title: Variational Autoencoders for Deforming 3D Mesh Models
Type: Poster
Abstracts: 3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are ﬂexible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows ﬂexibly adjusting the signiﬁcance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods.
Authors: Qingyang Tan, UCAS;Lin Gao, Chinese Academy of Sciences;Yu-Kun Lai, Cardiff University;Beijing, China;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tan_Variational_Autoencoders_for_CVPR_2018_paper.pdf


846
Title: DeepMVS: Learning Multi-View Stereopsis
Type: Poster
Abstracts: We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.
Authors: University of Illinois, U-C;Kevin Matzen, Facebook;Johannes Kopf, Facebook;University of Illinois at Urbana-Champaign, USA;Jia-Bin Huang, Virginia Tech;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.pdf


847
Title: HydraNets: Specialized Dynamic Architectures for Efficient Inference
Type: Poster
Abstracts: There is growing interest in improving the design of deep network architectures to be both accurate and low cost. This paper explores semantic specialization as a mechanism for improving the computational efficiency (accuracy-per-unit-cost) of inference in the context of image classification. Specifically, we propose a network architecture template called HydraNet, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image. This design is made possible by a soft gating mechanism that encourages component specialization during training and accurately performs component selection during inference. We evaluate the HydraNet approach on both the CIFAR-100 and ImageNet classification tasks. On CIFAR, applying the HydraNet template to the ResNet and DenseNet family of models reduces inference cost by 2-4x while retaining the accuracy of the baseline architectures. On ImageNet, applying the HydraNet template improves accuracy up to 2.5% when compared to an efficient baseline architecture with similar inference cost.
Authors: Ravi Teja Mullapudi, Carnegie Mellon University;Noam Shazeer, Google;William Mark, Google;Kayvon Fatahalian, Stanford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf


848
Title: Multimodal Explanations: Justifying Decisions and Pointing to the Evidence
Type: Spotlight
Abstracts: Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.
Authors: Lisa Anne Hendricks, UC Berkeley;UC Berkeley, USA;Anna Rohrbach, UC Berkeley;Zeynep Akata, University of Amsterdam;Bernt Schiele, MPI Informatics Germany;Marcus Rohrbach, UC Berkeley;Dong Huk Park, UC Berkeley;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.pdf


849
Title: Multimodal Explanations: Justifying Decisions and Pointing to the Evidence
Type: Poster
Abstracts: Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.
Authors: Lisa Anne Hendricks, UC Berkeley;UC Berkeley, USA;Anna Rohrbach, UC Berkeley;Zeynep Akata, University of Amsterdam;Bernt Schiele, MPI Informatics Germany;Marcus Rohrbach, UC Berkeley;Dong Huk Park, UC Berkeley;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.pdf


850
Title: Feature Generating Networks for Zero-Shot Learning
Type: Poster
Abstracts: Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network(GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet -- in both the zero-shot learning and generalized zero-shot learning settings.
Authors: Yongqin Xian, Max Planck Institute;Tobias Lorenz, Max Planck Institute for Informatics;Bernt Schiele, MPI Informatics Germany;Zeynep Akata, University of Amsterdam;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_Feature_Generating_Networks_CVPR_2018_paper.pdf


851
Title: Deep Image Prior
Type: Poster
Abstracts: Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity.
Authors: Dmitry Ulyanov, Skoltech;Andrea Vedaldi, U Oxford;Victor Lempitsky,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.pdf


852
Title: Pix3D: Dataset and Methods for 3D Object Modeling from a Single Image
Type: Poster
Authors: Xingyuan Sun, Shanghai Jiao Tong University;Jiajun Wu, MIT;Xiuming Zhang, MIT;Zhoutong Zhang, MIT;Tianfan Xue, Google;Joshua Tenenbaum, ;William Freeman, MIT/Google;

853
Title: Defense against Universal Adversarial Perturbations
Type: Poster
Authors: NAVEED AKHTAR, UNIVERSITY OF WESTERN AUSTRALI;Jian Liu, UWA;Ajmal Mian, UWA;

854
Title: Structure from Recurrent Motion: From Rigidity to Recurrency
Type: Poster
Authors: Xiu Li, Tsinghua University;Hongdong Li, Australian National University;Hanbyul Joo, CMU;Yebin Liu, Tsinghua University;Yaser Sheikh,;

855
Title: Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN
Type: Spotlight
Authors: Deepak Babu Sam, Indian Institute of Science;Neeraj Sajjan, Indian Institute of Science;Venkatesh Babu Radhakrishnan, Indian Institute of Science;Mukundhan Srinivasan, NVIDIA;

856
Title: Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN
Type: Poster
Authors: Deepak Babu Sam, Indian Institute of Science;Neeraj Sajjan, Indian Institute of Science;Venkatesh Babu Radhakrishnan, Indian Institute of Science;Mukundhan Srinivasan, NVIDIA;

857
Title: Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking
Type: Poster
Abstracts: In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected. An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.
Authors: Filip Radenovic, CTU Prague;Ahmet Iscen, Inria;Giorgos Tolias, Czech Technical University in Prague;Yannis Avrithis, Inria;Ondrej Chum, Czech Technical University in Prague;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.pdf


858
Title: Structured Set Matching Networks for One-Shot Part Labeling
Type: Spotlight
Abstracts: Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets.
Authors: Jonghyun Choi, ;Jayant Krishnamurthy, Semantic Machines;Aniruddha Kembhavi, Allen Institute for Artificial Intelligence;Ali Farhadi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Structured_Set_Matching_CVPR_2018_paper.pdf


859
Title: Structured Set Matching Networks for One-Shot Part Labeling
Type: Poster
Abstracts: Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets.
Authors: Jonghyun Choi, ;Jayant Krishnamurthy, Semantic Machines;Aniruddha Kembhavi, Allen Institute for Artificial Intelligence;Ali Farhadi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Structured_Set_Matching_CVPR_2018_paper.pdf


860
Title: DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation
Type: Poster
Abstracts: In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.
Authors: Jiang Liu, Carnegie Mellon University;Chenqiang Gao, Chongqing University of Posts and Telecommunications;Deyu Meng, Xi'an Jiaotong University;Alexander Hauptmann,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.pdf


861
Title: Deeply Learned Filter Response Functions for Hyperspectral Reconstruction
Type: Poster
Abstracts: Hyperspectral reconstruction from RGB imaging has recently achieved significant progress via sparse coding and deep learning. However, a largely ignored fact is that existing RGB cameras are tuned to mimic human richromatic perception, thus their spectral responses are not necessarily optimal for hyperspectral reconstruction. In this paper, rather than use RGB spectral responses, we simultaneously learn optimized camera spectral response functions (to be implemented in hardware) and a mapping for spectral reconstruction by using an end-to-end network. Our core idea is that since camera spectral filters act in effect like the convolution layer, their response functions could be optimized by training standard neural networks. We propose two types of designed filters: a three-chip setup without spatial mosaicing and a single-chip setup with a Bayer-style 2x2 filter array. Numerical simulations verify the advantages of deeply learned spectral responses compared to existing RGB cameras. More interestingly, by considering physical restrictions in the design process, we are able to realize the deeply learned spectral response functions by using modern film filter production technologies, and thus construct data-inspired multispectral cameras for snapshot hyperspectral imaging.
Authors: NII, Japan;Lin Gu, National Institute of Informatics;National Institute of Informatics, Japan;Antony Lam, Saitama University;Nobutaka Ono, Tokyo Metropolitan University;National Institute of Informatics, Japan;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Nie_Deeply_Learned_Filter_CVPR_2018_paper.pdf


862
Title: Learning Strict Identity Mappings in Deep Residual Networks
Type: Poster
Abstracts: A family of super deep networks, referred to as residual networks or ResNet~cite{he2016deep}, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose $epsilon$-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold $epsilon$, without any loss in performance. The $epsilon$-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters.
Authors: Xin Yu, University of Utah;Srikumar Ramalingam, ;Zhiding Yu, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_Strict_Identity_CVPR_2018_paper.pdf


863
Title: Face Detector Adaptation without Negative Transfer or Catastrophic Forgetting
Type: Poster
Authors: Muhammad Abdullah Jamal, University of Central Florida;Haoxiang Li, Adobe Research;Boqing Gong, University of Central Florida;

864
Title: Multi-Evidence Fusion and Filtering for Weakly Supervised Object Recognition
Type: Poster
Authors: Weifeng Ge, The University of Hong Kong;Yizhou Yu, The University of Hong Kong;

865
Title: SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval
Type: Poster
Abstracts: We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset.Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval.Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset,we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition.Such superior performances effectively demonstrate the benefit of our sketch-specific design.
Authors: Peng Xu, Beijing University of Posts an;Yongye Huang, Beijing University of Posts and Telecommunications;Tongtong Yuan, Beijing University of Posts and Telecommunications;Kaiyue Pang, QMUL;Yi-Zhe Song, ;Tao Xiang, Queen Mary University of London;Timothy Hospedales, University of Edinburgh;Zhanyu Ma, Beijing University of Posts and Telecommunications ;Jun Guo, Beijing University of Posts and Telecommunications;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.pdf


866
Title: Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams
Type: Poster
Authors: Daesik Kim, Seoul National University;YoungJoon Yoo, ;JeeSoo Kim, Seoul national university;SangKuk Lee, Seoul National University;Nojun Kwak, Seoul National University;

867
Title: The Perception-Distortion Tradeoff
Type: Oral
Abstracts: Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.
Authors: Yochai Blau, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.pdf


868
Title: The Perception-Distortion Tradeoff
Type: Poster
Abstracts: Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.
Authors: Yochai Blau, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.pdf


869
Title: Jerk-Aware Video Acceleration Magnification
Type: Poster
Abstracts: Video magnification reveals subtle changes invisible to the naked eye, but such tiny yet meaningful changes are often hidden under large motions: small deformation of the muscles in doing sports, or tiny vibrations of strings in ukulele playing. For magnifying subtle changes under large motions, video acceleration magnification method has recently been proposed. This method magnifies subtle acceleration changes and ignores slow large motions. However, quick large motions severely distort this method. In this paper, we present a novel use of jerk to make the acceleration method robust to quick large motions. Jerk has been used to assess smoothness of time series data in the neuroscience and mechanical engineering fields. On the basis of our observation that subtle changes are smoother than quick large motions at temporal scale, we used jerk-based smoothness to design a jerk-aware filter that passes subtle changes only under quick large motions. By applying our filter to the acceleration method, we obtain impressive magnification results better than those obtained with state-of-the-art.
Authors: Shoichiro Takeda, NTT Media Intelligence Lab.;Kazuki Okami, NTT Media Intelligence Lab.;Dan Mikami, NTT Media Intelligence Lab.;Megumi Isogai, NTT Media Intelligence Lab.;Hideaki Kimata, NTT Media Intelligence Lab.;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Takeda_Jerk-Aware_Video_Acceleration_CVPR_2018_paper.pdf


870
Title: Video Based Reconstruction of 3D People Models
Type: Spotlight
Abstracts: This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.
Authors: Thiemo Alldieck, TU Braunschweig;Marcus Magnor, TU Braunschweig;Weipeng Xu, MPI Informatics;Christian Theobalt, MPI Informatics;Gerard Pons-Moll, Max Planck for Informatics;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Alldieck_Video_Based_Reconstruction_CVPR_2018_paper.pdf


871
Title: Video Based Reconstruction of 3D People Models
Type: Poster
Abstracts: This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.
Authors: Thiemo Alldieck, TU Braunschweig;Marcus Magnor, TU Braunschweig;Weipeng Xu, MPI Informatics;Christian Theobalt, MPI Informatics;Gerard Pons-Moll, Max Planck for Informatics;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Alldieck_Video_Based_Reconstruction_CVPR_2018_paper.pdf


872
Title: Appearance-and-Relation Networks for Video Classification
Type: Poster
Abstracts: Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods.
Authors: Limin Wang, ETH Zurich;Wei Li, Google;Wen Li, ETH;Luc Van Gool, KTH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.pdf


873
Title: Fast Spectral Ranking for Similarity Search
Type: Poster
Abstracts: Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.
Authors: Ahmet Iscen, Inria;Yannis Avrithis, Inria;Giorgos Tolias, Czech Technical University in Prague;Teddy Furon, ;Ondrej Chum, Czech Technical University in Prague;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Iscen_Fast_Spectral_Ranking_CVPR_2018_paper.pdf


874
Title: Mining on Manifolds: Metric Learning without Labels
Type: Poster
Authors: Ahmet Iscen, Inria;Giorgos Tolias, Czech Technical University in Prague;Yannis Avrithis, Inria;Ondrej Chum, Czech Technical University in Prague;

875
Title: From source to target and back: Symmetric Bi-Directional Adaptive GAN
Type: Poster
Authors: Paolo Russo, University of Rome La Sapienza;Fabio Carlucci, University of Rome La Sapienza;Tatiana Tommasi, Italian Institute of Tecnology;University of Rome La Sapienza, Italy;

876
Title: Path Aggregation Network for Instance Segmentation
Type: Spotlight
Abstracts: The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes.
Authors: Shu Liu, CUHK;Lu Qi, CUHK;Haifang Qin, ;Jianping Shi, SenseTime;Jiaya Jia, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf


877
Title: Path Aggregation Network for Instance Segmentation
Type: Poster
Abstracts: The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes.
Authors: Shu Liu, CUHK;Lu Qi, CUHK;Haifang Qin, ;Jianping Shi, SenseTime;Jiaya Jia, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf


878
Title: Referring Image Segmentation via Recurrent Refinement Networks
Type: Poster
Abstracts: We address the problem of image segmentation from natural language descriptions. Existing deep learning-based methods encode image representations based on the output of the last convolutional layer. One general issue is that the resulting image representation lacks multi-scale semantics, which are key components in advanced segmentation systems. In this paper, we utilize the feature pyramids inherently existing in convolutional neural networks to capture the semantics at different scales. To produce suitable information flow through the path of feature hierarchy, we propose Recurrent Refinement Network (RRN) that takes pyramidal features as input to refine the segmentation mask progressively. Experimental results on four available datasets show that our approach outperforms multiple baselines and state-of-the-art.
Authors: Ruiyu Li, CUHK;Kaican Li, CUHK;Yi-Chun Kuo, CUHK;Michelle Shu, ;Xiaojuan Qi, CUHK;Xiaoyong Shen, CUHK;Jiaya Jia, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Referring_Image_Segmentation_CVPR_2018_paper.pdf


879
Title: Defense against adversarial attacks using guided denoiser
Type: Poster
Authors: Fangzhou Liao, Tsinghua University;Ming Liang, ;Yinpeng Dong, Tsinghua Univeristy;Tianyu Pang, Tsinghua University;Jun Zhu, Tsinghua University;Xiaolin Hu, Tsinghua University;

880
Title: Neural 3D Mesh Renderer
Type: Spotlight
Abstracts: For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.
Authors: Hiroharu Kato, Univ. Tokyo;Tatsuya Harada, University of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kato_Neural_3D_Mesh_CVPR_2018_paper.pdf


881
Title: Neural 3D Mesh Renderer
Type: Poster
Abstracts: For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.
Authors: Hiroharu Kato, Univ. Tokyo;Tatsuya Harada, University of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kato_Neural_3D_Mesh_CVPR_2018_paper.pdf


882
Title: Disentangling Factors of Variation by Mixing Them
Type: Poster
Abstracts: We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets.
Authors: Qiyang HU, University of bern;Attila Szabo, University of Bern;Tiziano Portenier, ;Matthias Zwicker, ;Bern University, Switzerland;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Disentangling_Factors_of_CVPR_2018_paper.pdf


883
Title: LSTM Pose Machines
Type: Poster
Abstracts: We observed that recent state-of-the-art results on single image human pose estimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations.
Authors: Yue Luo, SenseTime;Jimmy Ren, SenseTime Group Limited;Zhouxia Wang, SenseTime;Wenxiu Sun, SenseTime Group Limited;Jinshan Pan, UC Merced;Jianbo Liu, SenseTime;Jiahao Pang, SenseTime Group Limited;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_LSTM_Pose_Machines_CVPR_2018_paper.pdf


884
Title: CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition
Type: Poster
Authors: Anil Baslamisli, University of Amsterdam;Hoang-An Le, University of Amsterdam;Theo Gevers, University of Amsterdam;

885
Title: Learning Semantic Concepts and Order for Image and Sentence Matching
Type: Spotlight
Abstracts: Image and sentence matching has made great progress recently, but it remains challenging due to the large visual semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.
Authors: Yan Huang, ;Qi Wu, University of Adelaide;Liang Wang, unknown;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.pdf


886
Title: Learning Semantic Concepts and Order for Image and Sentence Matching
Type: Poster
Abstracts: Image and sentence matching has made great progress recently, but it remains challenging due to the large visual semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.
Authors: Yan Huang, ;Qi Wu, University of Adelaide;Liang Wang, unknown;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.pdf


887
Title: Modifying Non-Local Variations Across Multiple Views
Type: Spotlight
Abstracts: We present an algorithm for modifying small non-local variations between repeating structures and patterns in multiple images of the same scene. The modification is consistent across views, even-though the images could have been photographed from different view points and under different lighting conditions. We show that when modifying each image independently the correspondence between them breaks and the geometric structure of the scene gets distorted. Our approach modifies the views while maintaining correspondence, hence, we succeed in modifying appearance and structure variations consistently. We demonstrate our methods on a number of challenging examples, photographed in different lighting, scales and view points.
Authors: Tal Tlusty, Technion;Tomer Michaeli, Technion;Tali Dekel, Google;Lihi Zelnik-Manor,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tlusty_Modifying_Non-Local_Variations_CVPR_2018_paper.pdf


888
Title: Modifying Non-Local Variations Across Multiple Views
Type: Poster
Abstracts: We present an algorithm for modifying small non-local variations between repeating structures and patterns in multiple images of the same scene. The modification is consistent across views, even-though the images could have been photographed from different view points and under different lighting conditions. We show that when modifying each image independently the correspondence between them breaks and the geometric structure of the scene gets distorted. Our approach modifies the views while maintaining correspondence, hence, we succeed in modifying appearance and structure variations consistently. We demonstrate our methods on a number of challenging examples, photographed in different lighting, scales and view points.
Authors: Tal Tlusty, Technion;Tomer Michaeli, Technion;Tali Dekel, Google;Lihi Zelnik-Manor,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tlusty_Modifying_Non-Local_Variations_CVPR_2018_paper.pdf


889
Title: Discriminative Learning of Latent Features for Zero-Shot Recognition
Type: Oral
Abstracts: Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods.
Authors: Yan Li, CASIA;Junge Zhang, ;jianguo Zhang, ;Kaiqi Huang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Discriminative_Learning_of_CVPR_2018_paper.pdf


890
Title: Discriminative Learning of Latent Features for Zero-Shot Recognition
Type: Poster
Abstracts: Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods.
Authors: Yan Li, CASIA;Junge Zhang, ;jianguo Zhang, ;Kaiqi Huang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Discriminative_Learning_of_CVPR_2018_paper.pdf


891
Title: Learning Rich Features for Image Manipulation Detection
Type: Poster
Abstracts: Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it end-to- end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to find tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model filter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.
Authors: University of Maryland, Colleg;Xintong Han, University of Maryland;Vlad Morariu, University of Maryland;University of Maryland, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Learning_Rich_Features_CVPR_2018_paper.pdf


892
Title: GeoNet: Unsupervised Learning of Dense Depth
Type: Poster
Authors: Zhichao Yin, Sensetime Group Limited;Jianping Shi, SenseTime;

893
Title: Translating and Segmenting Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network
Type: Poster
Authors: Zizhao Zhang, University of Florida;Lin Yang, ;Yefeng Zheng, Simens;

894
Title: Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network
Type: Spotlight
Authors: Zizhao Zhang, University of Florida;Yuanpu Xie, University of Florida;Lin Yang,;

895
Title: Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network
Type: Poster
Authors: Zizhao Zhang, University of Florida;Yuanpu Xie, University of Florida;Lin Yang,;

896
Title: Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection
Type: Spotlight
Authors: David Novotny, Oxford University;Samuel Albanie, Oxford University;Diane Larlus, NAVER LABS Europe;Andrea Vedaldi, U Oxford;

897
Title: Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection
Type: Poster
Authors: David Novotny, Oxford University;Samuel Albanie, Oxford University;Diane Larlus, NAVER LABS Europe;Andrea Vedaldi, U Oxford;

898
Title: Human Semantic Parsing for Person Re-identification
Type: Poster
Authors: Mahdi Kalayeh, UCF;Emrah Basaran, ;Mubarak Shah, UCF;

899
Title: Ring loss: Convex Feature Normalization for Face Recognition
Type: Poster
Authors: Yutong Zheng, Carnegie Mellon University;Dipan Pal, Carnegie Mellon University;Marios Savvides,;

900
Title: Learned Shape-Tailored Descriptors for Segmentation
Type: Poster
Abstracts: We address the problem of texture segmentation by grouping dense pixel-wise descriptors. We introduce and construct learned Shape-Tailored Descriptors that aggregate image statistics only within regions of interest to avoid mixing statistics of different textures, and that are invariant to complex nuisances (e.g., illumination, perspective and deformations). This is accomplished by training a neural network to discriminate base shape-tailored descriptors of oriented gradients at various scales. These descriptors are defined through partial differential equations to obtain data at various scales in arbitrarily shaped regions. We formulate and optimize a joint optimization problem in the segmentation and descriptors to discriminate these base descriptors using the learned metric, equivalent to grouping learned descriptors. We test the method on datasets to illustrate the effect of both the shape-tailored and learned properties of the descriptors. Experiments show that the descriptors learned on a small dataset of segmented images generalize well to unseen textures in other datasets, showing the generic nature of these descriptors. We show stateof- the-art results on texture segmentation benchmarks.
Authors: Naeemullah Khan, KAUST;Ganesh Sundaramoorthi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Khan_Learned_Shape-Tailored_Descriptors_CVPR_2018_paper.pdf


901
Title: ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans
Type: Poster
Abstracts: We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.
Authors: Angela Dai, ;Daniel Ritchie, Brown University;Martin Bokeloh, Google;Scott Reed, Google;Juergen Sturm, Google;Matthias Nie��ner, Technical University of Munich;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper.pdf


902
Title: GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation
Type: Poster
Abstracts: In this paper, we propose Geometric Neural Network (GeoNet) to jointly predict depth and surface normal maps from a single image. Building on top of two-stream CNNs, our GeoNet incorporates geometric relation between depth and surface normal via the new depth-to-normal and normal- to-depth networks. Depth-to-normal network exploits the least square solution of surface normal from depth and im- proves its quality with a residual module. Normal-to-depth network, contrarily, refines the depth map based on the con- straints from the surface normal through a kernel regression module, which has no parameter to learn. These two net- works enforce the underlying model to efficiently predict depth and surface normal for high consistency and corre- sponding accuracy. Our experiments on NYU v2 dataset verify that our GeoNet is able to predict geometrically con- sistent depth and normal maps. It achieves top performance on surface normal estimation and is on par with state-of-the- art depth estimation methods.
Authors: Xiaojuan Qi, CUHK;Renjie Liao, ;Zhengzhe Liu, CUHK;Raquel Urtasun, University of Toronto;Jiaya Jia, Chinese University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_GeoNet_Geometric_Neural_CVPR_2018_paper.pdf


903
Title: Learning Compressible 360? Video Isomers
Type: Poster
Authors: Yu-Chuan Su, UT Austin;Kristen Grauman,;

904
Title: Geometric robustness of deep networks: analysis and improvement
Type: Poster
Authors: Can Kanbak, EPFL;Seyed-Mohsen Moosavi-Dezfooli, ;Pascal Frossard,;

905
Title: Weakly Supervised Facial Action Unit Recognition through Adversarial Training
Type: Poster
Authors: Guozhu Peng, USTC;Shangfei Wang,;

906
Title: Empirical study of the topology and geometry of deep networks
Type: Spotlight
Authors: Alhussein Fawzi, ;Seyed-Mohsen Moosavi-Dezfooli, ;Pascal Frossard, ;Stefano Soatto, UCLA;

907
Title: Empirical study of the topology and geometry of deep networks
Type: Poster
Authors: Alhussein Fawzi, ;Seyed-Mohsen Moosavi-Dezfooli, ;Pascal Frossard, ;Stefano Soatto, UCLA;

908
Title: Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition
Type: Poster
Abstracts: This paper proposes an encoder-decoder network to disentangle shape features during 3D face shape reconstruction from single 2D images, such that the tasks of learning discriminative shape features for face recognition and reconstructing accurate 3D face shapes can be done simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. We develop a multi image 3D morphable model (3DMM) fitting method for multiple 2D images of a subject to construct training data. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.
Authors: Feng Liu, Sichuan University;Dan Zeng, Sichuan University;Qijun Zhao, Sichuan University;Xiaoming Liu, Michigan State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Disentangling_Features_in_CVPR_2018_paper.pdf


909
Title: View Extrapolation of Human Body from a Single Image
Type: Poster
Authors: Hao Zhu, Nanjing University;hao Su, ;Peng Wang, Baidu;EE Department, Nanjing Univ;Ruigang Yang, University of Kentucky;

910
Title: Adversarially Occluded Samples for Person Re-identification
Type: Poster
Authors: Houjing Huang, CASIA;Dangwei Li, ;Zhang Zhang, ;Xiaotang Chen, ;Kaiqi Huang,;

911
Title: Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter
Type: Oral
Abstracts: Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.
Authors: Yuki Fujimura, Kyoto University;Masaaki Iiyama, Kyoto University;Atsushi Hashimoto, Kyoto University;Michihiko Minoh, Kyoto University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fujimura_Photometric_Stereo_in_CVPR_2018_paper.pdf


912
Title: Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter
Type: Poster
Abstracts: Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.
Authors: Yuki Fujimura, Kyoto University;Masaaki Iiyama, Kyoto University;Atsushi Hashimoto, Kyoto University;Michihiko Minoh, Kyoto University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fujimura_Photometric_Stereo_in_CVPR_2018_paper.pdf


913
Title: Single-Image Depth Estimation Based on Fourier Domain Analysis
Type: Poster
Abstracts: We propose a deep learning algorithm for single-image depth estimation based on the Fourier frequency domain analysis. First, we develop a convolutional neural network structure and propose a new loss function, called depth-balanced Euclidean loss, to train the network reliably for a wide range of depths. Then, we generate multiple depth map candidates by cropping input images with various cropping ratios. In general, a cropped image with a small ratio yields depth details more faithfully, while that with a large ratio provides the overall depth distribution more reliably. To take advantage of these complementary properties, we combine the multiple candidates in the frequency domain. Experimental results demonstrate that proposed algorithm provides the state-of-art performance. Furthermore, through the frequency domain analysis, we validate the efficacy of the proposed algorithm in most frequency bands.
Authors: Jaehan Lee, Korea University;Minhyeok Heo, Korea Unversity;Kyung-Rae Kim, Korea University;Chang-Su Kim,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Single-Image_Depth_Estimation_CVPR_2018_paper.pdf


914
Title: Future Person Localization in First-Person Videos
Type: Spotlight
Abstracts: We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.
Authors: Takuma Yagi, The University of Tokyo;Karttikeya Mangalam, IIT Kanpur;Ryo Yonetani, The University of Tokyo;Yoichi Sato, Univ of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yagi_Future_Person_Localization_CVPR_2018_paper.pdf


915
Title: Future Person Localization in First-Person Videos
Type: Poster
Abstracts: We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.
Authors: Takuma Yagi, The University of Tokyo;Karttikeya Mangalam, IIT Kanpur;Ryo Yonetani, The University of Tokyo;Yoichi Sato, Univ of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yagi_Future_Person_Localization_CVPR_2018_paper.pdf


916
Title: Classifier Learning with Prior Probabilities for Facial Action Unit Recognition
Type: Poster
Authors: Yong Zhang, CASIA;Weiming Dong, ;Bao-Gang Hu, CASIA;Qiang Ji, RPI;

917
Title: Smooth Neighbors on Teacher Graphs for Semi-supervised Learning
Type: Spotlight
Authors: Yucen Luo, Tsinghua University;Jun Zhu, Tsinghua University;Mengxi Li, Tsinghua University;Yong Ren, Tsinghua University;Bo Zhang,;

918
Title: Smooth Neighbors on Teacher Graphs for Semi-supervised Learning
Type: Poster
Authors: Yucen Luo, Tsinghua University;Jun Zhu, Tsinghua University;Mengxi Li, Tsinghua University;Yong Ren, Tsinghua University;Bo Zhang,;

919
Title: Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal
Type: Poster
Abstracts: Understanding shadows from a single image consists of two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.
Authors: Jifeng Wang, NJUST;Xiang Li, NJUST;Jian Yang, Nanjing University of Science and Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf


920
Title: Image Restoration by Estimating Frequency Distribution of Local Patches
Type: Poster
Abstracts: In this paper, we propose a method to solve the image restoration problem, which tries to restore the details of a corrupted image, especially due to the loss caused by JPEG compression. We have treated an image in the frequency domain to explicitly restore the frequency components lost during image compression. In doing so, the distribution in the frequency domain is learned using the cross entropy loss. Unlike recent approaches, we have reconstructed the details of an image without using the scheme of adversarial training. Rather, the image restoration problem is treated as a classification problem to determine the frequency coefficient for each frequency band in an image patch. In this paper, we show that the proposed method effectively restores a JPEG-compressed image with more detailed high frequency components, making the restored image more vivid.
Authors: Jaeyoung Yoo, Seoul National University;Sang ho Lee, Seoul National University;Nojun Kwak, Seoul National University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yoo_Image_Restoration_by_CVPR_2018_paper.pdf


921
Title: Fully Convolutional Attention Network for Multimodal Reasoning
Type: Poster
Authors: Haoqi Fan, Carnegie Mellon University;Jiatong Zhou,;

922
Title: A PID Controller Approach for Stochastic Optimization of Deep Networks
Type: Spotlight
Abstracts: Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet.
Authors: An Wangpeng , Tsinghua University;Tsinghua University, Shenzhen Graduate School;Qingyun Sun, Stanford Univsersity;Jun Xu, Hong Kong Polytechnic U;QIonghai Dai, Tsinghua University;Lei Zhang, The Hong Kong Polytechnic University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/An_A_PID_Controller_CVPR_2018_paper.pdf


923
Title: A PID Controller Approach for Stochastic Optimization of Deep Networks
Type: Poster
Abstracts: Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet.
Authors: An Wangpeng , Tsinghua University;Tsinghua University, Shenzhen Graduate School;Qingyun Sun, Stanford Univsersity;Jun Xu, Hong Kong Polytechnic U;QIonghai Dai, Tsinghua University;Lei Zhang, The Hong Kong Polytechnic University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/An_A_PID_Controller_CVPR_2018_paper.pdf


924
Title: Domain Generalization with Adversarial Feature Learning
Type: Poster
Authors: Haoliang Li, Nanyang Technological Universi;Nanyang Technological University, Singapore;Shiqi Wang, City University of Hong Kong;Alex Kot,;

925
Title: Camera Pose Estimation with Unknown Principal Point
Type: Poster
Authors: Viktor Larsson, Lund University;Zuzana Kukelova, Czech Technical University in Prague;National Institute of Informatics, Japan;

926
Title: Deformation Aware Image Compression
Type: Spotlight
Abstracts: Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more "compressible". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a "black box", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG~2000, WebP, BPG, and a recent deep-net method.
Authors: Tamar Rott Shaham, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shaham_Deformation_Aware_Image_CVPR_2018_paper.pdf


927
Title: Deformation Aware Image Compression
Type: Poster
Abstracts: Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more "compressible". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a "black box", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG~2000, WebP, BPG, and a recent deep-net method.
Authors: Tamar Rott Shaham, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shaham_Deformation_Aware_Image_CVPR_2018_paper.pdf


928
Title: AMNet: Memorability Estimation with Attention
Type: Poster
Authors: Jiri Fajtl, Kingston University;Vasileios Argyriou, Kingston University;Dorothy Monekosso, Leeds Beckett;Paolo Remagnino, Kingston University;

929
Title: High Performance Visual Tracking with Siamese Region Proposal Network
Type: Spotlight
Authors: Bo Li, SenseTime ;Wei Wu, ;Institute of Automation, CAS;Junjie Yan,;

930
Title: High Performance Visual Tracking with Siamese Region Proposal Network
Type: Poster
Authors: Bo Li, SenseTime ;Wei Wu, ;Institute of Automation, CAS;Junjie Yan,;

931
Title: Image Blind Denoising With Generative Adversarial Network Based Noise Modeling
Type: Poster
Abstracts: In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.
Authors: Jingwen Chen, Sun Yat-sen University;Jiawei Chen, Sun Yat-sen University;Hongyang Chao, Sun Yat-sen University;Ming Yang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Image_Blind_Denoising_CVPR_2018_paper.pdf


932
Title: Single View Stereo Matching
Type: Spotlight
Abstracts: Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.
Authors: Yue Luo, SenseTime;Jimmy Ren, SenseTime Group Limited;Mude Lin, Sun Yat-Sen University;Jiahao Pang, SenseTime Group Limited;Wenxiu Sun, SenseTime Group Limited;Hongsheng Li, ;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Single_View_Stereo_CVPR_2018_paper.pdf


933
Title: Single View Stereo Matching
Type: Poster
Abstracts: Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.
Authors: Yue Luo, SenseTime;Jimmy Ren, SenseTime Group Limited;Mude Lin, Sun Yat-Sen University;Jiahao Pang, SenseTime Group Limited;Wenxiu Sun, SenseTime Group Limited;Hongsheng Li, ;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Single_View_Stereo_CVPR_2018_paper.pdf


934
Title: Pyramid Stereo Matching Network
Type: Poster
Abstracts: Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.
Authors: Jia-Ren Chang, National Chiao Tung University;Yong-Sheng Chen, National Chiao Tung University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.pdf


935
Title: Interpret Neural Networks by Identifying Critical Data Routing Paths
Type: Spotlight
Abstracts: Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead.
Authors: Yulong Wang, Tsinghua University;Hang Su, Tsinghua University;Xiaolin Hu, tsinghua;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf


936
Title: Interpret Neural Networks by Identifying Critical Data Routing Paths
Type: Poster
Abstracts: Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead.
Authors: Yulong Wang, Tsinghua University;Hang Su, Tsinghua University;Xiaolin Hu, tsinghua;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf


937
Title: Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View
Type: Poster
Authors: Albert Pumarola, IRI (CSIC-UPC);Antonio Agudo, IRI (CSIC-UPC);Lorenzo Porzi, Mapillary Research;Alberto Sanfeliu, IRI (CSIC-UPC);Vincent Lepetit, University of Bordeaux;Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC);

938
Title: Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars
Type: Poster
Authors: Antonio Loquercio, University of Zurich;Ana Maqueda, Universidad Politecnica de Madrid;Guillermo Gallego, University of Zurich;Narciso Garcia, Universidad Politecnica de Madrid;Davide Scaramuzza, University of Zurich;

939
Title: Beyond Gr?bner Bases: Basis Selection for Minimal Solvers
Type: Spotlight
Authors: Viktor Larsson, Lund University;Magnus Oskarsson, Lund University Sweden;Kalle Astroem, Lund University;Alge Wallis, ;Zuzana Kukelova, Czech Technical University in Prague;Tomas Pajdla,;

940
Title: Beyond Gr?bner Bases: Basis Selection for Minimal Solvers
Type: Poster
Authors: Viktor Larsson, Lund University;Magnus Oskarsson, Lund University Sweden;Kalle Astroem, Lund University;Alge Wallis, ;Zuzana Kukelova, Czech Technical University in Prague;Tomas Pajdla,;

941
Title: A Unifying Contrast Maximization Framework for Event Cameras
Type: Spotlight
Authors: Guillermo Gallego, University of Zurich;Henri Rebecq, University of Zurich;Davide Scaramuzza, University of Zurich;

942
Title: A Unifying Contrast Maximization Framework for Event Cameras
Type: Poster
Authors: Guillermo Gallego, University of Zurich;Henri Rebecq, University of Zurich;Davide Scaramuzza, University of Zurich;

943
Title: PoTion: Pose MoTion Representation for Action Recognition
Type: Poster
Abstracts: Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition. We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by colorizing each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets.
Authors: Vasileios Choutas, Naver Labs Europe;Philippe Weinzaepfel, Xerox;Jerome Revaud, Naver Labs Europe;INRIA Grenoble, France;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.pdf


944
Title: Fight ill-posedness with ill-posedness: Single-shot variational depth super-resolution from shading
Type: Spotlight
Authors: Bjoern Haefner, TU Munich;Yvain Queau, Technical University Munich;Thomas M?llenhoff, Technical University of Munich;Daniel Cremers,;

945
Title: Fight ill-posedness with ill-posedness: Single-shot variational depth super-resolution from shading
Type: Poster
Authors: Bjoern Haefner, TU Munich;Yvain Queau, Technical University Munich;Thomas M?llenhoff, Technical University of Munich;Daniel Cremers,;

946
Title: Deep Lesion Graph in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-scale Lesion Database
Type: Poster
Authors: Ke Yan, National Institute of Health;Xiaosong Wang, NIH;Le Lu, Nvidia Corp;Ling Zhang, NIH;Adam Harrison, National Institutes of Health;MOHAMMADHADI Bagheri, NIH;Ronald Summers,;

947
Title: Inverse Composition Discriminative Optimization for Point Cloud Registration
Type: Poster
Abstracts: Rigid Point Cloud Registration (PCReg) refers to the problem of finding the rigid transformation between two sets of point clouds. This problem is particularly important due to the advances in new 3D sensing hardware, and it is challenging because neither the correspondence nor the transformation parameters are known. Traditional local PCReg methods (e.g., ICP) rely on local optimization algorithms, which can get trapped in bad local minima in the presence of noise, outliers, bad initializations, etc. To alleviate these issues, this paper proposes Inverse Composition Discriminative Optimization (ICDO), an extension of Discriminative Optimization (DO), which learns a sequence of update steps from synthetic training data that search the parameter space for an improved solution. Unlike DO, ICDO is object-independent and generalizes even to unseen shapes. We evaluated ICDO on both synthetic and real data, and show that ICDO can match the speed and outperform the accuracy of state-of-the-art PCReg algorithms.
Authors: Jayakorn Vongkulbhisal, Carnegie Mellon University;Be?at Irastorza Ugalde, ;Fernando de la Torre, ;Jo?o Costeira,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf


948
Title: Manifold Learning in Quotient Spaces
Type: Spotlight
Abstracts: When learning 3D shapes we are usually interested in their intrinsic geometry rather than in their orientation. To deal with the orientation variations the usual trick consists in augmenting the data to exhibit all possible variability, and thus let the model learn both the geometry as well as the rotations. In this paper we introduce a new autoencoder model for encoding and synthesis of 3D shapes. To get rid of undesirable input variability our model learns a manifold in a quotient space of the input space. Typically, we propose to quotient the space of 3D models by the action of rotations. Thus, our quotient autoencoder allows to directly learn in the space of interest, ignoring side information. This is reflected in better performances on reconstruction and interpolation tasks, as our experiments show that our model outperforms a vanilla autoencoder on the well-known Shapenet dataset. Moreover, our model learns a rotation-invariant representation, leading to interesting results in shapes co-alignment. Finally, we extend our quotient autoencoder to quotient by non-rigid transformations.
Authors: ?loi Mehr, LIP6;Andr? Lieutier, ;Fernando Sanchez Bermudez, ;Vincent Guitteny, ;Nicolas Thome, Conservatoire national des arts et m?tiers;Matthieu Cord,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mehr_Manifold_Learning_in_CVPR_2018_paper.pdf


949
Title: Manifold Learning in Quotient Spaces
Type: Poster
Abstracts: When learning 3D shapes we are usually interested in their intrinsic geometry rather than in their orientation. To deal with the orientation variations the usual trick consists in augmenting the data to exhibit all possible variability, and thus let the model learn both the geometry as well as the rotations. In this paper we introduce a new autoencoder model for encoding and synthesis of 3D shapes. To get rid of undesirable input variability our model learns a manifold in a quotient space of the input space. Typically, we propose to quotient the space of 3D models by the action of rotations. Thus, our quotient autoencoder allows to directly learn in the space of interest, ignoring side information. This is reflected in better performances on reconstruction and interpolation tasks, as our experiments show that our model outperforms a vanilla autoencoder on the well-known Shapenet dataset. Moreover, our model learns a rotation-invariant representation, leading to interesting results in shapes co-alignment. Finally, we extend our quotient autoencoder to quotient by non-rigid transformations.
Authors: ?loi Mehr, LIP6;Andr? Lieutier, ;Fernando Sanchez Bermudez, ;Vincent Guitteny, ;Nicolas Thome, Conservatoire national des arts et m?tiers;Matthieu Cord,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mehr_Manifold_Learning_in_CVPR_2018_paper.pdf


950
Title: Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation
Type: Poster
Abstracts: We propose a novel end-to-end trainable, deep, encoder-decoder architecture for single-pass semantic segmentation. Our approach is based on a cascaded architecture with feature-level long-range skip connections. The encoder incorporates the structure of ResNeXt's residual building blocks and adopts the strategy of repeating a building block that aggregates a set of transformations with the same topology. The decoder features a novel architecture, consisting of blocks, that (i) capture context information, (ii) generate semantic features, and (iii) enable fusion between different output resolutions. Crucially, we introduce dense decoder shortcut connections to allow decoder blocks to use semantic feature maps from all previous decoder levels, i.e. from all higher-level feature maps. The dense decoder connections allow for effective information propagation from one decoder block to another, as well as for multi-level feature fusion that significantly improves the accuracy. Importantly, these connections allow our method to obtain state-of-the-art performance on several challenging datasets, without the need of time-consuming multi-scale averaging of previous works.
Authors: Piotr Bilinski, University of Oxford;Victor Prisacariu, Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf


951
Title: Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks
Type: Poster
Authors: Alexander Sage, ETH Zurich;Eirikur Agustsson, ETH Zurich;Radu Timofte, ETH Zurich;Luc Van Gool, KTH;

952
Title: Seeing Voices and Hearing Faces: Cross-modal biometric matching
Type: Spotlight
Authors: Arsha Nagrani, University of Oxford;Samuel Albanie, Oxford University;Andrew Zisserman, Oxford;

953
Title: Seeing Voices and Hearing Faces: Cross-modal biometric matching
Type: Poster
Authors: Arsha Nagrani, University of Oxford;Samuel Albanie, Oxford University;Andrew Zisserman, Oxford;

954
Title: OL?: Orthogonal Low-rank Embedding
Type: Poster
Authors: Universidad de la Republica, Uruguay;Qiang Qiu, ;Universidad de la Republica, Uruguay;Guillermo Sapiro, Duke;

955
Title: Compressed Video Action Recognition
Type: Spotlight
Abstracts: Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video. This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.
Authors: Chao-Yuan Wu, UT Austin;Manzil Zaheer, Carnegie Mellon University;Hexiang Hu, ;R. Manmatha, A9;Alexander Smola, ;Philipp Krahenbuhl,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf


956
Title: Compressed Video Action Recognition
Type: Poster
Abstracts: Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video. This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.
Authors: Chao-Yuan Wu, UT Austin;Manzil Zaheer, Carnegie Mellon University;Hexiang Hu, ;R. Manmatha, A9;Alexander Smola, ;Philipp Krahenbuhl,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf


957
Title: Efficient parametrization of multi-domain deep neural networks
Type: Poster
Authors: Sylvestre-Alvise Rebuffi, University of Oxford;Hakan Bilen, University of Oxford;Andrea Vedaldi, U Oxford;

958
Title: Learning Answer Embeddings for Visual Question Answering
Type: Poster
Abstracts: We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach performs well not only on in-domain learning but also on transfer learning.
Authors: Hexiang Hu, ;Wei-Lun Chao, USC;Fei Sha, University of Southern California;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf


959
Title: Pixar: Real-time 3D Object Detection from Point Clouds
Type: Poster
Authors: Uber ATG, UofT;Wenjie Luo, Uber ATG.;Wenjie Luo, UofT;Raquel Urtasun, University of Toronto;

960
Title: Fast and Furious: Real Time End-to-End 3D Detection
Type: Oral
Authors: Wenjie Luo, Uber ATG.;Wenjie Luo, UofT;Uber ATG, UofT;Raquel Urtasun, University of Toronto;

961
Title: Fast and Furious: Real Time End-to-End 3D Detection
Type: Poster
Authors: Wenjie Luo, Uber ATG.;Wenjie Luo, UofT;Uber ATG, UofT;Raquel Urtasun, University of Toronto;

962
Title: Creating Capsule Wardrobes from Fashion Images
Type: Spotlight
Authors: Wei-Lin Hsiao, UT-Austin;Kristen Grauman,;

963
Title: Creating Capsule Wardrobes from Fashion Images
Type: Poster
Authors: Wei-Lin Hsiao, UT-Austin;Kristen Grauman,;

964
Title: Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics
Type: Spotlight
Abstracts: Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.
Authors: Alex Kendall, ;Yarin Gal, University of Cambridge;Roberto Cipolla,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf


965
Title: Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics
Type: Poster
Abstracts: Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.
Authors: Alex Kendall, ;Yarin Gal, University of Cambridge;Roberto Cipolla,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf


966
Title: Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories
Type: Spotlight
Authors: Antonio Agudo, IRI (CSIC-UPC);Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC);

967
Title: Image Collection Pop-up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories
Type: Poster
Authors: Antonio Agudo, IRI (CSIC-UPC);Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC);

968
Title: Sim2Real View Invariant Visual Servoing by Recurrent Control
Type: Poster
Authors: Fereshteh Sadeghi, University of Washington;Alexander Toshev, Google;Sergey Levine, UC Berkeley;

969
Title: Spanning Patches: Deep Patch Selection for Fast Multi-View Stereo
Type: Poster
Authors: Alex Poms, Carnegie Mellon University;Shoou-I Yu, Oculus;Chenglei Wu, Oculus;Yaser Sheikh,;

970
Title: Efficient Large-scale Approximate Nearest Neighbor Search on OpenCL FPGA
Type: Poster
Authors: Jialiang Zhang, University of Wisconsin-Madiso;Soroosh Khoram, UW-Madison;Jing Li, University of Wisconsin-Madison;

971
Title: Learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms
Type: Poster
Authors: Alexandre B?ne, Brain and Spine Institute;Olivier Colliot, Institut du Cerveau et de la Moelle ?pini?re;Stanley Durrleman, Institut du Cerveau et de la Moelle ?pini?re;

972
Title: Trapping Light for Time of Flight
Type: Oral
Abstracts: We propose a novel imaging method for near-complete, surround, 3D reconstruction of geometrically complex objects, in a single shot. The key idea is to augment a time-of-flight (ToF) based 3D sensor with a multi-mirror system, called a light-trap. The shape of the trap is chosen so that light rays entering it bounce multiple times inside the trap, thereby visiting every position inside the trap multiple times from various directions. We show via simulations that this enables light rays to reach more than 99.9% of the surface of objects placed inside the trap, even those with strong occlusions, for example, lattice-shaped objects. The ToF sensor provides the path length for each light ray, which, along with the known shape of the trap, is used to reconstruct the complete paths of all the rays. This enables performing dense, surround 3D reconstructions of objects with highly complex 3D shapes, in a single shot. We have developed a proof-of-concept hardware prototype consisting of a pulsed ToF sensor, and a light trap built with planar mirrors. We demonstrate the effectiveness of the light trap based 3D reconstruction method on a variety of objects with a broad range of geometry and reflectance properties.
Authors: Ruilin Xu, Columbia University;Mohit Gupta, Wisconsin;Shree Nayar, Columbia University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Trapping_Light_for_CVPR_2018_paper.pdf


973
Title: Trapping Light for Time of Flight
Type: Poster
Abstracts: We propose a novel imaging method for near-complete, surround, 3D reconstruction of geometrically complex objects, in a single shot. The key idea is to augment a time-of-flight (ToF) based 3D sensor with a multi-mirror system, called a light-trap. The shape of the trap is chosen so that light rays entering it bounce multiple times inside the trap, thereby visiting every position inside the trap multiple times from various directions. We show via simulations that this enables light rays to reach more than 99.9% of the surface of objects placed inside the trap, even those with strong occlusions, for example, lattice-shaped objects. The ToF sensor provides the path length for each light ray, which, along with the known shape of the trap, is used to reconstruct the complete paths of all the rays. This enables performing dense, surround 3D reconstructions of objects with highly complex 3D shapes, in a single shot. We have developed a proof-of-concept hardware prototype consisting of a pulsed ToF sensor, and a light trap built with planar mirrors. We demonstrate the effectiveness of the light trap based 3D reconstruction method on a variety of objects with a broad range of geometry and reflectance properties.
Authors: Ruilin Xu, Columbia University;Mohit Gupta, Wisconsin;Shree Nayar, Columbia University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Trapping_Light_for_CVPR_2018_paper.pdf


974
Title: Geometry Aware Optimization for Deep Learning: The Good Practice
Type: Poster
Authors: SOUMAVA KUMAR ROY, AUSTRALIAN NATIONAL UNIVERSITY;Data61, CSIRO;Mehrtash Harandi, Australian National University;

975
Title: Good View Hunting: Learning Photo Composition from 1 Million View Pairs
Type: Poster
Authors: Zijun Wei, Stony Brook University;Jianming Zhang, Adobe Research;Minh Hoai, Stony Brook University;Xiaohui Shen, Adobe Research;Adobe Systems, Inc.;Radom?r Mech, ;Dimitris Samaras,;

976
Title: Analyzing Filters Toward Efficient ConvNet
Type: Poster
Abstracts: Deep convolutional neural network (ConvNet) is a promising approach for high-performance image classification. The behavior of ConvNet is analyzed mainly based on the neuron activations, such as by visualizing them. In this paper, in contrast to the activations, we focus on filters which are main components of ConvNets. Through analyzing two types of filters at convolution and fully-connected (FC) layers, respectively, on various pre-trained ConvNets, we present the methods to efficiently reformulate the filters, contributing to improving both memory size and classification performance of the ConvNets. They render the filter bases formulated in a parameter-free form as well as the efficient representation for the FC layer. The experimental results on image classification show that the methods are favorably applied to improve various ConvNets, including ResNet, trained on ImageNet with exhibiting high transferability on the other datasets.
Authors: Takumi Kobayashi,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kobayashi_Analyzing_Filters_Toward_CVPR_2018_paper.pdf


977
Title: Feature Space Transfer for Data Augmentation
Type: Oral
Abstracts: The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.
Authors: Bo Liu, UCSD;Xudong Wang, UCSD;Mandar Dixit, UC San Diego;Roland Kwitt, ;UCSD, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Feature_Space_Transfer_CVPR_2018_paper.pdf


978
Title: Feature Space Transfer for Data Augmentation
Type: Poster
Abstracts: The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.
Authors: Bo Liu, UCSD;Xudong Wang, UCSD;Mandar Dixit, UC San Diego;Roland Kwitt, ;UCSD, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Feature_Space_Transfer_CVPR_2018_paper.pdf


979
Title: Bilateral Ordinal Relevance Multi-instance Regression for Facial Action Unit Intensity Estimation
Type: Poster
Authors: Yong Zhang, CASIA;Rui Zhao, Rensselaer Polytechnic Institu;Weiming Dong, ;Bao-Gang Hu, CASIA;Qiang Ji, RPI;

980
Title: Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250Hz
Type: Oral
Authors: Ayush Tewari, MPI Informatics;Michael Zollh?fer, MPI Informatics;Pablo Garrido, ;Florian Bernard, ;Hyeongwoo Kim, MPII;Patrick Perez, Technicolor Research;Christian Theobalt, MPI Informatics;

981
Title: Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250Hz
Type: Poster
Authors: Ayush Tewari, MPI Informatics;Michael Zollh?fer, MPI Informatics;Pablo Garrido, ;Florian Bernard, ;Hyeongwoo Kim, MPII;Patrick Perez, Technicolor Research;Christian Theobalt, MPI Informatics;

982
Title: Interpretable Video Captioning via Trajectory Structured Localization
Type: Poster
Abstracts: Automatically describing open-domain videos with natural language are attracting increasing interest in the field of artificial intelligence. Most existing methods simply borrow ideas from image captioning and obtain a compact video representation from an ensemble of global image feature before feeding to an RNN decoder which outputs a sentence of variable length. However, it is not only arduous for the generator to focus on specific salient objects at different time given the global video representation, it is more formidable to capture the fine-grained motion information and the relation between moving instances for more subtle linguistic descriptions. In this paper, we propose a Trajectory Structured Attentional Encoder-Decoder (TSA-ED) neural network framework for more elaborate video captioning which works by integrating local spatial-temporal representation at trajectory level through structured attention mechanism. Our proposed method is based on a LSTM-based encoder-decoder framework, which incorporates an attention modeling scheme to adaptively learn the correlation between sentence structure and the moving objects in videos, and consequently generates more accurate and meticulous statement description in the decoding stage. Experimental results demonstrate that the feature representation and structured attention mechanism based on the trajectory cluster can efficiently obtain the local motion information in the video to help generate a more fine-grained video description, and achieve the state-of-the-art performance on the well-known Charades and MSVD datasets.
Authors: Xian Wu, Sysu;Guanbin Li, ;Liang Lin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.pdf


983
Title: Joint Optimization Framework for Learning with Noisy Labels
Type: Poster
Authors: Daiki Tanaka, The University of Tokyo;Daiki Ikami, The University of Tokyo;Toshihiko Yamasaki, The University of Tokyo;Kiyoharu Aizawa,;

984
Title: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks
Type: Spotlight
Authors: Benjamin Graham, Facebook AI Research;Laurens van der Maaten, Facebook;Martin Engelcke, University of Oxford;

985
Title: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks
Type: Poster
Authors: Benjamin Graham, Facebook AI Research;Laurens van der Maaten, Facebook;Martin Engelcke, University of Oxford;

986
Title: Learning a Complete Image Indexing Pipeline
Type: Poster
Abstracts: To work at scale, a complete image indexing system comprises two components: An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter, the former continues to be based on unsupervised clustering in the literature. In this work, we propose a first system that learns both components within a unifying neural framework of structured binary encoding.
Authors: Inria, Technicolor;Joaquin Zepeda, ;Patrick Perez, Technicolor Research;R?mi Gribonval, Inria;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Jain_Learning_a_Complete_CVPR_2018_paper.pdf


987
Title: Real-Time Seamless Single Shot 6D Object Pose Prediction
Type: Poster
Abstracts: We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [Kehl et al. 2017] that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by [Redmon et al. 2016, Redmon and Farhadi 2017] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm. For single object and multiple object pose estimation on the LineMod and Occlusion datasets, our approach substantially outperforms other recent CNN-based approaches [Kehl et al. 2017, Rad and Lepetit 2017] when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.
Authors: Bugra Tekin, ;Sudipta Sinha, Microsoft Research;Pascal Fua,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.pdf


988
Title: Inferring Co-Attention in Social Scene Videos
Type: Poster
Authors: Lifeng Fan, VCLA@UCLA;Yixin Chen, VCLA@UCLA;Ping Wei, Xi'an Jiaotong University;Song-Chun Zhu,;

989
Title: A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation
Type: Poster
Abstracts: We propose a novel neural network architecture for point cloud classification. Our key idea is to automatically transform the 3D unordered input data into a set of useful 2D depth images, and classify them by exploiting well performing image classification CNNs. We present new differentiable module designs to generate depth images from a point cloud. These modules can be combined with any network architecture for processing point clouds. We utilize them in combination with state-of-the-art classification networks, and get results competitive with the state of the art in point cloud classification. Furthermore, our architecture automatically produces informative images representing the input point cloud, which could be used for further applications such as point cloud visualization.
Authors: Lukas Rahmann, ;Riccardo Roveri, ETH Zurich;Cengiz Oztireli, ;Markus Gross,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf


990
Title: Blind Predicting Similar Quality Map for Image Quality Assessment
Type: Poster
Abstracts: A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efficient BIQA model based on a novel framework which consists of a fully convolutional neural network (FCNN) and a pooling network to solve this problem. In principle, FCNN is capable of predicting a pixel-by-pixel similar quality map only from a distorted image by using the intermediate similarity maps derived from conventional full-reference image quality assessment methods. The predicted pixel-by-pixel quality maps have good consistency with the distortion correlations between the reference and distorted images. Finally, a deep pooling network regresses the quality map into a score. Experiments have demonstrated that our predictions outperform many state-of-the-art BIQA methods.
Authors: Da Pan, Communication University of CN;Ping Shi, ;Ming Hou, ;Zefeng Ying, ;Sizhe Fu, ;Yuan Zhang,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Blind_Predicting_Similar_CVPR_2018_paper.pdf


991
Title: CodeSLAM --- Learning a Compact
Type: Oral
Authors: Michael Bloesch, Imperial College London;Jan Czarnowski, Imperial College London;Ronald Clark, Imperial College London;Stefan Leutenegger, Imperial College London;Andrew Davison, Imperial College London UK;

992
Title: CodeSLAM --- Learning a Compact
Type: Poster
Authors: Michael Bloesch, Imperial College London;Jan Czarnowski, Imperial College London;Ronald Clark, Imperial College London;Stefan Leutenegger, Imperial College London;Andrew Davison, Imperial College London UK;

993
Title: Image Correction via Deep Reciprocating HDR Transformation
Type: Poster
Abstracts: Image correction aims to adjust an input image into a visually pleasing one with the detail in the under/over exposed regions recovered. However, existing image correction methods are mainly based on image pixel operations, and attempting to recover the lost detail from these under/over exposed regions is challenging. We, therefore, revisit the image formation procedure and notice that detail is contained in the high dynamic range (HDR) light intensities(which can be perceived by human eyes) but is lost during the nonlinear imaging process by of the camera in the low dynamic range (LDR) domain. Inspired by this observation, we formulate the image correction problem as the Deep Reciprocating HDR Transformation (DRHT) process and propose a novel approach to first reconstruct the lost detail in the HDR domain and then transfer them back to the LDR image as the output image with the recovered detail preserved. To this end, we propose an end-to-end DRHT model, which contains two CNNs, one for HDR detail reconstruction and the other for LDR detail correction. Experiments on the standard benchmarks demonstrate the effectiveness of the proposed method, compared with state-of-the-art image correction methods.
Authors: Dalian University of Technology, City University of Hong Kong;Ke Xu, Dalian University of Technology;Ke Xu, City University of Hong Kong;Yibing Song, Tencent AI Lab;Qiang Zhang, Dalian University of Technology;Xiaopeng Wei, Dalian University of Technology;Rynson Lau, City University of Hong Kong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Image_Correction_via_CVPR_2018_paper.pdf


994
Title: Towards Human-Machine Cooperation: Evolving Active Learning with Self-supervised Process for Object Detection
Type: Poster
Authors: Keze Wang, ;Liang Lin, ;Xiaopeng Yan, Sun Yat-sen University;Lei Zhang, The Hong Kong Polytechnic University;

995
Title: Modeling Facial Geometry using Compositional VAEs
Type: Spotlight
Authors: Timur Bagautdinov, ;Chenglei Wu, Oculus;Jason Saragih, Oculus Research;Pascal Fua, ;Yaser Sheikh,;

996
Title: Modeling Facial Geometry using Compositional VAEs
Type: Poster
Authors: Timur Bagautdinov, ;Chenglei Wu, Oculus;Jason Saragih, Oculus Research;Pascal Fua, ;Yaser Sheikh,;

997
Title: PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition
Type: Poster
Abstracts: Unlike its image based counterpart, point cloud based retrieval for place recognition has remained as an unexplored and unsolved problem. This is largely due to the difficulty in extracting local feature descriptors from a point cloud that can subsequently be encoded into a global descriptor for the retrieval task. In this paper, we propose the PointNetVLAD where we leverage on the recent success of deep networks to solve point cloud based retrieval for place recognition. Specifically, our PointNetVLAD is a combination/modification of the existing PointNet and NetVLAD, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. Furthermore, we propose the "lazy triplet and quadruplet" loss functions that can achieve more discriminative and generalizable global descriptors to tackle the retrieval task. We create benchmark datasets for point cloud based retrieval for place recognition, and the experimental results on these datasets show the feasibility of our PointNetVLAD.
Authors: Mikaela Angelina Uy, NUS;Gim Hee Lee, National University of SIngapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.pdf


998
Title: PoseFlow: A Deep Motion Representation for Understanding Human Behaviors in Videos
Type: Poster
Abstracts: Motion of the human body is the critical cue for understanding and characterizing human behavior in videos. Most existing approaches explore the motion cue using optical flows. However, optical flow usually contains motion on both the interested human bodies and the undesired background. This "noisy" motion representation makes it very challenging for pose estimation and action recognition in real scenarios. To address this issue, this paper presents a novel deep motion representation, called PoseFlow, which reveals human motion in videos while suppressing background and motion blur, and being robust to occlusion. For learning PoseFlow with mild computational cost, we propose a functionally structured spatial-temporal deep network, PoseFlow Net (PFN), to jointly solve the skeleton localization and matching problems of PoseFlow. Comprehensive experiments show that PFN outperforms the state-of-the-art deep flow estimation models in generating PoseFlow. Moreover, PoseFlow demonstrates its potential on improving two challenging tasks in human video analysis: pose estimation and action recognition.
Authors: Dingwen Zhang, ;Guangyu Guo, ;Dong Huang, Carnegie Mellon University;Fernando de la Torre, ;Junwei Han, Northwestern Polytechnical U.;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_PoseFlow_A_Deep_CVPR_2018_paper.pdf


999
Title: Leveraging Unlabeled Data for Crowd Counting by Learning to Rank
Type: Poster
Abstracts: We propose a novel crowd counting approach that leverages abundantly available unlabeled crowd imagery in a learning-to-rank framework. To induce a ranking of cropped images , we use the observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the super-image. This allows us to address the problem of limited size of existing datasets for crowd counting. We collect two crowd scene datasets from Google using keyword searches and query-by-example image retrieval, respectively. We demonstrate how to efficiently learn from these unlabeled datasets by incorporating learning-to-rank in a multi-task network which simultaneously ranks images and estimates crowd density maps. Experiments on two of the most challenging crowd counting datasets show that our approach obtains state-of-the-art results.
Authors: Xialei Liu, Computer Vision Center of UAB;Joost van de Weijer, Computer Vision Center Barcelona;Computer Vision Center, Barcelona;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Leveraging_Unlabeled_Data_CVPR_2018_paper.pdf


1000
Title: Deep Density Clustering of Unconstrained Faces
Type: Poster
Abstracts: In this paper, we consider the problem of grouping a collection of unconstrained face images in which the number of subjects is not known. We propose an unsupervised clustering algorithm called Deep Density Clustering (DDC) which is based on measuring density affinities between local neighborhoods in the feature space. By learning the minimal covering sphere for each neighborhood, information about the underlying structure is encapsulated. The encapsulation is also capable of locating high-density region of the neighborhood, which aids in measuring the neighborhood similarity. We theoretically show that the encapsulation asymptotically converges to a Parzen window density estimator. Our experiments show that DDC is a superior candidate for clustering unconstrained faces when the number of subjects is unknown. Unlike conventional linkage and density-based methods that are sensitive to the selection operating points, DDC attains more consistent and improved performance. Furthermore, the density-aware property reduces the difficulty in finding appropriate operating points.
Authors: Wei-An Lin, UMD;Jun-Cheng Chen, ;Carlos Castillo, ;University of Maryland, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Deep_Density_Clustering_CVPR_2018_paper.pdf


1001
Title: Learning Steerable Filters for Rotation Equivariant CNNs
Type: Poster
Abstracts: In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.
Authors: Maurice Weiler, Heidelberg University;Heidelberg University, Germany;Martin Storath,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Weiler_Learning_Steerable_Filters_CVPR_2018_paper.pdf


1002
Title: Radially-Distorted Conjugate Translations
Type: Poster
Abstracts: This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Gr{\"o}bner-basis method are stable, small and fast. The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation.
Authors: James Pritts, Czech Technical University;Zuzana Kukelova, Czech Technical University in Prague;Viktor Larsson, Lund University;Ondrej Chum, Czech Technical University in Prague;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pritts_Radially-Distorted_Conjugate_Translations_CVPR_2018_paper.pdf


1003
Title: Deep Spatio-Temporal Random Fields for Efficient Video Segmentation
Type: Spotlight
Abstracts: In this work we introduce a time- and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. Our implementation is based on the Caffe2 framework and will be available at https://github.com/siddharthachandra/gcrf-v3.0.
Authors: Siddhartha Chandra, INRIA;Camille Couprie, Facebook Artificial Intelligence Research;Iasonas Kokkinos, FAIR/UCL;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chandra_Deep_Spatio-Temporal_Random_CVPR_2018_paper.pdf


1004
Title: Deep Spatio-Temporal Random Fields for Efficient Video Segmentation
Type: Poster
Abstracts: In this work we introduce a time- and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. Our implementation is based on the Caffe2 framework and will be available at https://github.com/siddharthachandra/gcrf-v3.0.
Authors: Siddhartha Chandra, INRIA;Camille Couprie, Facebook Artificial Intelligence Research;Iasonas Kokkinos, FAIR/UCL;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chandra_Deep_Spatio-Temporal_Random_CVPR_2018_paper.pdf


1005
Title: Progressively Complementarity-aware Fusion Network for RGB-D Salient Object Detection
Type: Poster
Authors: Hao Chen, City University of Hong Kong;You fu Li, City University of Hong Kong;

1006
Title: Regularizing Deep Networks by Modeling and Predicting Label Structure
Type: Poster
Abstracts: We construct custom regularization functions for use in supervised training of deep neural networks. Our technique is applicable when the ground-truth labels themselves exhibit internal structure; we derive a regularizer by learning an autoencoder over the set of annotations. Training thereby becomes a two-phase procedure. The first phase models labels with an autoencoder. The second phase trains the actual network of interest by attaching an auxiliary branch that must predict output via a hidden layer of the autoencoder. After training, we discard this auxiliary branch. We experiment in the context of semantic segmentation, demonstrating this regularization strategy leads to consistent accuracy boosts over baselines, both when training from scratch, or in combination with ImageNet pretraining. Gains are also consistent over different choices of convolutional network architecture. As our regularizer is discarded after training, our method has zero cost at test time; the performance improvements are essentially free. We are simply able to learn better network weights by building an abstract model of the label space, and then training the network to understand this abstraction alongside the original task.
Authors: Mohammadreza Mostajabi, TTI-Chicago;Michael Maire, ;Greg Shakhnarovich,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mostajabi_Regularizing_Deep_Networks_CVPR_2018_paper.pdf


1007
Title: Image Super-resolution via Dual-state Recurrent Neural Networks
Type: Poster
Authors: Wei Han, UIUC;Shiyu Chang, ;Ding Liu, UIUC;Michael Witbrock, ;Thomas Huang,;

1008
Title: Robust Hough Transform Based 3D Reconstruction from Circular Light Fields
Type: Spotlight
Authors: Alessandro Vianello, Robert Bosch GmbH;Jens Ackermann, Robert Bosch GmbH;Maximilian Diebold, Heidelberg University;Bernd J?hne, University of Heidelberg;

1009
Title: Robust Hough Transform Based 3D Reconstruction from Circular Light Fields
Type: Poster
Authors: Alessandro Vianello, Robert Bosch GmbH;Jens Ackermann, Robert Bosch GmbH;Maximilian Diebold, Heidelberg University;Bernd J?hne, University of Heidelberg;

1010
Title: Probabilistic Joint Face-Skull Modelling for Facial Reconstruction
Type: Poster
Abstracts: We present a novel method for co-registration of two independent statistical shape models. We solve the problem of aligning a face model to a skull model with stochastic optimization based on Markov Chain Monte Carlo (MCMC). We create a probabilistic joint face-skull model and show how to obtain a distribution of plausible face shapes given a skull shape. Due to environmental and genetic factors, there exists a distribution of possible face shapes arising from the same skull. We pose facial reconstruction as a conditional distribution of plausible face shapes given a skull shape. Because it is very difficult to obtain the distribution directly from MRI or CT data, we create a dataset of artificial face-skull pairs. To do this, we propose to combine three data sources of independent origin to model the joint face-skull distribution: a face shape model, a skull shape model and tissue depth marker information. For a given skull, we compute the posterior distribution of faces matching the tissue depth distribution with Metropolis-Hastings. We estimate the joint face-skull distribution from samples of the posterior. To find faces matching to an unknown skull, we estimate the probability of the face under the joint face-skull model. To our knowledge, we are the first to provide a whole distribution of plausible faces arising from a skull instead of only a single reconstruction. We show how the face-skull model can be used to rank a face dataset and on average successfully identify the correct match in top 30%. The face ranking even works when obtaining the face shapes from 2D images. We furthermore show how the face-skull model can be useful to estimate the skull position in an MR-image.
Authors: Dennis Madsen, University of Basel;Marcel L?thi, ;Andreas Schneider, ;Thomas Vetter, U. Basel;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Madsen_Probabilistic_Joint_Face-Skull_CVPR_2018_paper.pdf


1011
Title: Making Convolutional Networks Recurrent for Visual Sequence Learning
Type: Poster
Abstracts: Recurrent neural networks (RNNs) have emerged as a powerful model for a broad range of machine learning problems that involve sequential data. While an abundance of work exists to understand and improve RNNs in the context of language and audio signals such as language modeling and speech recognition, relatively little attention has been paid to analyze or modify RNNs for visual sequences, which by nature have distinct properties. In this paper, we aim to bridge this gap and present the first large-scale exploration of RNNs for visual sequence learning. In particular, with the intention of leveraging the strong generalization capacity of pre-trained convolutional neural networks (CNNs), we propose a novel and effective approach, PreRNN, to make pre-trained CNNs recurrent by transforming convolutional layers or fully connected layers into recurrent layers. We conduct extensive evaluations on three representative visual sequence learning tasks: sequential face alignment, dynamic hand gesture recognition, and action recognition. Our experiments reveal that PreRNN consistently outperforms the traditional RNNs and achieves state-of-the-art results on the three applications, suggesting that PreRNN is more suitable for visual sequence learning.
Authors: Xiaodong Yang, NVIDIA;Pavlo Molchanov, NVIDIA Research;Jan Kautz, NVIDIA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Making_Convolutional_Networks_CVPR_2018_paper.pdf


1012
Title: Zero-Shot Kernel Learning.
Type: Poster
Authors: Hongguang Zhang, Data61;Piotr Koniusz, Data61/CSIRO;

1013
Title: Deeper Look at Power Normalizations.
Type: Poster
Authors: Piotr Koniusz, Data61/CSIRO;Hongguang Zhang, Data61;NICTA, Australia;

1014
Title: Deformable GANs for Pose-based Human Image Generation
Type: Poster
Authors: DISI, University of Trento;Enver Sangineto, University of Trento;St?phane Lathuili?re, Inria;Nicu Sebe, University of Trento;

1015
Title: On the Duality Between Retinex and Image Dehazing
Type: Poster
Abstracts: Image dehazing deals with the removal of undesired loss of visibility in outdoor images due to the presence of fog. Retinex is a color vision model mimicking the ability of the Human Visual System to robustly discount varying illuminations when observing a scene under different spectral lighting conditions. Retinex has been widely explored in the computer vision literature for image enhancement and other related tasks. While these two problems are apparently unrelated, the goal of this work is to show that they can be connected by a simple linear relationship. Specifically, most Retinex-based algorithms have the characteristic feature of always increasing image brightness, which turns them into ideal candidates for effective image dehazing by directly applying Retinex to a hazy image whose intensities have been inverted. In this paper, we give theoretical proof that Retinex on inverted intensities is a solution to the image dehazing problem. Comprehensive qualitative and quantitative results indicate that several classical and modern implementations of Retinex can be transformed into competing image dehazing algorithms performing on pair with more complex fog removal methods, and can overcome some of the main challenges associated with this problem.
Authors: Adrian Galdran, INESC TEC Porto;Aitor Alvarez-Gila, Tecnalia / CVC-Universitat Autonoma de Barcelona;Alessandro Bria, University of Cassino and L.M.;Javier Vazquez-Corral, Universitat Pompeu Fabra;Marcelo Bertalmio,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Galdran_On_the_Duality_CVPR_2018_paper.pdf


1016
Title: LDMNet: Low Dimensional Manifold Regularized Neural Networks
Type: Spotlight
Abstracts: Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional- Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition.
Authors: Wei Zhu, Duke University;Qiang Qiu, ;Jiaji Huang, Baidu Silicon Valley AI Lab;Robert Calderbank, Duke University;Guillermo Sapiro, Duke;Ingrid Daubechies, Duke University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_LDMNet_Low_Dimensional_CVPR_2018_paper.pdf


1017
Title: LDMNet: Low Dimensional Manifold Regularized Neural Networks
Type: Poster
Abstracts: Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional- Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition.
Authors: Wei Zhu, Duke University;Qiang Qiu, ;Jiaji Huang, Baidu Silicon Valley AI Lab;Robert Calderbank, Duke University;Guillermo Sapiro, Duke;Ingrid Daubechies, Duke University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_LDMNet_Low_Dimensional_CVPR_2018_paper.pdf


1018
Title: Pulling Actions out of Context: Explicit Separation for Effective Combination
Type: Poster
Abstracts: The ability to recognize human actions in video has many potential applications. Human action recognition, however, is tremendously challenging for computers due to the complexity of video data and the subtlety of human actions. Most current recognition systems flounder on the inability to separate human actions from co-occurring factors that usually dominate subtle human actions. In this paper, we propose a novel approach for training a human action recognizer, one that can: (1) explicitly factorize human actions from the co-occurring factors; (2) deliberately build a model for human actions and a separate model for all correlated contextual elements; and (3) effectively combine the models for human action recognition. Our approach exploits the benefits of conjugate samples of human actions, which are video clips that are contextually similar to human action samples, but do not contain the action. Experiments on ActionThread, PASCAL VOC, UCF101, and Hollywood2 datasets demonstrate the ability to separate action from context of the proposed approach.
Authors: Yang Wang, Stony Brook University;Minh Hoai, Stony Brook University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Pulling_Actions_out_CVPR_2018_paper.pdf


1019
Title: An Unsupervised Learning Model for Deformable Medical Image Registration
Type: Poster
Abstracts: We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a CNN, and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph.
Authors: Guha Balakrishnan, MIT;Adrian Dalca, ;Amy Zhao, MIT;Mert Sabuncu, Cornell;John Guttag,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.pdf


1020
Title: Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs
Type: Poster
Abstracts: This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point. We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation.
Authors: Emanuel Laude, TUM;Jan-Hendrik Lange, ;Jonas Schuepfer, ;Csaba Domokos, ;Laura Leal-Taixe, Technical University of Munich;Frank Schmidt, BCAI;Bjoern Andres, ;Daniel Cremers,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Laude_Discrete-Continuous_ADMM_for_CVPR_2018_paper.pdf


1021
Title: Multi-Scale Weighted Nuclear Norm Image Restoration
Type: Poster
Abstracts: A prominent property of natural images is that groups of similar patches within them tend to lie on low-dimensional subspaces. This property has been previously used for image denoising, with particularly notable success via weighted nuclear norm minimization (WNNM). In this paper, we extend the WNNM method into a general image restoration algorithm, capable of handling arbitrary degradations (e.g. blur, missing pixels, etc.). Our approach is based on a novel regularization term which simultaneously penalizes for high weighted nuclear norm values of all the patch groups in the image. Our regularizer is isolated from the data-term, thus enabling convenient treatment of arbitrary degradations. Furthermore, it exploits the fractal property of natural images, by accounting for patch similarities also across different scales of the image. We propose a variable splitting method for solving the resulting optimization problem. This leads to an algorithm that is quite different from `plug-and-play' techniques, which solve image-restoration problems using a sequence of denoising steps. As we verify through extensive experiments, our algorithm achieves state of the art results in deblurring and inpainting, outperforming even the recent deep net based methods.
Authors: Noam Yair, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yair_Multi-Scale_Weighted_Nuclear_CVPR_2018_paper.pdf


1022
Title: Finding beans in burgers: Deep semantic-visual embedding with localization
Type: Poster
Authors: Patrick Perez, Technicolor Research;Matthieu Cord, ;Louis Chevallier, technicolor;Martin Engilberge, technicolor;

1023
Title: FlipDial: A Generative Model for Two-Way Visual Dialogue
Type: Oral
Abstracts: We present FlipDial, a generative model for Visual Dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (1VD) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue (2VD), where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics.
Authors: Daniela Massiceti, University of Oxford;Siddharth Narayanaswamy, University of Oxford;Puneet Kumar Dokania, University of Oxford;Phil Torr, Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Massiceti_FlipDial_A_Generative_CVPR_2018_paper.pdf


1024
Title: FlipDial: A Generative Model for Two-Way Visual Dialogue
Type: Poster
Abstracts: We present FlipDial, a generative model for Visual Dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (1VD) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue (2VD), where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics.
Authors: Daniela Massiceti, University of Oxford;Siddharth Narayanaswamy, University of Oxford;Puneet Kumar Dokania, University of Oxford;Phil Torr, Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Massiceti_FlipDial_A_Generative_CVPR_2018_paper.pdf


1025
Title: Transparency by Design: Closing the Gap Between Performance and Interpretabilty in Visual Reasoning
Type: Poster
Authors: David Mascharka, MIT Lincoln Laboratory;Philip Tran, Planck Aerosystems;Ryan Soklaski, MIT Lincoln Laboratory;Arjun Majumdar, MIT Lincoln Laboratory;

1026
Title: Webly Supervised Learning Meets Zero-shot Learning: A Hybrid Approach for Fine-grained Classification
Type: Spotlight
Authors: Li Niu, Rice University;Ashok Veeraraghavan, Rice University;Ashutosh Sabharwal,;

1027
Title: Webly Supervised Learning Meets Zero-shot Learning: A Hybrid Approach for Fine-grained Classification
Type: Poster
Authors: Li Niu, Rice University;Ashok Veeraraghavan, Rice University;Ashutosh Sabharwal,;

1028
Title: Cross-modal Deep Variational Hand Pose Estimation
Type: Spotlight
Authors: Adrian Spurr, ETH Zurich;Jie Song, ETHZ;Seonwook Park, ETH Zurich;Otmar HIlliges, ETH Zurich;

1029
Title: Cross-modal Deep Variational Hand Pose Estimation
Type: Poster
Authors: Adrian Spurr, ETH Zurich;Jie Song, ETHZ;Seonwook Park, ETH Zurich;Otmar HIlliges, ETH Zurich;

1030
Title: Occlusion Aware Unsupervised Learning of Optical Flow
Type: Poster
Abstracts: It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper- vised learning. However, the performance of the unsuper- vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper- vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Espe- cially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.
Authors: Yang Wang, Baidu USA;Yi Yang, ;Zhenheng Yang, ;Liang Zhao, Baidu USA;Wei Xu,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.pdf


1031
Title: PAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing
Type: Poster
Authors: Dan Xu, ;Wanli Ouyang, The University of Sydney;Xiaogang Wang, Chinese University of Hong Kong;Nicu Sebe, University of Trento;

1032
Title: OATM: Occlusion Aware Template Matching by Consensus Set Maximization
Type: Oral
Abstracts: We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions.
Authors: Simon Korman, Weizmann Institute;Mark Milam, NGC;Stefano Soatto, UCLA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Korman_OATM_Occlusion_Aware_CVPR_2018_paper.pdf


1033
Title: OATM: Occlusion Aware Template Matching by Consensus Set Maximization
Type: Poster
Abstracts: We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions.
Authors: Simon Korman, Weizmann Institute;Mark Milam, NGC;Stefano Soatto, UCLA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Korman_OATM_Occlusion_Aware_CVPR_2018_paper.pdf


1034
Title: MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories and head poses
Type: Spotlight
Authors: Irtiza Hasan, University of Verona;Francesco Setti, ;Theodore Tsesmelis, ;Alessio Del Bue, Istituto Italiano di Tecnologia (IIT);Fabio Galasso, ;Marco Cristani, U. Verona;

1035
Title: MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories and head poses
Type: Poster
Authors: Irtiza Hasan, University of Verona;Francesco Setti, ;Theodore Tsesmelis, ;Alessio Del Bue, Istituto Italiano di Tecnologia (IIT);Fabio Galasso, ;Marco Cristani, U. Verona;

1036
Title: Fooling Vision and Language Models Despite Localization and Attention Mechanism
Type: Poster
Abstracts: Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., >90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.
Authors: Xiaojun Xu, Shanghai Jiao Tong University;Xinyun Chen, UC Berkeley;Chang Liu, UC Berkeley;Anna Rohrbach, UC Berkeley;UC Berkeley, USA;Dawn Song, UC Berkeley;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Fooling_Vision_and_CVPR_2018_paper.pdf


1037
Title: Learning Transferable Architectures for Scalable Image Recognition
Type: Spotlight
Abstracts: Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the "NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS -- a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.
Authors: Barret Zoph, Google;Vijay Vasudevan, Google;Jonathon Shlens, Google;Quoc Le, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf


1038
Title: Learning Transferable Architectures for Scalable Image Recognition
Type: Poster
Abstracts: Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the "NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS -- a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.
Authors: Barret Zoph, Google;Vijay Vasudevan, Google;Jonathon Shlens, Google;Quoc Le, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf


1039
Title: 4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications
Type: Poster
Abstracts: The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contain recordings of 180 subjects captured in four different sessions spanned over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database in various applications. The database will be made publicly available for research purposes.
Authors: Shiyang Cheng, Imperial College London;Irene Kotsia, Middlesex University London;Imperial College London, UK;Stefanos Zafeiriou, Imperial College London;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_4DFAB_A_Large_CVPR_2018_paper.pdf


1040
Title: An Efficient and Provable Approach for Mixture Proportion Estimation Using Linear Independence Assumption
Type: Poster
Abstracts: In this paper, we study the mixture proportion estimation (MPE) problem in a new setting: given samples from the mixture and the component distributions, we identify the proportions of the components in the mixture distribution. To address this problem, we make use of a linear independence assumption, i.e., the component distributions are independent from each other, which is much weaker than assumptions exploited in the previous MPE methods. Based on this assumption, we propose a method (1) that uniquely identifies the mixture proportions, (2) whose output provably converges to the optimal solution, and (3) that is computationally efficient. We show the superiority of the proposed method over the state-of-the-art methods in two applications including learning with label noise and semi-supervised learning on both synthetic and real-world datasets.
Authors: Xiyu Yu, The University of Sydney;Tongliang Liu, The University of Sydney;Mingming Gong, ;Kayhan Batmanghelich, University of Pittsburgh;Dacheng Tao, University of Sydney;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_An_Efficient_and_CVPR_2018_paper.pdf


1041
Title: MovieGraphs: Towards Understanding Human-Centric Situations from Videos
Type: Spotlight
Authors: Paul Vicol, University of Toronto;Makarand Tapaswi, University of Toronto;Llu?s Castrej?n, ;Sanja Fidler,;

1042
Title: MovieGraphs: Towards Understanding Human-Centric Situations from Videos
Type: Poster
Authors: Paul Vicol, University of Toronto;Makarand Tapaswi, University of Toronto;Llu?s Castrej?n, ;Sanja Fidler,;

1043
Title: Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration
Type: Poster
Abstracts: Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates.
Authors: Xinyuan Zhang, Duke University;Xin Yuan, Nokia Bell Labs;Lawrence Carin,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Nonlocal_Low-Rank_Tensor_CVPR_2018_paper.pdf


1044
Title: Hierarchical Recurrent Attention Networks for Structured Online Maps
Type: Poster
Abstracts: In this paper, we tackle the problem of online road network extraction from sparse 3D point clouds. Our method is inspired by how an annotator builds a lane graph, by first identifying how many lanes there are and then drawing each one in turn. We develop a hierarchical recurrent network that attends to initial regions of a lane boundary and traces them out completely by outputting a structured polyline. We also propose a novel differentiable loss function that measures the deviation of the edges of the ground truth polylines and their predictions. This is more suitable than distances on vertices, as there exists many ways to draw equivalent polylines. We demonstrate the effectiveness of our method on a 90 km stretch of highway, and show that we can recover the right topology 92% of the time.
Authors: Namdar Homayounfar, Uber ATG;Wei-Chiu Ma, MIT;Shrinidhi Kowshika Lakshmikanth, Uber ATG;Raquel Urtasun, University of Toronto;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf


1045
Title: Surface Networks
Type: Oral
Abstracts: We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs, namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the Laplacian operator. Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions --- this is in stark contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models emph{Surface Networks (SN)}. We prove that these models define shape representations that are stable to deformation and to discretization, and we demonstrate the efficiency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by SNs.
Authors: Ilya Kostrikov, NYU;Joan Bruna, New York University;Daniele Panozzo, NYU;Denis Zorin, NYU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kostrikov_Surface_Networks_CVPR_2018_paper.pdf


1046
Title: Surface Networks
Type: Poster
Abstracts: We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs, namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the Laplacian operator. Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric deformations, making it unsuitable for many applications. To overcome this limitation, we propose several upgrades to GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions --- this is in stark contrast with the classical Laplace operator, which directly measures mean curvature. We coin the resulting models emph{Surface Networks (SN)}. We prove that these models define shape representations that are stable to deformation and to discretization, and we demonstrate the efficiency and versatility of SNs on two challenging tasks: temporal prediction of mesh deformations under non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by SNs.
Authors: Ilya Kostrikov, NYU;Joan Bruna, New York University;Daniele Panozzo, NYU;Denis Zorin, NYU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kostrikov_Surface_Networks_CVPR_2018_paper.pdf


1047
Title: Deep Depth Completion of a Single RGB-D Image
Type: Spotlight
Abstracts: The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.
Authors: Yinda Zhang, Princeton;Thomas Funkhouser, Princeton;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Depth_Completion_CVPR_2018_paper.pdf


1048
Title: Deep Depth Completion of a Single RGB-D Image
Type: Poster
Abstracts: The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.
Authors: Yinda Zhang, Princeton;Thomas Funkhouser, Princeton;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Depth_Completion_CVPR_2018_paper.pdf


1049
Title: Convolutional Image Captioning
Type: Poster
Abstracts: Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the LSTM baseline, while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.
Authors: Jyoti Aneja, UIUC;Aditya Deshpande, University of Illinois at UC;Alex Schwing,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf


1050
Title: Geometric Multi-Model Fitting with a Convex Relaxation Algorithm
Type: Poster
Authors: Paul Amayo, Oxford ;Pedro Pinies, University of Oxford;Lina Paz, University of Oxford;Paul Newman, University of Oxford;

1051
Title: Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion
Type: Spotlight
Authors: Rushil Anirudh, Lawrence Livermore National La;Hyojin Kim, Lawrence Livermore National Laboratory;Jayaraman J. Thiagarajan, LLNL;K. Aditya Mohan, Lawrence Livermore National Laboratory;Kyle Champley, Lawrence Livermore National Laboratory;Timo Bremer, Lawrence Livermore National Laboratory;

1052
Title: Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion
Type: Poster
Authors: Rushil Anirudh, Lawrence Livermore National La;Hyojin Kim, Lawrence Livermore National Laboratory;Jayaraman J. Thiagarajan, LLNL;K. Aditya Mohan, Lawrence Livermore National Laboratory;Kyle Champley, Lawrence Livermore National Laboratory;Timo Bremer, Lawrence Livermore National Laboratory;

1053
Title: Now You Shake Me: Towards Automatic 4D Cinema
Type: Spotlight
Abstracts: We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone’s homes.
Authors: Yuhao Zhou, University of Toronto;Makarand Tapaswi, University of Toronto;Sanja Fidler,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Now_You_Shake_CVPR_2018_paper.pdf


1054
Title: Now You Shake Me: Towards Automatic 4D Cinema
Type: Poster
Abstracts: We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone’s homes.
Authors: Yuhao Zhou, University of Toronto;Makarand Tapaswi, University of Toronto;Sanja Fidler,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Now_You_Shake_CVPR_2018_paper.pdf


1055
Title: VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection
Type: Poster
Abstracts: Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.
Authors: Yin Zhou, Lawrence Berkeley National Lab;Oncel Tuzel,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf


1056
Title: Image to Image Translation for Domain Adaptation
Type: Poster
Abstracts: We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-to-image translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets.
Authors: Zak Murez, UCSD;HRL Laboratories, LLC;David Kriegman, University of California at San Diego;University of California, San Diego;Kyungnam Kim, HRL Laboratories;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Murez_Image_to_Image_CVPR_2018_paper.pdf


1057
Title: Learning-Compression algorithms for neural net pruning
Type: Spotlight
Authors: Miguel Carreira-perpinan, UC Merced;Yerlan Idelbayev, UC Merced;

1058
Title: Learning-Compression algorithms for neural net pruning
Type: Poster
Authors: Miguel Carreira-perpinan, UC Merced;Yerlan Idelbayev, UC Merced;

1059
Title: Reinforcement Cutting-Agent Learning for Video Object Segmentation
Type: Spotlight
Abstracts: Video object segmentation is a fundamental yet challenging task in computer vision community. In this paper, we formulate this problem as a Markov Decision Process, where agents are learned to segment object regions under a deep reinforcement learning framework. Essentially, learning agents for segmentation is nontrivial as segmentation is a nearly continuous decision-making process, where the number of the involved agents (pixels or superpixels) and action steps from the seed (super)pixels to the whole object mask might be incredibly huge. To overcome this difficulty, this paper simplifies the learning of segmentation agents to the learning of a cutting-agent, which only has a limited number of action units and can converge in just a few action steps. The basic assumption is that object segmentation mainly relies on the interaction between object regions and their context. Thus, with an optimal object (box) region and context (box) region, we can obtain the desirable segmentation mask through further inference. Based on this assumption, we establish a novel reinforcement cutting-agent learning framework, where the cutting-agent consists of a cutting-policy network and a cutting-execution network. The former learns policies for deciding optimal object-context box pair, while the latter executing the cutting function based on the inferred object-context box pair. With the collaborative interaction between the two networks, our method can achieve the outperforming VOS performance on two public benchmarks, which demonstrates the rationality of our assumption as well as the effectiveness of the proposed learning framework.
Authors: Junwei Han, Northwestern Polytechnical U.;Le Yang, Northwestern Polytechnical Uni;Dingwen Zhang, ;Xiaojun Chang, Carnegie Mellon University;Xiaodan Liang, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Reinforcement_Cutting-Agent_Learning_CVPR_2018_paper.pdf


1060
Title: Reinforcement Cutting-Agent Learning for Video Object Segmentation
Type: Poster
Abstracts: Video object segmentation is a fundamental yet challenging task in computer vision community. In this paper, we formulate this problem as a Markov Decision Process, where agents are learned to segment object regions under a deep reinforcement learning framework. Essentially, learning agents for segmentation is nontrivial as segmentation is a nearly continuous decision-making process, where the number of the involved agents (pixels or superpixels) and action steps from the seed (super)pixels to the whole object mask might be incredibly huge. To overcome this difficulty, this paper simplifies the learning of segmentation agents to the learning of a cutting-agent, which only has a limited number of action units and can converge in just a few action steps. The basic assumption is that object segmentation mainly relies on the interaction between object regions and their context. Thus, with an optimal object (box) region and context (box) region, we can obtain the desirable segmentation mask through further inference. Based on this assumption, we establish a novel reinforcement cutting-agent learning framework, where the cutting-agent consists of a cutting-policy network and a cutting-execution network. The former learns policies for deciding optimal object-context box pair, while the latter executing the cutting function based on the inferred object-context box pair. With the collaborative interaction between the two networks, our method can achieve the outperforming VOS performance on two public benchmarks, which demonstrates the rationality of our assumption as well as the effectiveness of the proposed learning framework.
Authors: Junwei Han, Northwestern Polytechnical U.;Le Yang, Northwestern Polytechnical Uni;Dingwen Zhang, ;Xiaojun Chang, Carnegie Mellon University;Xiaodan Liang, Carnegie Mellon University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Reinforcement_Cutting-Agent_Learning_CVPR_2018_paper.pdf


1061
Title: CNN Driven Sparse Multi-Level B-spline Image Registration
Type: Poster
Authors: Pingge Jiang, Drexel University;James Shackleford, Drexel University;

1062
Title: DocUNet: Document Image Unwarping via A Stacked U-Net
Type: Poster
Authors: Ke Ma, Stony Brook University;Zhixin Shu, Stony Brook University;Xue Bai, Megvii Inc;Jue Wang, Megvii;Dimitris Samaras,;

1063
Title: Texture Mapping for 3D Reconstruction with RGB-D Sensor
Type: Poster
Authors: Yanping Fu, WuHan University;Qingan Yan, JD.com;Long Yang, Northwest A&F University;Jie Liao , WuHan University;Chunxia Xiao, Wuhan University;

1064
Title: Sliced Wasserstein Distance for Learning Gaussian Mixture Models
Type: Poster
Abstracts: Gaussian mixture models (GMM) are powerful parametric tools with many applications in machine learning and computer vision. Expectation maximization (EM) is the most popular algorithm for estimating the GMM parameters. However, EM guarantees only convergence to a stationary point of the log-likelihood function, which could be arbitrarily worse than the optimal solution. Inspired by the relationship between the negative log-likelihood function and the Kullback-Leibler (KL) divergence, we propose an alternative formulation for estimating the GMM parameters using the sliced Wasserstein distance, which gives rise to a new algorithm. Specifically, we propose minimizing the sliced-Wasserstein distance between the mixture model and the data distribution with respect to the GMM parameters. In contrast to the KL-divergence, the energy landscape for the sliced-Wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal GMM parameters. We show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the EM algorithm.
Authors: HRL Laboratories, LLC;Gustavo Rohde, University Virginia ;HRL Laboratories, LLC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kolouri_Sliced_Wasserstein_Distance_CVPR_2018_paper.pdf


1065
Title: Convolutional Sequence to Sequence Model for Human Dynamics
Type: Poster
Abstracts: Human motion modeling is a classic problem in com- puter vision and graphics. Challenges in modeling human motion include high dimensional prediction as well as extremely complicated dynamics.We present a novel approach to human motion modeling based on convolutional neural networks (CNN). The hierarchical structure of CNN makes it capable of capturing both spatial and temporal correlations effectively. In our proposed approach, a convolutional long-term encoder is used to encode the whole given motion sequence into a long-term hidden variable, which is used with a decoder to predict the remainder of the sequence. The decoder itself also has an encoder-decoder structure, in which the short-term encoder encodes a shorter sequence to a short-term hidden variable, and the spatial decoder maps the long and short-term hidden variable to motion predictions. By using such a model, we are able to capture both invariant and dynamic information of human motion, which results in more accurate predictions. Experiments show that our algorithm outperforms the state-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our code is available at the project website
Authors: Chen Li, ;Zhen Zhang, National University of Singapore;Wee Sun Lee, ;Gim Hee Lee, National Univeristy of Singapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Convolutional_Sequence_to_CVPR_2018_paper.pdf


1066
Title: Composing Two Objects of Interest for Flying Camera Photography
Type: Poster
Authors: ZIQUAN LAN, NUS;David Hsu, NUS;Gim Hee Lee, National University of SIngapore;

1067
Title: Time-resolved Light Transport Decomposition for Thermal Photometric Stereo
Type: Poster
Authors: Nobuhiro Ikeya, NAIST;Kenichiro Tanaka, NAIST;Tsuyoshi Takatani, NAIST;Hiroyuki Kubo, ;Takuya Funatomi, NAIST;Yasuhiro Mukaigawa, NAIST;

1068
Title: Large-scale Distance Metric Learning with Uncertainty
Type: Spotlight
Authors: Qi Qian, Alibaba Group;Shenghuo Zhu, Alibaba Group;Rong Jin, Alibaba Group;Jiasheng Tang, Alibaba Group;Hao Li, Alibaba Group;

1069
Title: Large-scale Distance Metric Learning with Uncertainty
Type: Poster
Authors: Qi Qian, Alibaba Group;Shenghuo Zhu, Alibaba Group;Rong Jin, Alibaba Group;Jiasheng Tang, Alibaba Group;Hao Li, Alibaba Group;

1070
Title: Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation
Type: Poster
Abstracts: Domain shift, which occurs when there is a mismatch between the distributions of training (source) and testing (target) datasets, usually results in poor performance of the trained model on the target domain. Existing algorithms typically solve this issue by reducing the distribution discrepancy in the input spaces. However, for kernel-based learning machines, performance highly depends on the statistical properties of data in reproducing kernel Hilbert spaces (RKHS). Motivated by these considerations, we propose a novel strategy for matching distributions in RKHS, which is done by aligning the RKHS covariance matrices (descriptors) across domains. This strategy is a generalization of the correlation alignment problem in Euclidean spaces to (potentially) infinite-dimensional feature spaces. In this paper, we provide two alignment approaches, for both of which we obtain closed-form expressions via kernel matrices. Furthermore, our approaches are scalable to large datasets since they can naturally handle out-of-sample instances. We conduct extensive experiments (248 domain adaptation tasks) to evaluate our approaches. Experiment results show that our approaches outperform other state-of-the-art methods in both accuracy and computationally efficiency.
Authors: Zhen Zhang, WASHINGTON UNIVERSITY IN ST.LO;Mianzhi Wang, WASHINGTON UNIVERSITY IN ST.LOUIS;Yan Huang, ;Arye Nehorai, WASHINGTON UNIVERSITY IN ST.LOUIS;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Aligning_Infinite-Dimensional_Covariance_CVPR_2018_paper.pdf


1071
Title: R-FCN-3000 at 30fps: Decoupling Detection and Classification
Type: Poster
Abstracts: We propose a modular approach towards large-scale real-time object detection by decoupling objectness detection and classification. We exploit the fact that many object classes are visually similar and share parts. Thus, a universal objectness detector can be learned for class-agnostic object detection followed by fine-grained classification using a (non)linear classifier. Our approach is a modification of the R-FCN architecture to learn shared filters for performing localization across different object classes. We trained a detector for 3000 object classes, called R-FCN-3000, that obtains an mAP of 34.9% on the ImageNet detection dataset. It outperforms YOLO-9000 by 18% while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector.
Authors: Bharat Singh, ;Hengduo Li, ;Abhishek Sharma, ;University of Maryland, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_R-FCN-3000_at_30fps_CVPR_2018_paper.pdf


1072
Title: Distributable Consistent Multi-Graph Matching
Type: Spotlight
Authors: Nan Hu, Stanford Unviversity;Boris Thibert, ;Leonidas J. Guibas,;

1073
Title: Distributable Consistent Multi-Graph Matching
Type: Poster
Authors: Nan Hu, Stanford Unviversity;Boris Thibert, ;Leonidas J. Guibas,;

1074
Title: VirtualHome: Simulating Household Activities via Programs
Type: Oral
Abstracts: In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language
Authors: Xavier Puig, MIT;Kevin Ra, ;Marko Boben, ;Jiaman Li, University of Toronto;Tingwu Wang, ;Sanja Fidler, ;Antonio Torralba, MIT;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf


1075
Title: VirtualHome: Simulating Household Activities via Programs
Type: Poster
Abstracts: In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to "drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language
Authors: Xavier Puig, MIT;Kevin Ra, ;Marko Boben, ;Jiaman Li, University of Toronto;Tingwu Wang, ;Sanja Fidler, ;Antonio Torralba, MIT;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf


1076
Title: Robust Physical-World Attacks on Deep Learning Visual Classification
Type: Poster
Abstracts: Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP 2 ), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP 2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.
Authors: Ivan Evtimov, University of Washington;Kevin Eykholt, University of Michigan;Earlence Fernandes, University of Washington;Tadayoshi Kohno, University of Washington;Bo Li, UC Berkeley;Atul Prakash, University of Michigan;Amir Rahmati, University of Michigan;Chaowei Xiao, University of Michigan;Dawn Song, UC Berkeley;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.pdf


1077
Title: Feature Super-Resolution: Make Machine See More Clearly
Type: Poster
Abstracts: Identifying small size images or small objects is a notoriously challenging problem, as discriminative representations are difficult to learn from the limited information contained in them with poor-quality appearance and unclear object structure. Existing research works usually increase the resolution of low-resolution image in the pixel space in order to provide better visual quality for human viewing. However, the improved performance of such methods is usually limited or even trivial in the case of very small image size (we will show it in this paper explicitly). In this paper, different from image super-resolution (ISR), we propose a novel super-resolution technique called feature super-resolution (FSR), which aims at enhancing the discriminatory power of small size image in order to provide high recognition precision for machine. To achieve this goal, we propose a new Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model that transforms the raw poor features of small size images to highly discriminative ones by performing super-resolution in the feature space. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. By training the G and the D networks in an alternative manner, we encourage the G network to discover the latent distribution correlations between small size and large size images and then use G to improve the representations of small images. Extensive experiment results on Oxford5K, Paris, Holidays, and Flick100k datasets demonstrate that the proposed FSR approach can effectively enhance the discriminatory ability of features. Even when the resolution of query images is reduced greatly, e.g., 1/64 original size, the query feature enhanced by our FSR approach achieves surprisingly high retrieval performance at different image resolutions and increases the retrieval precision by 25% compared to the raw query feature.
Authors: Weimin Tan, Fudan University;Bo Yan, Fudan University;Bahetiyaer Bare, Fudan University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tan_Feature_Super-Resolution_Make_CVPR_2018_paper.pdf


1078
Title: Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++
Type: Poster
Authors: David Acuna, University of Toronto;Huan Ling, UofT;Amlan Kar, University of Toronto;Sanja Fidler,;

1079
Title: CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition
Type: Poster
Abstracts: This work addresses the novel problem of one-shot one-class classification. The goal is to estimate a classification decision boundary for a novel class based on a single image example. Our method exploits transfer learning to model the transformation from a representation of the input, extracted by a Convolutional Neural Network, to a classification decision boundary. We use a deep neural network to learn this transformation from a large labelled dataset of images and their associated class decision boundaries generated from ImageNet, and then apply the learned decision boundary to classify subsequent query images. We tested our approach on several benchmark datasets and significantly outperformed the baseline methods.
Authors: Jedrzej Kozerawski, UCSB;Matthew Turk, UC Santa Barbara USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kozerawski_CLEAR_Cumulative_LEARning_CVPR_2018_paper.pdf


1080
Title: Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification
Type: Poster
Authors: Mark Sandler, Google;Andrew Howard, Google;Menglong Zhu, ;Andrey Zhmoginov, Google;Liang-Chieh Chen,;

1081
Title: Learning Nested Structures in Deep Neural Networks
Type: Spotlight
Authors: Eunwoo Kim, Seoul National University;Chanho Ahn, Seoul National University;Songhwai Oh, Seoul National University;

1082
Title: Learning Nested Structures in Deep Neural Networks
Type: Poster
Authors: Eunwoo Kim, Seoul National University;Chanho Ahn, Seoul National University;Songhwai Oh, Seoul National University;

1083
Title: CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise
Type: Poster
Authors: Kuang-Huei Lee, Microsoft;Xiaodong He, ;Lei Zhang, Microsoft;Linjun Yang, Facebook;

1084
Title: Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN
Type: Poster
Authors: Shuai Li, University of Wollongong;Wanqing Li, ;Chris Cook, University of Wollongong;Ce Zhu, University of Electronic Science and Technology of China;Yanbo Gao, University of Electronic Science and Technology of China;

1085
Title: Lions and Tigers and Bears: Capturing Non-Rigid
Type: Spotlight
Authors: Silvia Zuffi, IMATI-CNR;Angjoo Kanazawa, University of Maryland;Michael Black, Max Planck Institute for Intelligent Systems;

1086
Title: Lions and Tigers and Bears: Capturing Non-Rigid
Type: Poster
Authors: Silvia Zuffi, IMATI-CNR;Angjoo Kanazawa, University of Maryland;Michael Black, Max Planck Institute for Intelligent Systems;

1087
Title: MoNet: Moments Embedding Network
Type: Poster
Abstracts: Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks. Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion. However, a serious drawback of this family of pooling layers is their dimensionality explosion. Approximate pooling methods with compact properties have been explored towards resolving this weakness. Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information. However, combining compact pooling with matrix normalization and other order information has not been explored until now. In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix. In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods. Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet. Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.
Authors: Mengran Gou, Northeastern University;Fei Xiong, University of Southern California ;Northeastern University, USA;Mario Sznaier,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Gou_MoNet_Moments_Embedding_CVPR_2018_paper.pdf


1088
Title: Self-calibrating polarising radiometric calibration
Type: Poster
Authors: Daniel Teo, SUTD;Boxin Shi, Peking University;National Institute of Informatics, Japan;Sai-Kit Yeung,;

1089
Title: Representing and Learning High Dimensional Data with the Optimal Transport Map from a Probabilistic Viewpoint
Type: Poster
Authors: Serim Park, Oath;Matthew Thorpe,;

1090
Title: Differential Attention for Visual Question Answering
Type: Poster
Abstracts: In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions.
Authors: Badri Patro, IIT Kanpur;Vinay P. Namboodiri, Indian Institute of Technology Kanpur;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Patro_Differential_Attention_for_CVPR_2018_paper.pdf


1091
Title: Deep Ordinal Regression Network for Monocular Depth Estimation
Type: Poster
Abstracts: Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob- lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi- layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, i.e., KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin.
Authors: Huan Fu, The University of Sydney;Mingming Gong, ;Chaohui Wang, Universit? Paris-Est;Kayhan Batmanghelich, University of Pittsburgh;Dacheng Tao, University of Sydney;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.pdf


1092
Title: ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information
Type: Poster
Abstracts: Object detection in wide area motion imagery (WAMI) has drawn the attention of the computer vision research community for a number of years. WAMI proposes a number of unique challenges including extremely small object sizes, both sparse and densely-packed objects, and extremely large search spaces (large video frames). Nearly all state-of-the-art methods in WAMI object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing. In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection. To reduce the large search space, the first stage (ClusterNet) takes in a set of extremely large video frames, combines the motion and appearance information within the convolutional architecture, and proposes regions of objects of interest (ROOBI). These ROOBI can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in WAMI. The second stage (FoveaNet) then estimates the centroid location of all objects in that given ROOBI simultaneously via heatmap estimation. The proposed method exceeds state-of-the-art results on the WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped objects, as well as being the first proposed method in wide area motion imagery to detect completely stationary objects.
Authors: Rodney LaLonde, University of Central Florida;Dong Zhang, University of Central Florida;Mubarak Shah, UCF;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/LaLonde_ClusterNet_Detecting_Small_CVPR_2018_paper.pdf


1093
Title: Seeing Small Faces from Robust Anchor's Perspective
Type: Poster
Authors: Chenchen Zhu, Carnegie Mellon University;Ran Tao, Carnegie Mellon University;Khoa Luu, ;Marios Savvides,;

1094
Title: Gesture Recognition: Focus on the Hands
Type: Poster
Abstracts: Gestures are a common form of human communication and important for human computer interfaces (HCI). Recent approaches to gesture recognition use deep learning methods, including multi-channel methods. We show that when spatial channels are focused on the hands, gesture recognition improves significantly, particularly when the channels are fused using a sparse network. Using this technique, we improve performance on the ChaLearn IsoGD dataset from a previous best of 67.71% to 82.07%, and on the NVIDIA dataset from 83.8% to 91.28%.
Authors: Pradyumna Narayana, Colorado State University;Ross Beveridge, Colorado State University;Bruce Draper, Colorado State University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Narayana_Gesture_Recognition_Focus_CVPR_2018_paper.pdf


1095
Title: A Common Framework for Interactive Texture Transfer
Type: Spotlight
Abstracts: In this paper, we present a general-purpose solution to interactive texture transfer problems that better preserves both local structure and visual richness. It is challenging due to the diversity of tasks and the simplicity of required user guidance. The core idea of our common framework is to use multiple custom channels to dynamically guide the synthesis process. For interactivity, users can control the spatial distribution of stylized textures via semantic channels. The structure guidance, acquired by two stages of automatic extraction and propagation of structure information, provides a prior for initialization and preserves the salient structure by searching the nearest neighbor fields (NNF) with structure coherence. Meanwhile, texture coherence is also exploited to maintain similar style with the source image. In addition, we leverage an improved PatchMatch with extended NNF and matrix operations to obtain transformable source patches with richer geometric information at high speed. We demonstrate the effectiveness and superiority of our method on a variety of scenes through extensive comparisons with state-of-the-art algorithms.
Authors: Yifang Men, Peking University;Zhouhui Lian, ;Jianguo Xiao, Peking University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Men_A_Common_Framework_CVPR_2018_paper.pdf


1096
Title: A Common Framework for Interactive Texture Transfer
Type: Poster
Abstracts: In this paper, we present a general-purpose solution to interactive texture transfer problems that better preserves both local structure and visual richness. It is challenging due to the diversity of tasks and the simplicity of required user guidance. The core idea of our common framework is to use multiple custom channels to dynamically guide the synthesis process. For interactivity, users can control the spatial distribution of stylized textures via semantic channels. The structure guidance, acquired by two stages of automatic extraction and propagation of structure information, provides a prior for initialization and preserves the salient structure by searching the nearest neighbor fields (NNF) with structure coherence. Meanwhile, texture coherence is also exploited to maintain similar style with the source image. In addition, we leverage an improved PatchMatch with extended NNF and matrix operations to obtain transformable source patches with richer geometric information at high speed. We demonstrate the effectiveness and superiority of our method on a variety of scenes through extensive comparisons with state-of-the-art algorithms.
Authors: Yifang Men, Peking University;Zhouhui Lian, ;Jianguo Xiao, Peking University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Men_A_Common_Framework_CVPR_2018_paper.pdf


1097
Title: PieAPP: Perceptual Image-Error Assessment through Pairwise Preference
Type: Poster
Authors: Ekta Prashnani, UCSB;University of California, Santa Barbara;Yasamin Mostofi, UCSB;University of California, Santa Barbara;

1098
Title: Guide Me: Interacting with Deep Networks
Type: Spotlight
Authors: Christian Rupprecht, Technische Unitversit?t M?nchen;Iro Laina, ;Federico Tombari, ;Nassir Navab, Technical University of Munich;Gregory Hager, Johns Hopkins University;

1099
Title: Guide Me: Interacting with Deep Networks
Type: Poster
Authors: Christian Rupprecht, Technische Unitversit?t M?nchen;Iro Laina, ;Federico Tombari, ;Nassir Navab, Technical University of Munich;Gregory Hager, Johns Hopkins University;

1100
Title: Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation
Type: Spotlight
Abstracts: Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.
Authors: Dan Xu, ;Wei Wang, University of Trento;Hao Tang, University of Trento;Nicu Sebe, University of Trento;Elisa Ricci, U. Perugia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Structured_Attention_Guided_CVPR_2018_paper.pdf


1101
Title: Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation
Type: Poster
Abstracts: Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.
Authors: Dan Xu, ;Wei Wang, University of Trento;Hao Tang, University of Trento;Nicu Sebe, University of Trento;Elisa Ricci, U. Perugia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Structured_Attention_Guided_CVPR_2018_paper.pdf


1102
Title: FFNet: Video Fast-Forwarding via Reinforcement Learning
Type: Poster
Abstracts: For many intelligent applications with limited computation, communication, storage and energy resources, there is an imperative need of vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a "trailer" for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video (about 6%-20% improvement on coverage of important frames) with much less processing requirement (more than 80% reduction in the number of frames processed).
Authors: Shuyue Lan, Northwestern University;Rameswar Panda, UC Riverside;Qi Zhu, UC Riverside;Amit Roy-Chowdhury, UC Riverside;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lan_FFNet_Video_Fast-Forwarding_CVPR_2018_paper.pdf


1103
Title: Two can play this Game: Visual Dialog with Discriminative Visual Question Generation and Visual Question Answering
Type: Poster
Authors: Unnat Jain, UIUC;Lana Lazebnik, ;Alex Schwing,;

1104
Title: A Prior-Less Method for Multi-Face Tracking in Unconstrained Videos
Type: Poster
Abstracts: This paper presents a prior-less method for tracking and clustering an unknown number of human faces and maintaining their individual identities in unconstrained videos. The key challenge is to accurately track faces with partial occlusion and drastic appearance changes in multiple shots resulting from significant variations of makeup, facial expression, head pose and illumination. To address this challenge, we propose a new multi-face tracking and re-identification algorithm, which provides high accuracy in face association in the entire video with automatic cluster number generation, and is robust to outliers. We develop a co-occurrence model of multiple body parts to seamlessly create face tracklets, and recursively link tracklets to construct a graph for extracting clusters. A Gaussian Process model is introduced to compensate the deep feature insufficiency, and is further used to refine the linking results. The advantages of the proposed algorithm are demonstrated using a variety of challenging music videos and newly introduced body-worn camera videos. The proposed method obtains significant improvements over the state of the art [51], while relying less on handling video-specific prior information to achieve high performance.
Authors: CHUNG-CHING LIN, IBM Research;Ying Hung, Rutgers University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_A_Prior-Less_Method_CVPR_2018_paper.pdf


1105
Title: Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras
Type: Poster
Abstracts: Vanishing points and vanishing lines are classical geometrical concepts in perspective cameras that have a lineage dating back to 3 centuries. A vanishing point is a point on the image space where parallel lines in 3D space appear to converge, whereas a vanishing line passes through 2 or more vanishing points. While such concepts are simple and intuitive in perspective cameras, their counterparts in catadioptric cameras (obtained using mirrors and lenses) are more involved. For example, lines in the 3D space map to higher degree curves in catadioptric cameras. The projection of a set of 3D parallel lines converges on a single point in perspective images, whereas they converge to more than one point in catadioptric cameras. To the best of our knowledge, we are not aware of any systematic development of analytical models for vanishing points and vanishing curves in different types of catadioptric cameras. In this paper, we derive parametric equations for vanishing points and vanishing curves using the calibration parameters, mirror shape coefficients, and direction vectors of parallel lines in 3D space. We show compelling experimental results on vanishing point estimation and absolute pose estimation for a wide variety of catadioptric cameras in both simulations and real experiments.
Authors: Instituto Superior T?cnico, Lisboa;Francisco Girbal Eiras, University of Oxford;Srikumar Ramalingam,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Miraldo_Analytical_Modeling_of_CVPR_2018_paper.pdf


1106
Title: Egocentric Activity Recognition on a Budget
Type: Oral
Abstracts: Recent advances in embedded technology have enabled more pervasive machine learning. One of the common applications in this field is Egocentric Activity Recognition (EAR), where users wearing a device such as a smartphone or smartglasses are able to receive feedback from the embedded device. Recent research on activity recognition has mainly focused on improving accuracy by using resource intensive techniques such as multi-stream deep networks. Although this approach has provided state-of-the-art results, in most cases it neglects the natural resource constraints (e.g. battery) of wearable devices. We develop a Reinforcement Learning model-free method to learn energy-aware policies that maximize the use of low-energy cost predictors while keeping competitive accuracy levels. Our results show that a policy trained on an egocentric dataset is able use the synergy between motion sensors and vision to effectively tradeoff energy expenditure and accuracy on smartglasses operating in realistic, real-world conditions.
Authors: Rafael Possas, University of Sydney;Sheila Maricela Pinto Caceres, University of Sydney;Fabio Ramos, University of Sydney;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.pdf


1107
Title: Egocentric Activity Recognition on a Budget
Type: Poster
Abstracts: Recent advances in embedded technology have enabled more pervasive machine learning. One of the common applications in this field is Egocentric Activity Recognition (EAR), where users wearing a device such as a smartphone or smartglasses are able to receive feedback from the embedded device. Recent research on activity recognition has mainly focused on improving accuracy by using resource intensive techniques such as multi-stream deep networks. Although this approach has provided state-of-the-art results, in most cases it neglects the natural resource constraints (e.g. battery) of wearable devices. We develop a Reinforcement Learning model-free method to learn energy-aware policies that maximize the use of low-energy cost predictors while keeping competitive accuracy levels. Our results show that a policy trained on an egocentric dataset is able use the synergy between motion sensors and vision to effectively tradeoff energy expenditure and accuracy on smartglasses operating in realistic, real-world conditions.
Authors: Rafael Possas, University of Sydney;Sheila Maricela Pinto Caceres, University of Sydney;Fabio Ramos, University of Sydney;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.pdf


1108
Title: TextureGAN: Controlling Deep Image Synthesis with Texture Patches
Type: Spotlight
Authors: Wenqi Xian, ;Patsorn Sangkloy, Georgia Institute of Technology;Varun Agrawal, ;Amit Raj, Georgia Institute of Technolog;Jingwan Lu, Adobe Research;Chen Fang, Adobe Research;Fisher Yu, UC Berkeley;James Hays, Georgia Tech;

1109
Title: TextureGAN: Controlling Deep Image Synthesis with Texture Patches
Type: Poster
Authors: Wenqi Xian, ;Patsorn Sangkloy, Georgia Institute of Technology;Varun Agrawal, ;Amit Raj, Georgia Institute of Technolog;Jingwan Lu, Adobe Research;Chen Fang, Adobe Research;Fisher Yu, UC Berkeley;James Hays, Georgia Tech;

1110
Title: Rethinking Feature Distribution for Loss Functions in Image Classification
Type: Spotlight
Abstracts: We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.
Authors: Weitao Wan, Tsinghua University;Yuanyi Zhong, UIUC;Tianpeng Li, Tsinghua University;Jiansheng Chen, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Rethinking_Feature_Distribution_CVPR_2018_paper.pdf


1111
Title: Rethinking Feature Distribution for Loss Functions in Image Classification
Type: Poster
Abstracts: We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.
Authors: Weitao Wan, Tsinghua University;Yuanyi Zhong, UIUC;Tianpeng Li, Tsinghua University;Jiansheng Chen, Tsinghua University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Rethinking_Feature_Distribution_CVPR_2018_paper.pdf


1112
Title: Coding Kendall's Shape Trajectories for 3D Action Recognition
Type: Poster
Abstracts: Suitable shape representations as well as their temporal evolution, termed trajectories, often lie to non-linear manifolds. This puts an additional constraint (i.e., non-linearity) in using conventional machine learning techniques for the purpose of classification, event detection, prediction, etc. This paper accommodates the well-known Sparse Coding and Dictionary Learning to the Kendall's shape space and illustrates effective coding of 3D skeletal sequences for action recognition. Grounding on the Riemannian geometry of the shape space, an intrinsic sparse coding and dictionary learning formulation is proposed for static skeletal shapes to overcome the inherent non-linearity of the manifold. As a main result, initial trajectories give rise to sparse code functions with suitable computational properties, including sparsity and vector space representation. To achieve action recognition, two different classification schemes were adopted. A bi-directional LSTM is directly performed on sparse code functions, while a linear SVM is applied after representing sparse code functions using Fourier temporal pyramid. Experiments conducted on three publicly available datasets show the superiority of the proposed approach compared to existing Riemannian representations and its competitiveness with respect to other recently-proposed approaches. When the benefits of invariance are maintained from the Kendall's shape representation, our approach not only overcomes the problem of non-linearity but also yields to discriminative sparse code functions.
Authors: Amor Ben Tanfous, IMT Lille Douai;Hassen Drira, IMT Lille Douai;Boulbaba Ben Amor, IMT Lille Douai;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tanfous_Coding_Kendalls_Shape_CVPR_2018_paper.pdf


1113
Title: Latent RANSAC
Type: Poster
Abstracts: We present a method that can evaluate a RANSAC hypothesis in constant time, i.e. independent of the size of the data. A key observation here is that correct hypotheses are tightly clustered together in the latent parameter domain. In a manner similar to the generalized Hough transform we seek to find this cluster, only that we need as few as two votes for a successful detection. Rapidly locating such pairs of similar hypotheses is made possible by adapting the recent "Random Grids" range-search technique. We only perform the usual (costly) hypothesis verification stage upon the discovery of a close pair of hypotheses. We show that this event rarely happens for incorrect hypotheses, enabling a significant speedup of the RANSAC pipeline. The suggested approach is applied and tested on three robust estimation problems: camera localization, 3D rigid alignment and 2D-homography estimation. We perform rigorous testing on both synthetic and real datasets, demonstrating an improvement in efficiency without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D alignment results on the challenging ``Redwood'' loop-closure challenge.
Authors: Simon Korman, Weizmann Institute;Roee Litman, Tel-Aviv University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Korman_Latent_RANSAC_CVPR_2018_paper.pdf


1114
Title: Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images
Type: Spotlight
Abstracts: Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a "redaction by segmentation" paradigm. Hence, we propose the first sizable dataset of private images "in the wild" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information. It is effective at achieving various privacy-utility trade-offs within 83% of the performance of redactions based on ground-truth annotation.
Authors: Tribhuvanesh Orekondy, MPI-INF;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Orekondy_Connecting_Pixels_to_CVPR_2018_paper.pdf


1115
Title: Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images
Type: Poster
Abstracts: Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a "redaction by segmentation" paradigm. Hence, we propose the first sizable dataset of private images "in the wild" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information. It is effective at achieving various privacy-utility trade-offs within 83% of the performance of redactions based on ground-truth annotation.
Authors: Tribhuvanesh Orekondy, MPI-INF;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Orekondy_Connecting_Pixels_to_CVPR_2018_paper.pdf


1116
Title: Boosting Domain Adaptation by Discovering Latent Domains
Type: Spotlight
Abstracts: Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, i.e. latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin.
Authors: Massimiliano Mancini, Sapienza University of Rome;Lorenzo Porzi, Mapillary Research;Samuel Rota Bul?, Mapillary Research;University of Rome La Sapienza, Italy;Elisa Ricci, U. Perugia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.pdf


1117
Title: Boosting Domain Adaptation by Discovering Latent Domains
Type: Poster
Abstracts: Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, i.e. latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin.
Authors: Massimiliano Mancini, Sapienza University of Rome;Lorenzo Porzi, Mapillary Research;Samuel Rota Bul?, Mapillary Research;University of Rome La Sapienza, Italy;Elisa Ricci, U. Perugia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.pdf


1118
Title: Fast and Robust Estimation for Unit-Norm Constrained Linear Fitting Problems
Type: Poster
Abstracts: M-estimator using iteratively reweighted least squares (IRLS) is one of the best-known methods for robust estimation. However, IRLS is ineffective for robust unit-norm constrained linear fitting (UCLF) problems, such as fundamental matrix estimation because of a poor initial solution. We overcome this problem by developing a novel objective function and its optimization, named iteratively reweighted eigenvalues minimization (IREM). IREM is guaranteed to decrease the objective function and achieves fast convergence and high robustness. In robust fundamental matrix estimation, IREM performs approximately 5-500 times faster than random sampling consensus (RANSAC) while preserving comparable or superior robustness.
Authors: Daiki Ikami, The University of Tokyo;Toshihiko Yamasaki, The University of Tokyo;Kiyoharu Aizawa,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ikami_Fast_and_Robust_CVPR_2018_paper.pdf


1119
Title: Local and Global Optimization Techniques in Graph-based Clustering
Type: Poster
Authors: Daiki Ikami, The University of Tokyo;Toshihiko Yamasaki, The University of Tokyo;Kiyoharu Aizawa,;

1120
Title: Generating a Fusion Image: One' s Identity and Another's Shape
Type: Poster
Authors: DongGyu Joo, KAIST;Doyeon Kim, KAIST;Junmo Kim, KAIST;

1121
Title: Categorizing Concepts with Basic Level for Vision-to-Language
Type: Poster
Authors: Hanzhang Wang, Tongji University;Hanli Wang, Tongji University;Kaisheng Xu, Tongji University;

1122
Title: Importance Weighted Adversarial Nets for Partial Domain Adaptation
Type: Poster
Abstracts: This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.
Authors: Jing Zhang, University of Wollongong;Zewei Ding, University of Wollongong;Wanqing Li, ;Philip Ogunbona, University of Wollongong;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.pdf


1123
Title: AON: Towards Arbitrarily-Oriented Text Recognition
Type: Poster
Abstracts: Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.
Authors: Zhanzhan Cheng, Hikvision Research Institute;Yangliu Xu, Tongji University;Fan Bai, Fudan University;Yi Niu, Hikvision Research Institute;Shiliang Pu, ;Shuigeng Zhou, Fudan University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.pdf


1124
Title: Towards dense object tracking in a 2D honeybee hive
Type: Poster
Authors: Katarzyna Bozek, Okinawa Institute of Science a;Laetitia Hebert, ;Alexander Mikheyev, ;Greg Stephens, OIST Graduate University and Vrije Universiteit Amsterdam;

1125
Title: Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering
Type: Oral
Abstracts: A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.
Authors: Nguyen Duy Kien, Tohoku University;Takayuki Okatani, Tohoku University/RIKEN AIP;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Improved_Fusion_of_CVPR_2018_paper.pdf


1126
Title: Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering
Type: Poster
Abstracts: A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.
Authors: Nguyen Duy Kien, Tohoku University;Takayuki Okatani, Tohoku University/RIKEN AIP;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Improved_Fusion_of_CVPR_2018_paper.pdf


1127
Title: Efficient Optimization for Rank-based Loss Functions
Type: Oral
Authors: Pritish Mohapatra, IIIT Hyderabad;Max Planck Institute for Intelligent Systems, Tuebingen;C.V. Jawahar, IIIT Hyderabad;Institute of Science and Technology, Austria;M. Pawan Kumar,;

1128
Title: Efficient Optimization for Rank-based Loss Functions
Type: Poster
Authors: Pritish Mohapatra, IIIT Hyderabad;Max Planck Institute for Intelligent Systems, Tuebingen;C.V. Jawahar, IIIT Hyderabad;Institute of Science and Technology, Austria;M. Pawan Kumar,;

1129
Title: Learning Less is More - 6D Camera Localization via 3D Surface Regression
Type: Poster
Authors: Eric Brachmann, TU Dresden;Carsten Rother, University of Heidelberg;

1130
Title: xUnit: Learning a Spatial Activation Function for Efficient Image Restoration
Type: Spotlight
Abstracts: In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.
Authors: Idan Kligvasser, Technion;Tamar Rott Shaham, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.pdf


1131
Title: xUnit: Learning a Spatial Activation Function for Efficient Image Restoration
Type: Poster
Abstracts: In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.
Authors: Idan Kligvasser, Technion;Tamar Rott Shaham, Technion;Tomer Michaeli, Technion;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.pdf


1132
Title: Multi-task Learning by Maximizing Statistical Dependence
Type: Poster
Authors: Youssef Alami Mejjati, University of Bath;Darren Cosker, University of Bath;Kwang In Kim, University of Bath;

1133
Title: Deep Back-Projection Networks For Super-Resolution
Type: Poster
Authors: Muhammad Haris, Toyota Technological Institute;Greg Shakhnarovich, ;Norimichi Ukita, NAIST;

1134
Title: Encoder-Decoder Alignment for Zero-Pair Image-to-Image Translation
Type: Poster
Authors: Yaxing Wang, Computer vision center;Joost van de Weijer, Computer Vision Center Barcelona;Luis Herranz, Computer Vision Center;

1135
Title: Dynamic Feature Learning for Partial Face Recognition
Type: Poster
Abstracts: Partial face recognition (PFR) in unconstrained environment is a very important task, especially in video surveillance, mobile devices, etc. However, a few studies have tackled how to recognize an arbitrary patch of a face image. This study combines Fully Convolutional Network (FCN) with Sparse Representation Classification (SRC) to propose a novel partial face recognition approach, called Dynamic Feature Matching (DFM), to address partial face images regardless of sizes. Based on DFM, we propose a sliding loss to optimize FCN by reducing the intra-variation between a face patch and face images of a subject, which further improves the performance of DFM. The proposed DFM is evaluated on several partial face databases, including LFW, YTF and CASIA-NIR-Distance databases. Experimental results demonstrate the effectiveness and advantages of DFM in comparison with state-of-the-art PFR methods.
Authors: Lingxiao He, Institute of AutomationChines;Haiqing Li, ;qi Zhang, ;Zhenan Sun, CRIPAC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/He_Dynamic_Feature_Learning_CVPR_2018_paper.pdf


1136
Title: MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks
Type: Oral
Authors: Huiwen Chang, ;Jingwan Lu, Adobe Research;Fisher Yu, UC Berkeley;Adam Finkelstein, Princeton University;

1137
Title: MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks
Type: Poster
Authors: Huiwen Chang, ;Jingwan Lu, Adobe Research;Fisher Yu, UC Berkeley;Adam Finkelstein, Princeton University;

1138
Title: Revisiting Deep Intrinsic Image Decompositions
Type: Oral
Abstracts: While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets. We then apply flexibly supervised loss layers that are customized for each source of ground truth labels. The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time.
Authors: Qingnan Fan, Shandong University;David Wipf, Microsoft Research Asia;Jiaolong Yang, Microsoft Research Asia;Gang Hua, Microsoft Research;Baoquan Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Revisiting_Deep_Intrinsic_CVPR_2018_paper.pdf


1139
Title: Revisiting Deep Intrinsic Image Decompositions
Type: Poster
Abstracts: While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data. The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes. In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets. We then apply flexibly supervised loss layers that are customized for each source of ground truth labels. The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time.
Authors: Qingnan Fan, Shandong University;David Wipf, Microsoft Research Asia;Jiaolong Yang, Microsoft Research Asia;Gang Hua, Microsoft Research;Baoquan Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Revisiting_Deep_Intrinsic_CVPR_2018_paper.pdf


1140
Title: Multi-Image Semantic Matching by Mining Consistent Features
Type: Poster
Abstracts: This work proposes a multi-image matching method to estimate semantic correspondences across multiple images. In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method for reconstructing object-class models and discovering object-class landmarks from images without using any annotation.
Authors: Qianqian Wang, Zhejiang University;Xiaowei Zhou, Zhejiang University;Kostas Daniilidis, University of Pennsylvania;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf


1141
Title: Indoor RGB-D Compass from a Single Line and Plane
Type: Poster
Authors: Pyojin Kim, Seoul National University;Brian Coltin, NASA Ames Research Center;H. Jin Kim,;

1142
Title: Learning Face Age Progression: A Pyramid Architecture of GANs
Type: Poster
Abstracts: The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable to diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.
Authors: Hongyu Yang, BEIHANG UNIVERSITY;Di Huang, ;Yunhong Wang, ;Anil Jain, MSU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Learning_Face_Age_CVPR_2018_paper.pdf


1143
Title: Multispectral Image Intrinsic Decomposition via Low Rank Constraint
Type: Poster
Authors: Qian Huang, Nanjing University;Zhu Weixin, Nanjing university;Yang Zhao, Nanjing University;Linsen Chen, Nanjing University;yao wang, new york university;Tao Yue, Nanjing Univ.;EE Department, Nanjing Univ;

1144
Title: Non-Linear Temporal Subspace Representations for Activity Recognition
Type: Poster
Abstracts: Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.
Authors: Anoop Cherian, ;Suvrit Sra, MIT;Stephen Gould, Australian National University;Richard Hartley, Australian National University Australia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cherian_Non-Linear_Temporal_Subspace_CVPR_2018_paper.pdf


1145
Title: CondenseNet: An Efficient DenseNet using Learned Group Convolutions
Type: Spotlight
Authors: Gao Huang, ;Shichen Liu, Tsinghua University;Laurens van der Maaten, Facebook;Kilian Weinberger, Cornell University;

1146
Title: CondenseNet: An Efficient DenseNet using Learned Group Convolutions
Type: Poster
Authors: Gao Huang, ;Shichen Liu, Tsinghua University;Laurens van der Maaten, Facebook;Kilian Weinberger, Cornell University;

1147
Title: Link and code: Fast indexing with graphs and compact regression codes
Type: Spotlight
Authors: Matthijs Douze, ;Herve Jegou, Facebook AI Research;

1148
Title: Link and code: Fast indexing with graphs and compact regression codes
Type: Poster
Authors: Matthijs Douze, ;Herve Jegou, Facebook AI Research;

1149
Title: StarGAN: Unified Generative Adversarial Networks for Controllable Multi-Domain Image-to-Image Translation
Type: Oral
Authors: Jaegul Choo, Korea University;Jung-Woo Ha, NAVER Corp;Munyoung Kim, The College of New Jersey;Yunjey Choi, Korea University;Minje Choi, Korea University;Sunghun Kim, HKUST;

1150
Title: StarGAN: Unified Generative Adversarial Networks for Controllable Multi-Domain Image-to-Image Translation
Type: Poster
Authors: Jaegul Choo, Korea University;Jung-Woo Ha, NAVER Corp;Munyoung Kim, The College of New Jersey;Yunjey Choi, Korea University;Minje Choi, Korea University;Sunghun Kim, HKUST;

1151
Title: Learning Deep Descriptors with Scale-Aware Triplet Networks
Type: Spotlight
Authors: Michel Keller, ETH Z?rich;Zetao Chen, ETH Zurich;Fabiola Maffra, ETH Z?rich;Patrik Schmuck, ETH Zurich;Margarita Chli, ETH Zurich;

1152
Title: Learning Deep Descriptors with Scale-Aware Triplet Networks
Type: Poster
Authors: Michel Keller, ETH Z?rich;Zetao Chen, ETH Zurich;Fabiola Maffra, ETH Z?rich;Patrik Schmuck, ETH Zurich;Margarita Chli, ETH Zurich;

1153
Title: MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features
Type: Poster
Authors: Liang-Chieh Chen, ;Alexander Hermans, RWTH Aachen University;George Papandreou, Google Inc.;Florian Schroff, Google Inc.;Peng Wang, Baidu;Hartwig Adam, Google;

1154
Title: Robust Classification with Convolutional Prototype Learning
Type: Poster
Authors: Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;cheng-lin Liu,;

1155
Title: Normalized Cut Loss for Weakly Supervised CNN Segmentation
Type: Poster
Authors: Meng Tang, UWO;Federico Perazzi, Disney Research Zurich;Abdelaziz Djelouah, The Walt Disney Company;Yuri Boykov, University of Western Ontario;Christopher Schroers, Disney Research Zurich;

1156
Title: Structured Uncertainty Prediction Networks
Type: Poster
Abstracts: This paper is the first work to propose a network to predict a structured uncertainty distribution for a synthesized image. Previous approaches have been mostly limited to predicting diagonal covariance matrices. Our novel model learns to predict a full Gaussian covariance matrix for each reconstruction, which permits efficient sampling and likelihood evaluation. We demonstrate that our model can accurately reconstruct ground truth correlated residual distributions for synthetic datasets and generate plausible high frequency samples for real face images. We also illustrate the use of these predicted covariances for structure preserving image denoising.
Authors: Garoe Dorta, University of Bath;Sara Vicente, Anthropics Technology Ltd;Lourdes Agapito, University College London;Neill Campbell, University of bath;Ivor Simpson, Anthropics Technology Ltd;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.pdf


1157
Title: CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization
Type: Poster
Abstracts: Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern deep networks contain millions of learned weights; a more efficient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with fine-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by In-Parallel Pruning-Quantization) compresses AlexNet by 51-fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet.
Authors: Frederick Tung, Simon Fraser University;Greg Mori,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.pdf


1158
Title: Inference in Higher Order MRF-MAP Problems with Small and Large Cliques
Type: Poster
Authors: Ishant Shanu, Iiit delhi;Chetan Arora, Indraprastha Institute of Information Technology Delhi;S.N. Maheshwari, IIT Delhi;

1159
Title: Ordinal Depth Supervision for 3D Human Pose Estimation
Type: Oral
Abstracts: Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.
Authors: Georgios Pavlakos, ;Xiaowei Zhou, Zhejiang University;Kostas Daniilidis, University of Pennsylvania;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.pdf


1160
Title: Ordinal Depth Supervision for 3D Human Pose Estimation
Type: Poster
Abstracts: Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.
Authors: Georgios Pavlakos, ;Xiaowei Zhou, Zhejiang University;Kostas Daniilidis, University of Pennsylvania;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.pdf


1161
Title: Generative Modeling using the Sliced Wasserstein Distance
Type: Poster
Authors: Ishan Deshpande, UIUC;Ziyu Zhang, Snap Research;Alex Schwing,;

1162
Title: Multi-Cell Classification by Convolutional Dictionary Learning with Class Proportion Priors
Type: Oral
Authors: Florence Yellin, Johns Hopkins University;Benjamin Haeffele, Johns Hopkins University;Rene Vidal, Johns Hopkins University;

1163
Title: Multi-Cell Classification by Convolutional Dictionary Learning with Class Proportion Priors
Type: Poster
Authors: Florence Yellin, Johns Hopkins University;Benjamin Haeffele, Johns Hopkins University;Rene Vidal, Johns Hopkins University;

1164
Title: CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes
Type: Poster
Abstracts: We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.
Authors: Yuhong Li, Beijing Univ. of Posts & Tels;Xiaofan Zhang, UIUC;deming Chen, UIUC;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.pdf


1165
Title: Learning to Estimate 3D Human Pose and Shape from a Single Color Image
Type: Poster
Authors: Georgios Pavlakos, ;Luyang Zhu, Peking University;Xiaowei Zhou, Zhejiang University;Kostas Daniilidis, University of Pennsylvania;

1166
Title: Revisiting knowledge transfer for training object class detectors
Type: Poster
Authors: Jasper Uijlings, Google;Stefan Popov, Google;Vitto Ferrari,;

1167
Title: Learning Intelligent Dialogs for Bounding Box Annotation
Type: Spotlight
Abstracts: We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.
Authors: Ksenia Konyushkova, Google;Jasper Uijlings, Google;Christoph Lampert, ;Vittorio Ferrari, google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.pdf


1168
Title: Learning Intelligent Dialogs for Bounding Box Annotation
Type: Poster
Abstracts: We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.
Authors: Ksenia Konyushkova, Google;Jasper Uijlings, Google;Christoph Lampert, ;Vittorio Ferrari, google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.pdf


1169
Title: Aperture Supervision for Monocular Depth Estimation
Type: Poster
Abstracts: We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image.
Authors: Pratul Srinivasan, Berkeley;Rahul Garg, ;Neal Wadhwa, ;Ren Ng, Berkeley;Jonathan Barron, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Srinivasan_Aperture_Supervision_for_CVPR_2018_paper.pdf


1170
Title: Burst Denoising with Kernel Prediction Networks
Type: Spotlight
Authors: Ben Mildenhall, UC Berkeley;Jiawen Chen, Google;Jonathan Barron, Google;Robert Carroll, Google;Dillon Sharlet, ;Ren Ng, Berkeley;

1171
Title: Burst Denoising with Kernel Prediction Networks
Type: Poster
Authors: Ben Mildenhall, UC Berkeley;Jiawen Chen, Google;Jonathan Barron, Google;Robert Carroll, Google;Dillon Sharlet, ;Ren Ng, Berkeley;

1172
Title: Art of singular vectors and universal adversarial perturbations
Type: Spotlight
Authors: Valentin Khrulkov, Skoltech;Ivan Oseledets, Skoltech;

1173
Title: Art of singular vectors and universal adversarial perturbations
Type: Poster
Authors: Valentin Khrulkov, Skoltech;Ivan Oseledets, Skoltech;

1174
Title: A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos
Type: Poster
Abstracts: Thanks to the advances in the technology of low-cost digital cameras and the popularity of the self-recording culture, the amount of visual data on the Internet is going to the opposite side of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched in a computer folder or website. In this work, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem, which combined with a smoothing frame transition method accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. The experiments show that our method is able to fast-forward videos to retain as much relevant information and smoothness as the state-of-the-art techniques in less time. We also present a new 80-hour multimodal (RGB-D, IMU, and GPS) dataset of first-person videos with annotations for recorder profile, frame scene, activities, interaction, and attention.
Authors: Michel Silva, Universidade de Minas Gerais;Washington Luis Ramos, Universidade Federal de Minas Gerais;Jo?o Pedro Ferreira, Universidade Federal de Minas Gerais;Felipe Chamone, Universidade Federal de Minas Gerais;Mario F Campos, Universidade Federal de Minas Gerais;Erickson Nascimento, Universidade Federal de Minas Gerais;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Silva_A_Weighted_Sparse_CVPR_2018_paper.pdf


1175
Title: A Low Power
Type: Spotlight
Authors: Alexander Andreopoulos, IBM Research;Hirak Kashyap, UC Irvine and IBM;Tapan Nayak, IBM;Arnon Amir, IBM;Myron Flickner, IBM;

1176
Title: A Low Power
Type: Poster
Authors: Alexander Andreopoulos, IBM Research;Hirak Kashyap, UC Irvine and IBM;Tapan Nayak, IBM;Arnon Amir, IBM;Myron Flickner, IBM;

1177
Title: Learning Latent Super-Events to Detect Multiple Activities in Videos
Type: Poster
Abstracts: In this paper, we introduce the concept of learning latent super-events from activity videos, and present how it benefits activity detection in continuous videos. We define a super-event as a set of multiple events occurring together in videos with a particular temporal organization; it is the opposite concept of sub-events. Real-world videos contain multiple activities and are rarely segmented (e.g., surveillance videos), and learning latent super-events allows the model to capture how the events are temporally related in videos. We design emph{temporal structure filters} that enable the model to focus on particular sub-intervals of the videos, and use them together with a soft attention mechanism to learn representations of latent super-events. Super-event representations are combined with per-frame or per-segment CNNs to provide frame-level annotations. Our approach is designed to be fully differentiable, enabling end-to-end learning of latent super-event representations jointly with the activity detector using them. Our experiments with multiple public video datasets confirm that the proposed concept of latent super-event learning significantly benefits activity detection, advancing the state-of-the-arts.
Authors: AJ Piergiovanni, Indiana University;Michael Ryoo, Indiana University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper.pdf


1178
Title: Active Fixation Control to Predict Saccade Sequences
Type: Poster
Abstracts: Visual attention is a field with a considerable history, with eye movement control and prediction forming an important subfield. Fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models, such as the Itti-Koch-Niebur model. The accuracy of such models has dramatically increased recently due to deep learning. However, on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map. Very few implemented models can generate temporally ordered human-like sequences of saccades beyond an initial fixation point. Towards addressing these shortcomings we present STAR-FC, a novel multi-saccade generator based on the integration of central high-level and object-based saliency and peripheral lower-level feature-based saliency. We have evaluated our model using the CAT2000 database, successfully predicting human patterns of fixation with equivalent accuracy and quality compared to what can be achieved by using one human sequence to predict another.
Authors: Calden Wloka, York University;Iuliia Kotseruba, York University;John Tsotsos, York University Canada;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wloka_Active_Fixation_Control_CVPR_2018_paper.pdf


1179
Title: Two-Stream Convolutional Networks for Dynamic Texture Synthesis
Type: Poster
Abstracts: We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.
Authors: Matthew Tesfaldet, York University;Marcus Brubaker, York University;Konstantinos Derpanis, Ryerson University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper.pdf


1180
Title: Jointly Localizing and Describing Events for Dense Video Captioning
Type: Spotlight
Abstracts: Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as ``dense video captioning." In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set.
Authors: Yehao Li, Sun Yat-Sen University;Ting Yao, Microsoft Research Asia;Yingwei Pan, University of Science and Technology of China;Hongyang Chao, Sun Yat-sen University;Tao Mei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Jointly_Localizing_and_CVPR_2018_paper.pdf


1181
Title: Jointly Localizing and Describing Events for Dense Video Captioning
Type: Poster
Abstracts: Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as ``dense video captioning." In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set.
Authors: Yehao Li, Sun Yat-Sen University;Ting Yao, Microsoft Research Asia;Yingwei Pan, University of Science and Technology of China;Hongyang Chao, Sun Yat-sen University;Tao Mei, Microsoft Research Asia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Jointly_Localizing_and_CVPR_2018_paper.pdf


1182
Title: Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks
Type: Poster
Authors: Tom Veniat, Lip6 - MLIA;Ludovic Denoyer, UPMC;

1183
Title: Customized Image Narrative Generation via Interactive Visual Question Generation and Answering
Type: Spotlight
Abstracts: Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.
Authors: null,Andr;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_Customized_Image_Narrative_CVPR_2018_paper.pdf


1184
Title: Customized Image Narrative Generation via Interactive Visual Question Generation and Answering
Type: Poster
Abstracts: Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.
Authors: Andrew Shin, The University of Tokyo;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_Customized_Image_Narrative_CVPR_2018_paper.pdf


1185
Title: Good Appearance Features for Multi-Target Multi-Camera Tracking
Type: Spotlight
Authors: Ergys Ristani, Duke University;Carlo Tomasi, Duke University;

1186
Title: Good Appearance Features for Multi-Target Multi-Camera Tracking
Type: Poster
Authors: Ergys Ristani, Duke University;Carlo Tomasi, Duke University;

1187
Title: Depth-Aware Stereo Video Retargeting
Type: Poster
Abstracts: As compared with traditional video retargeting, stereo video retargeting poses new challenges because stereo video contains the depth information of salient objects and its time dynamics. In this work, we propose a depth-aware stereo video retargeting method by imposing the depth fidelity constraint. The proposed depth-aware retargeting method reconstructs the 3D scene to obtain the depth information of salient objects. We cast it as a constrained optimization problem, where the total cost function includes the shape, temporal and depth distortions of salient objects. As a result, the solution can preserve the shape, temporal and depth fidelity of salient objects simultaneously. It is demonstrated by experimental results that the depth-aware retargeting method achieves higher retargeting quality and provides better user experience.
Authors: Bing Li, University of Southern Califor;Chia-Wen Lin, ;Tiejun Huang, ;Boxin Shi, Peking University;Wen Gao, ;C.-C. Jay Kuo, University of Southern California;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Depth-Aware_Stereo_Video_CVPR_2018_paper.pdf


1188
Title: Learning from Noisy Web Data with Category-level Supervision
Type: Poster
Authors: Li Niu, Rice University;Qingtao Tang, ;Ashok Veeraraghavan, Rice University;Ashutosh Sabharwal,;

1189
Title: Pixels
Type: Poster
Authors: Daeyun Shin, UC Irvine;Irvine, USA;Derek Hoiem,;

1190
Title: SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels
Type: Poster
Authors: Matthias Fey, TU Dortmund;Jan Lenssen, TU Dortmund;Frank Weichert, TU Dortmund;Heinrich M?ller, TU Dortmund;

1191
Title: Learning Depth from Monocular Videos using Direct Methods
Type: Poster
Authors: Chaoyang Wang, Carnegie Mellon University;Jose Buenaposada, Universidad Rey Juan Carlos;Rui Zhu, Carnegie Mellon University;Simon Lucey,;

1192
Title: Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning
Type: Poster
Abstracts: Driving Scene understanding is a key ingredient for intelligent transportation systems. To achieve systems that can operate in a complex physical and social environment, they need to understand and learn how humans drive and interact with traffic scenes. We present the Honda Research Institute Driving Dataset (HDD), a challenging dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors. We provide a detailed analysis of HDD with a comparison to other driving datasets. A novel annotation methodology is introduced to enable research on driver behavior understanding from untrimmed data sequences. As the first step, baseline algorithms for driver behavior detection are trained and tested to demonstrate the feasibility of the proposed task.
Authors: Vasili Ramanishka, Boston University;Yi-Ting Chen, Honda Research Institute USA;Teruhisa Misu, Honda Research Institute;Kate Saenko,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf


1193
Title: Generative Adversarial Image Synthesis with Decision Tree Latent Controller
Type: Poster
Authors: Takuhiro Kaneko, NTT Corporation;Kaoru Hiramatsu, NTT Corporation;Kunio Kashino, NTT;

1194
Title: Cross-View Image Synthesis using Conditional Generative Adversarial Nets
Type: Poster
Authors: Krishna Regmi, Ucf;Ali Borji, UCF;

1195
Title: Focus Manipulation Detection via Photometric Histogram Analysis
Type: Poster
Abstracts: With the rise of misinformation spread via social media channels, enabled by the increasing automation and realism of image manipulation tools, image forensics is an increasingly relevant problem. Classic image forensic methods leverage low-level cues such as metadata, sensor noise fingerprints, and others that are easily fooled when the image is re-encoded upon upload to facebook, etc. This necessitates the use of higher-level physical and semantic cues that, once hard to estimate reliably in the wild, have become more effective due to the increasing power of computer vision. In particular, we detect manipulations introduced by artificial blurring of the image, which creates inconsistent photometric relationships between image intensity and various cues. We achieve 98% accuracy on the most challenging cases in a new dataset of blur manipulations, where the blur is geometrically correct and consistent with the scene's physical arrangement. Such manipulations are now easily generated, for instance, by smartphone cameras having hardware to measure depth, e.g. `Portrait Mode' of the iPhone7Plus. We also demonstrate good performance on a challenge dataset evaluating a wider range of manipulations in imagery representing `in the wild' conditions.
Authors: Can Chen, University of Delaware;Scott McCloskey, Honeywell;University of Delaware, USA;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Focus_Manipulation_Detection_CVPR_2018_paper.pdf


1196
Title: Efficient
Type: Poster
Authors: Alexander Huth, University of Texas at Austin;Javier Turek, Intel Corporation;

1197
Title: A Robust Method for Strong Rolling Shutter Effects Correction Using Lines with Automatic Feature Selection
Type: Poster
Authors: Yizhen Lao, Institut Pascal;Omar Ait-Aider, Institut Pascal;

1198
Title: Learning Attribute Representations with Localization for Flexible Fashion Search
Type: Poster
Authors: Kenan Ak, National University of Singapo;I2R, Astar;Ashraf Kassim, ;JO YEW THAM,;

1199
Title: Analysis of Hand Segmentation in the Wild
Type: Poster
Abstracts: A large number of works in egocentric vision have concentrated on action and object recognition. Detection and segmentation of hands in first-person videos, however, has less been explored. For many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. Here, we take an in-depth look at the hand segmentation problem. In the quest for robust hand segmentation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and fine-tuned, on existing datasets. We fine-tune RefineNet, a leading semantic segmentation method, for hand segmentation and find that it does much better than the best contenders. Existing hand segmentation datasets are collected in the laboratory settings. To overcome this limitation, we contribute by collecting two new datasets: a) EgoYouTubeHands including egocentric videos containing hands in the wild, and b) HandOverFace to analyze the performance of our models in presence of similar appearance occlusions. We further explore whether conditional random fields can help refine generated hand segmentations. To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for fine-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%).
Authors: Aisha Urooj, University of Central Florida;Ali Borji, UCF;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Urooj_Analysis_of_Hand_CVPR_2018_paper.pdf


1200
Title: Long-Term On-Board Prediction of People in Traffic Scenes under Uncertainty
Type: Poster
Authors: Apratim Bhattacharyya, MPI Informatics;Bernt Schiele, MPI Informatics Germany;Saarbrucken, Germany;

1201
Title: Accurate and Diverse Sampling of Sequences based on a ``Best of Many'' Sample Objective
Type: Oral
Authors: Apratim Bhattacharyya, MPI Informatics;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;

1202
Title: Accurate and Diverse Sampling of Sequences based on a ``Best of Many'' Sample Objective
Type: Poster
Authors: Apratim Bhattacharyya, MPI Informatics;Saarbrucken, Germany;Bernt Schiele, MPI Informatics Germany;

1203
Title: Wrapped Gaussian Process Regression on Riemannian Manifolds
Type: Poster
Abstracts: Gaussian process (GP) regression is a powerful tool in non-parametric regression providing uncertainty estimates. However, it is limited to data in vector spaces. In fields such as shape analysis and diffusion tensor imaging, the data often lies on a manifold, making GP regression non- viable, as the resulting predictive distribution does not live in the correct geometric space. We tackle the problem by defining wrapped Gaussian processes (WGPs) on Rieman- nian manifolds, using the probabilistic setting to general- ize GP regression to the context of manifold-valued targets. The method is validated empirically on diffusion weighted imaging (DWI) data, directional data on the sphere and in the Kendall shape space, endorsing WGP regression as an efficient and flexible tool for manifold-valued regression.
Authors: Anton Mallasto, University of Copenhagen;Aasa Feragen, University of Copenhagen;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Mallasto_Wrapped_Gaussian_Process_CVPR_2018_paper.pdf


1204
Title: Between-class Learning for Image Classification
Type: Poster
Authors: Yuji Tokozume, The University of Tokyo;Yoshitaka Ushiku, ;Tatsuya Harada, University of Tokyo;

1205
Title: Unsupervised Person Image Synthesis in Arbitrary Poses
Type: Spotlight
Abstracts: We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks. First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches.
Authors: Albert Pumarola, IRI (CSIC-UPC);Antonio Agudo, IRI (CSIC-UPC);Alberto Sanfeliu, IRI (CSIC-UPC);Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC);
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pumarola_Unsupervised_Person_Image_CVPR_2018_paper.pdf


1206
Title: Unsupervised Person Image Synthesis in Arbitrary Poses
Type: Poster
Abstracts: We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks. First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches.
Authors: Albert Pumarola, IRI (CSIC-UPC);Antonio Agudo, IRI (CSIC-UPC);Alberto Sanfeliu, IRI (CSIC-UPC);Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC);
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Pumarola_Unsupervised_Person_Image_CVPR_2018_paper.pdf


1207
Title: Visual Feature Attribution using Wasserstein GANs
Type: Poster
Authors: Christian Baumgartner, ETH Zurich;Lisa Koch, ETH Zurich;Kerem Tezcan, ETH Zurich;Jia Xi Ang, ETH Zurich;Ender Konukoglu, ETH Zurich;

1208
Title: ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes
Type: Poster
Abstracts: Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.
Authors: Yuhua Chen, CVL@ETHZ;Wen Li, ETH;Luc Van Gool, KTH;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_ROAD_Reality_Oriented_CVPR_2018_paper.pdf


1209
Title: Im2Struct: Recovering 3D Shape Structure from a Single RGB Image
Type: Poster
Authors: Chengjie Niu, National University of Defense Technology;Jun Li, ;Kai Xu, NUDT & Princeton Univeristy;

1210
Title: MapNet: An Allocentric Spatial Memory for Mapping Environments
Type: Oral
Abstracts: Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to it. In this paper, we develop a differentiable module that satisfies such requirements, while being robust, efficient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an LSTM or similar mechanism. We formulate efficient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from RGBD input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structure-from-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot.
Authors: Joao Henriques, ;Andrea Vedaldi, U Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.pdf


1211
Title: MapNet: An Allocentric Spatial Memory for Mapping Environments
Type: Poster
Abstracts: Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to it. In this paper, we develop a differentiable module that satisfies such requirements, while being robust, efficient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an LSTM or similar mechanism. We formulate efficient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from RGBD input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structure-from-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot.
Authors: Joao Henriques, ;Andrea Vedaldi, U Oxford;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.pdf


1212
Title: A Globally Optimal Solution to the Non-Minimal Relative Pose Problem
Type: Oral
Authors: Jesus Briales, University of Malaga;Laurent Kneip, ;Javier Gonzalez-Jimenez,;

1213
Title: A Globally Optimal Solution to the Non-Minimal Relative Pose Problem
Type: Poster
Authors: Jesus Briales, University of Malaga;Laurent Kneip, ;Javier Gonzalez-Jimenez,;

1214
Title: Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework
Type: Spotlight
Abstracts: Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of misalignment blur. Extensive evaluations show that up to 5dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.
Authors: Jie Chen, Nanyang Technological University;Cheen-Hau Tan, ;Junhui Hou, City University of Hong Kong;Lap-Pui Chau, Nanyang Technological University;He Li,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Robust_Video_content_cvpr_2018_paper.pdf


1215
Title: Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework
Type: Poster
Abstracts: Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of misalignment blur. Extensive evaluations show that up to 5dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.
Authors: Jie Chen, Nanyang Technological University;Cheen-Hau Tan, ;Junhui Hou, City University of Hong Kong;Lap-Pui Chau, Nanyang Technological University;He Li,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Robust_Video_content_cvpr_2018_paper.pdf


1216
Title: Unsupervised Learning and Segmentation of Complex Activities from Video
Type: Spotlight
Authors: Fadime Sener, University of Bonn;Angela Yao, University of Bonn;

1217
Title: Unsupervised Learning and Segmentation of Complex Activities from Video
Type: Poster
Authors: Fadime Sener, University of Bonn;Angela Yao, University of Bonn;

1218
Title: Inferring Light Fields from Shadows
Type: Spotlight
Authors: Manel Baradad, MIT;Vickie Ye, MIT;Adam Yedida, MIT;Fredo Durand, ;William Freeman, MIT/Google;Gregory Wornell, ;Antonio Torralba, MIT;

1219
Title: Inferring Light Fields from Shadows
Type: Poster
Authors: Manel Baradad, MIT;Vickie Ye, MIT;Adam Yedida, MIT;Fredo Durand, ;William Freeman, MIT/Google;Gregory Wornell, ;Antonio Torralba, MIT;

1220
Title: Bidirecional Retrieval Made Simple
Type: Poster
Authors: J?natas Wehrmann, PUCRS;Rodrigo Barros, PUCRS;

1221
Title: A Twofold Siamese Network for Real-Time Object Tracking
Type: Poster
Abstracts: Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similarity learning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.
Authors: Anfeng He, USTC;Chong Luo, Microsoft Research Asia;Xinmei Tian, USTC;Wenjun Zeng,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf


1222
Title: Multi-shot Pedestrian Re-identification via Sequential Decision Making
Type: Poster
Authors: Jianfu Zhang, Shanghai Jiaotong University;Naiyan Wang, tusimple;Liqing Zhang, Shanghai Jiaotong University;

1223
Title: Wide Compression: Tensor Ring Nets
Type: Poster
Abstracts: Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep networks. Our results show that our TR-Nets approach is able to compress LeNet-5 by 11x without losing accuracy, and can compress the state-of-the-art Wide ResNet by 243x with only 2.3% degradation in Cifar10 image classification. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.
Authors: Wenqi Wang, Purdue University;YIfan Sun, Technicolor Research;Brian Eriksson, Adobe;Wenlin Wang, Duke University;Vaneet Aggarwal, Purdue University;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Wide_Compression_Tensor_CVPR_2018_paper.pdf


1224
Title: Shift: A Zero FLOP
Type: Spotlight
Authors: Bichen Wu, UC Berkeley;Xiangyu Yue, UC Berkeley;Alvin Wan, UC Berkeley;Peter Jin, UC Berkeley;Sicheng Zhao, UC Berkeley;Noah Golmant, UC Berkeley;Amir Gholaminejad, UC Berkeley;Joseph Gonzalez, UC Berkeley;Kurt Keutzer, UC Berkeley;

1225
Title: Shift: A Zero FLOP
Type: Poster
Authors: Bichen Wu, UC Berkeley;Xiangyu Yue, UC Berkeley;Alvin Wan, UC Berkeley;Peter Jin, UC Berkeley;Sicheng Zhao, UC Berkeley;Noah Golmant, UC Berkeley;Amir Gholaminejad, UC Berkeley;Joseph Gonzalez, UC Berkeley;Kurt Keutzer, UC Berkeley;

1226
Title: Improvements to context based self-supervised learning
Type: Poster
Authors: Terrell Mundhenk, LLNL;Daniel Ho, LLNL;Barry Chen, LLNL;

1227
Title: On the convergence of PatchMatch and its variants
Type: Poster
Authors: CMLA, ENS Cachan;CMLA, ENS Cachan;

1228
Title: Adversarial Feature Augmentation for Unsupervised Domain Adaptation
Type: Poster
Abstracts: Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.
Authors: Riccardo Volpi, IIT (Italy);Pietro Morerio, Istituto Italiano di Tecnologi;Silvio Savarese, ;Vittorio Murino, Istituto Italiano di Tecnologia;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.pdf


1229
Title: Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations
Type: Poster
Authors: Aditya Dhawale, Carnegie Mellon University;Kumar Shaurya Shankar, Carnegie Mellon University;Nathan Michael, Carnegie Mellon University;

1230
Title: Learning Multi-Instance Enriched Image Representation via Non-Greedy Simultaneous L1 -Norm Minimization and Maximization
Type: Poster
Authors: Hua Wang, Colorado School of Mines;

1231
Title: Automatic Map Inference from Aerial Images
Type: Poster
Authors: Favyen Bastani, MIT CSAIL;Songtao He, MIT CSAIL;Mohammad Alizadeh, MIT CSAIL;Hari Balakrishnan, MIT CSAIL;Sam Madden, MIT CSAIL;Sanjay Chawla, Qatar Computing Research Institute;Sofiane Abbar, Qatar Computing Research Institute;David DeWitt, MIT CSAIL;

1232
Title: Captioning Images with Style Transfer from Unaligned Text Corpora
Type: Spotlight
Authors: Alexander Mathews, Australian National University;Xuming He, ShanghaiTech;Australian National University, Data61;

1233
Title: Captioning Images with Style Transfer from Unaligned Text Corpora
Type: Poster
Authors: Alexander Mathews, Australian National University;Xuming He, ShanghaiTech;Australian National University, Data61;

1234
Title: Exploiting Transitivity for Learning Person Re-identification Models on a Budget
Type: Poster
Authors: Sourya Roy, UC Riverside ;Sujoy Paul, UC Riverside;Neal Young, UC Riverside ;Amit Roy-Chowdhury, UC Riverside;

1235
Title: Salience Guided Depth Calibration for Perceptually Optimized Compressive Light Field 3D Display
Type: Poster
Abstracts: Multi-layer light field displays are a type of computational three-dimensional (3D) display which has recently gained increasing interest for its holographic-like effect and natural compatibility with 2D displays. However, the major shortcoming, depth limitation, still cannot be overcome in the traditional light field modeling and reconstruction based on multi-layer liquid crystal displays (LCDs). Considering this disadvantage, our paper incorporates a salience guided depth optimization over a limited display range to calibrate the displayed depth and present the maximum area of salience region for multi-layer light field display. Different from previously reported cascaded light field displays that use the fixed initialization plane as the depth center of display content, our method automatically calibrates the depth initialization based on the salience results derived from the proposed contrast enhanced salience detection method. Experiments demonstrate that the proposed method provides a promising advantage in visual perception for the compressive light field displays from both software simulation and prototype demonstration.
Authors: NTU, Singapore;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Salience_Guided_Depth_CVPR_2018_paper.pdf


1236
Title: Extreme 3D Face Reconstruction: Looking Past Occlusions
Type: Spotlight
Authors: Anh Tran, USC;Tal Hassner, Open Univ Israel;Iacopo Masi, USC;G?rard Medioni,;

1237
Title: Extreme 3D Face Reconstruction: Looking Past Occlusions
Type: Poster
Authors: Anh Tran, USC;Tal Hassner, Open Univ Israel;Iacopo Masi, USC;G?rard Medioni,;

1238
Title: Deflecting Adversarial Attacks with Pixel Deflection
Type: Spotlight
Authors: Aaditya Prakash, Brandeis University;Nick Moran, Bradeis University;Solomon Garber, Brandeis University;Antonella DiLillo, Brandeis University;James Storer, Brandeis University;

1239
Title: Deflecting Adversarial Attacks with Pixel Deflection
Type: Poster
Authors: Aaditya Prakash, Brandeis University;Nick Moran, Bradeis University;Solomon Garber, Brandeis University;Antonella DiLillo, Brandeis University;James Storer, Brandeis University;

1240
Title: Boosting Adversarial Attacks with Momentum
Type: Spotlight
Authors: Yinpeng Dong, Tsinghua Univeristy;Fangzhou Liao, Tsinghua University;Tianyu Pang, Tsinghua University;Hang Su, Tsinghua University;Jun Zhu, Tsinghua University;Xiaolin Hu, tsinghua;Jianguo Li, Intel Lab;

1241
Title: Boosting Adversarial Attacks with Momentum
Type: Poster
Authors: Yinpeng Dong, Tsinghua Univeristy;Fangzhou Liao, Tsinghua University;Tianyu Pang, Tsinghua University;Hang Su, Tsinghua University;Jun Zhu, Tsinghua University;Xiaolin Hu, tsinghua;Jianguo Li, Intel Lab;

1242
Title: A Robust Generative Framework for Generalized Zero-Shot Learning
Type: Poster
Authors: Vinay Verma, IIT Kanpur;Gundeep Arora, IIT Kanpur;Ashish Mishra, IIT MADRAS;Piyush Rai, IIT Kanpur;

1243
Title: 3D Registration of Curves and Surfaces using Local Differential Information
Type: Poster
Authors: Carolina Raposo, Institute of Systems and Robot;University of Coimbra, Portugal;

1244
Title: Trust your Model: Light Field Depth Estimation with inline Occlusion Handling
Type: Poster
Authors: Hendrik Schilling, Universit?t Heidelberg;Maximilian Diebold, Heidelberg University;Carsten Rother, University of Heidelberg;Bernd J?hne, University of Heidelberg;

1245
Title: Partially Shared Multi-Task Convolutional Neural Network with Local Constraint for Face Attribute Learning
Type: Poster
Authors: Jiajiong Cao, ;Yingming Li, Zhejiang University;Zhongfei Zhang,;

1246
Title: Decoupled Networks
Type: Spotlight
Abstracts: Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness.
Authors: Weiyang Liu, Georgia Tech;Zhen Liu, ;Zhiding Yu, Carnegie Mellon University;Bo Dai, ;Yisen Wang, Tsinghua University;Thomas Breuel, ;James Rehg, Georgia Institute of Technology;Jan Kautz, NVIDIA;Le Song, Georgia Institute of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Decoupled_Networks_CVPR_2018_paper.pdf


1247
Title: Decoupled Networks
Type: Poster
Abstracts: Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness.
Authors: Weiyang Liu, Georgia Tech;Zhen Liu, ;Zhiding Yu, Carnegie Mellon University;Bo Dai, ;Yisen Wang, Tsinghua University;Thomas Breuel, ;James Rehg, Georgia Institute of Technology;Jan Kautz, NVIDIA;Le Song, Georgia Institute of Technology;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Decoupled_Networks_CVPR_2018_paper.pdf


1248
Title: Learning Structure and Strength of CNN Filters for Small Sample Size Training
Type: Poster
Abstracts: Convolutional Neural Networks have provided state-of-the-art results in several computer vision problems. However, due to a large number of parameters in CNNs, they require a large number of training samples which is a limiting factor for small sample size problems. To address this limitation, in this paper, we propose SSF-CNN which focuses on learning the “structure" and “strength" of filters. The structure of the filter is initialized using a dictionary based filter learning algorithm and the strength of the filter is learned using the small sample training data. The architecture provides the flexibility of training with both small and large training databases, and yields good accuracies even with small size training data. The effectiveness of the algorithm is demonstrated on MNIST, CIFAR10, NORB, Omniglot, and Newborn Face Image databases, with varying number of training samples. The results show that SSF-CNN significantly reduces the number of parameters required for training while providing high accuracies on the test database. On small problems such as newborn face recognition, the results demonstrate improvement in rank-1 identification accuracy by at least 10%.
Authors: Rohit Keshari, IIIT Delhi;Mayank Vatsa, IIIT Dehli;Richa Singh, IIT Dehli;Afzel Noore, WVU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf


1249
Title: Motion Segmentation by Exploiting Complementary Geometric Models
Type: Poster
Abstracts: Many real-world sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-view spectral clustering framework that synergistically combines multiple models together. We show that the performance can be substantially improved in this way. We perform extensive testing on existing motion segmentation datasets, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.
Authors: Xun Xu, National University of Singapore;Loong Fah Cheong, National University of Singapore;Zhuwen Li, Intel Labs;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Motion_Segmentation_by_CVPR_2018_paper.pdf


1250
Title: Unsupervised Learning of Single View Depth Estimation and Visual Odometry with Deep Feature Reconstruction
Type: Poster
Authors: Huangying Zhan, The University of Adelaide;Ravi Garg, The University of Adelaide;Chamara Weerasekera, The University of Adelaide;Kejie Li, The University of Adelaide;Harsh Agarwal, Indian Institute of Technology (BHU);Ian Reid,;

1251
Title: GAGAN: Geometry Aware Generative Adverserial Networks
Type: Poster
Authors: Jean Kossaifi, Imperial College London;Linh Tran, Imperial College London;Yannis Panagakis, ;Imperial College London, UK;

1252
Title: Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks
Type: Spotlight
Authors: Ruth Fong, University of Oxford;Andrea Vedaldi, U Oxford;

1253
Title: Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks
Type: Poster
Authors: Ruth Fong, University of Oxford;Andrea Vedaldi, U Oxford;

1254
Title: SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks
Type: Poster
Authors: Julian Faraone, University of Sydney;Nicholas Fraser, Xilinx;Michaela Blott, Xilinx;Philip Leong,;

1255
Title: Learning Representations for Single Cells in Microscopy Images
Type: Poster
Authors: Juan Caicedo, Broad Institute of Harvard and;Claire Mcquin, Broad Institute of Harvard and MIT;Allen Goodman, Broad Institute of Harvard and MIT;Shantanu Singh, Broad Institute of Harvard and MIT;Anne Carpenter, Broad Institute of Harvard and MIT;

1256
Title: Estimation of Camera Locations in Highly Corrupted Scenarios: All About the Base
Type: Poster
Authors: Yunpeng Shi, University of Minnesota;Gilad Lerman, University of Minnesota;

1257
Title: Deep Spatial Feature Reconstruction for Partial Person Re-identification
Type: Poster
Authors: Lingxiao He, Institute of AutomationChines;Jian Liang, CASIA;Haiqing Li, ;Zhenan Sun, CRIPAC;

1258
Title: Cross-Dataset Adaptation for Visual Question Answering
Type: Poster
Abstracts: We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an ``in-domain'' model. The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers. We overcome this difficulty by proposing a novel domain adaptation algorithm. Our method reduces the difference in statistical distributions by transforming the feature representation of the data in the target dataset. Moreover, it maximizes the likelihood of answering questions (in the target dataset) correctly using the Visual QA model trained on the source dataset. We empirically studied the effectiveness of the proposed approach on adapting among several popular Visual QA datasets. We show that the proposed method improves over baselines where there is no adaptation and several other adaptation methods. We both quantitatively and qualitatively analyze when the adaptation can be mostly effective.
Authors: Wei-Lun Chao, USC;Hexiang Hu, ;Fei Sha, University of Southern California;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.pdf


1259
Title: Eye In-Painting with Exemplar Generative Adversarial Networks
Type: Poster
Authors: Brian Dolhansky, Facebook;Cristian Canton Ferrer, Facebook;

1260
Title: Learning Visual Knowledge Memory Networks for Visual Question Answering
Type: Poster
Abstracts: Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can't be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.
Authors: Zhou Su, ;Jianguo Li, Intel Lab;Zhiqiang Shen, Fudan University;Yurong Chen,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Learning_Visual_Knowledge_CVPR_2018_paper.pdf


1261
Title: Compassionately Conservative Balanced Cuts for Image Segmentation
Type: Poster
Abstracts: The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the Buehler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $ell_{ au}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.
Authors: Nathan Cahill, Rochester Institute of Technol;Tyler Hayes, Rochester Institute of Tech;Renee Meinhold, Rochester Institute of Technology;John Hamilton, RIT;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.pdf


1262
Title: Neural Motifs: Scene Graph Parsing with Global Context
Type: Poster
Authors: Rowan Zellers, University of Washington;Mark Yatskar, University of Washington;Samuel Thomson, Carnegie Mellon University;Yejin Choi, University of Washington;

1263
Title: Alternating-Stereo VINS: Observability Analysis and Performance Evaluation
Type: Poster
Abstracts: One approach to improve the accuracy and robustness of vision-aided inertial navigation systems (VINS) that employ low-cost inertial sensors, is to obtain scale information from stereoscopic vision. Processing images from two cameras, however, is computationally expensive and increases latency. To address this limitation, in this work, a novel two-camera alternating-stereo VINS is presented. Specifically, the proposed system triggers the left-right cameras in an alternating fashion, estimates the poses corresponding to the left camera only, and introduces a linear interpolation model for processing the alternating right camera measurements. Although not a regular stereo system, the alternating visual observations when employing the proposed interpolation scheme, still provide scale information, as shown by analyzing the observability properties of the vision-only corresponding system. Finally, the performance gain, of the proposed algorithm over its monocular and stereo counterparts is assessed using various datasets.
Authors: Mrinal Kanti Paul, Google;Stergios Roumeliotis, Google;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Paul_Alternating-Stereo_VINS_Observability_CVPR_2018_paper.pdf


1264
Title: clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions
Type: Poster
Authors: Dongqing Zhang, ImaginationAI LLC;

1265
Title: Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution
Type: Spotlight
Abstracts: In many computer vision applications, obtaining images of high resolution in both the spatial and spectral domains are equally important. However, due to hardware limitations, one can only expect to acquire images of high resolution in either the spatial or spectral domains. This paper focuses on hyperspectral image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low spatial resolution (LR) but high spectral resolution is fused with a multispectral image (MSI) with high spatial resolution (HR) but low spectral resolution to obtain HR HSI. Existing deep learning-based solutions are all supervised that would need a large training set and the availability of HR HSI, which is unrealistic. Here, we make the first attempt to solving the HSI-SR problem using an unsupervised encoder-decoder architecture that carries the following uniquenesses. First, it is composed of two encoder-decoder networks, coupled through a shared decoder, in order to preserve the rich spectral information from the HSI network. Second, the network encourages the representations from both modalities to follow a sparse Dirichlet distribution which naturally incorporates the two physical constraints of HSI and MSI. Third, the angular difference between representations are minimized in order to reduce the spectral distortion. We refer to the proposed architecture as unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results demonstrate the superior performance of uSDN as compared to the state-of-the-art.
Authors: Ying Qu, The University of Tennessee;Hairong Qi, University of Tennessee;Chiman Kwan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Qu_Unsupervised_Sparse_Dirichlet-Net_CVPR_2018_paper.pdf


1266
Title: Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution
Type: Poster
Abstracts: In many computer vision applications, obtaining images of high resolution in both the spatial and spectral domains are equally important. However, due to hardware limitations, one can only expect to acquire images of high resolution in either the spatial or spectral domains. This paper focuses on hyperspectral image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low spatial resolution (LR) but high spectral resolution is fused with a multispectral image (MSI) with high spatial resolution (HR) but low spectral resolution to obtain HR HSI. Existing deep learning-based solutions are all supervised that would need a large training set and the availability of HR HSI, which is unrealistic. Here, we make the first attempt to solving the HSI-SR problem using an unsupervised encoder-decoder architecture that carries the following uniquenesses. First, it is composed of two encoder-decoder networks, coupled through a shared decoder, in order to preserve the rich spectral information from the HSI network. Second, the network encourages the representations from both modalities to follow a sparse Dirichlet distribution which naturally incorporates the two physical constraints of HSI and MSI. Third, the angular difference between representations are minimized in order to reduce the spectral distortion. We refer to the proposed architecture as unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results demonstrate the superior performance of uSDN as compared to the state-of-the-art.
Authors: Ying Qu, The University of Tennessee;Hairong Qi, University of Tennessee;Chiman Kwan,;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Qu_Unsupervised_Sparse_Dirichlet-Net_CVPR_2018_paper.pdf


1267
Title: A Volumetric Descriptive Network for 3D Object Synthesis
Type: Oral
Authors: Jianwen Xie, UCLA;Zilong Zheng, ucla;

1268
Title: A Volumetric Descriptive Network for 3D Object Synthesis
Type: Poster
Authors: Jianwen Xie, UCLA;Zilong Zheng, ucla;

1269
Title: Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation
Type: Poster
Abstracts: We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images that we use to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior enables fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code, model definitions and model weights are freely available at http://github.com/adalca/neuron
Authors: Adrian Dalca, ;John Guttag, ;Mert Sabuncu, Cornell;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Dalca_Anatomical_Priors_in_CVPR_2018_paper.pdf


1270
Title: Learning Face Age Progression: A Pyramid Architecture of GANs
Type: Oral
Abstracts: The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable to diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.
Authors: Hongyu Yang, BEIHANG UNIVERSITY;Di Huang, ;Yunhong Wang, ;Anil Jain, MSU;
PDF_Link: http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Learning_Face_Age_CVPR_2018_paper.pdf


